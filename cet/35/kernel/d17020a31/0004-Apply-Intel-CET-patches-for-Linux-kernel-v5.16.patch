From 215df8118362d0bfe6ffc626af8cfdfc04d41732 Mon Sep 17 00:00:00 2001
From: "H.J. Lu" <hjl.tools@gmail.com>
Date: Tue, 14 Apr 2020 10:43:52 -0700
Subject: [PATCH 4/7] Apply Intel CET patches for Linux kernel v5.16

---
 ...ocumentation-x86-Add-CET-description.patch | 183 +++++++
 ...-Add-Kconfig-option-for-Shadow-Stack.patch |  83 ++++
 ...Add-CET-CPU-feature-flags-for-Contro.patch |  82 ++++
 ...Introduce-CPU-setup-and-option-parsi.patch |  97 ++++
 ...ntroduce-CET-MSR-and-XSAVES-supervis.patch | 212 ++++++++
 ...Add-control-protection-fault-handler.patch | 247 ++++++++++
 ...ove-_PAGE_DIRTY-from-kernel-RO-pages.patch |  67 +++
 ...e-pmd_write-pud_write-up-in-the-file.patch |  68 +++
 0009-x86-mm-Introduce-_PAGE_COW.patch         | 452 ++++++++++++++++++
 ...ange-_PAGE_DIRTY-to-_PAGE_DIRTY_BITS.patch |  39 ++
 ...6-mm-Update-pte_modify-for-_PAGE_COW.patch |  92 ++++
 ...ep_set_wrprotect-and-pmdp_set_wrprot.patch |  95 ++++
 ...Move-VM_UFFD_MINOR_BIT-from-37-to-38.patch |  33 ++
 ...SHADOW_STACK-for-shadow-stack-memory.patch |  94 ++++
 ...Check-Shadow-Stack-page-fault-errors.patch | 100 ++++
 ...pdate-maybe_mkwrite-for-shadow-stack.patch | 130 +++++
 ...laces-that-call-pte_mkwrite-directly.patch | 107 +++++
 ...dd-guard-pages-around-a-shadow-stack.patch | 137 ++++++
 ...dow-stack-pages-to-memory-accounting.patch | 102 ++++
 ...an_follow_write_pte-for-shadow-stack.patch | 109 +++++
 ...ude-shadow-stack-from-preserve_write.patch |  68 +++
 ...-mm-Re-introduce-vm_flags-to-do_mmap.patch | 153 ++++++
 ...pers-for-modifying-supervisor-xstate.patch | 215 +++++++++
 ...k-Add-user-mode-shadow-stack-support.patch | 270 +++++++++++
 ...ge-copy_thread-argument-arg-to-stack.patch |  53 ++
 ...-fpu-Add-unsafe-xsave-buffer-helpers.patch |  96 ++++
 ...cet-shstk-Handle-thread-shadow-stack.patch | 223 +++++++++
 ...troduce-shadow-stack-token-setup-ver.patch | 250 ++++++++++
 ...hstk-Handle-signals-for-shadow-stack.patch | 259 ++++++++++
 ...Add-arch_prctl-elf-feature-functions.patch | 218 +++++++++
 ...validate_flags-to-test-vma-anonymous.patch | 117 +++++
 ...k-Introduce-map_shadow_stack-syscall.patch | 285 +++++++++++
 ...86-Add-map_shadow_stack-syscall-test.patch | 135 ++++++
 ...cet-shstk-Support-wrss-for-userspace.patch | 177 +++++++
 ...res-Limit-shadow-stack-to-Intel-CPUs.patch |  41 ++
 ...r-thread.shstk-and-feat_prctl_locked.patch |  31 ++
 ...x86-cet-Add-PTRACE-interface-for-CET.patch | 166 +++++++
 ...la-sections-when-CONFIG_RELOCATABLE-.patch |  56 +++
 ...u.property-sections-in-generic-NOTES.patch |  81 ++++
 kernel.spec                                   |  82 +++-
 40 files changed, 5504 insertions(+), 1 deletion(-)
 create mode 100644 0001-Documentation-x86-Add-CET-description.patch
 create mode 100644 0002-x86-cet-shstk-Add-Kconfig-option-for-Shadow-Stack.patch
 create mode 100644 0003-x86-cpufeatures-Add-CET-CPU-feature-flags-for-Contro.patch
 create mode 100644 0004-x86-cpufeatures-Introduce-CPU-setup-and-option-parsi.patch
 create mode 100644 0005-x86-fpu-xstate-Introduce-CET-MSR-and-XSAVES-supervis.patch
 create mode 100644 0006-x86-cet-Add-control-protection-fault-handler.patch
 create mode 100644 0007-x86-mm-Remove-_PAGE_DIRTY-from-kernel-RO-pages.patch
 create mode 100644 0008-x86-mm-Move-pmd_write-pud_write-up-in-the-file.patch
 create mode 100644 0009-x86-mm-Introduce-_PAGE_COW.patch
 create mode 100644 0010-drm-i915-gvt-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_BITS.patch
 create mode 100644 0011-x86-mm-Update-pte_modify-for-_PAGE_COW.patch
 create mode 100644 0012-x86-mm-Update-ptep_set_wrprotect-and-pmdp_set_wrprot.patch
 create mode 100644 0013-mm-Move-VM_UFFD_MINOR_BIT-from-37-to-38.patch
 create mode 100644 0014-mm-Introduce-VM_SHADOW_STACK-for-shadow-stack-memory.patch
 create mode 100644 0015-x86-mm-Check-Shadow-Stack-page-fault-errors.patch
 create mode 100644 0016-x86-mm-Update-maybe_mkwrite-for-shadow-stack.patch
 create mode 100644 0017-mm-Fixup-places-that-call-pte_mkwrite-directly.patch
 create mode 100644 0018-mm-Add-guard-pages-around-a-shadow-stack.patch
 create mode 100644 0019-mm-mmap-Add-shadow-stack-pages-to-memory-accounting.patch
 create mode 100644 0020-mm-Update-can_follow_write_pte-for-shadow-stack.patch
 create mode 100644 0021-mm-mprotect-Exclude-shadow-stack-from-preserve_write.patch
 create mode 100644 0022-mm-Re-introduce-vm_flags-to-do_mmap.patch
 create mode 100644 0023-x86-fpu-Add-helpers-for-modifying-supervisor-xstate.patch
 create mode 100644 0024-x86-cet-shstk-Add-user-mode-shadow-stack-support.patch
 create mode 100644 0025-x86-process-Change-copy_thread-argument-arg-to-stack.patch
 create mode 100644 0026-x86-fpu-Add-unsafe-xsave-buffer-helpers.patch
 create mode 100644 0027-x86-cet-shstk-Handle-thread-shadow-stack.patch
 create mode 100644 0028-x86-cet-shstk-Introduce-shadow-stack-token-setup-ver.patch
 create mode 100644 0029-x86-cet-shstk-Handle-signals-for-shadow-stack.patch
 create mode 100644 0030-x86-cet-shstk-Add-arch_prctl-elf-feature-functions.patch
 create mode 100644 0031-mm-Update-arch_validate_flags-to-test-vma-anonymous.patch
 create mode 100644 0032-x86-cet-shstk-Introduce-map_shadow_stack-syscall.patch
 create mode 100644 0033-selftests-x86-Add-map_shadow_stack-syscall-test.patch
 create mode 100644 0034-x86-cet-shstk-Support-wrss-for-userspace.patch
 create mode 100644 0035-x86-cpufeatures-Limit-shadow-stack-to-Intel-CPUs.patch
 create mode 100644 0036-x86-cet-Clear-thread.shstk-and-feat_prctl_locked.patch
 create mode 100644 0037-x86-cet-Add-PTRACE-interface-for-CET.patch
 create mode 100644 0038-powerpc-Keep-.rela-sections-when-CONFIG_RELOCATABLE-.patch
 create mode 100644 0039-Discard-.note.gnu.property-sections-in-generic-NOTES.patch

diff --git a/0001-Documentation-x86-Add-CET-description.patch b/0001-Documentation-x86-Add-CET-description.patch
new file mode 100644
index 000000000..dd2d835f5
--- /dev/null
+++ b/0001-Documentation-x86-Add-CET-description.patch
@@ -0,0 +1,183 @@
+From 3514ed2bec3c5fea500c64fc9163600888e73fb5 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:14:57 -0700
+Subject: [PATCH 01/39] Documentation/x86: Add CET description
+
+Introduce a new document on Control-flow Enforcement Technology (CET).
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Cc: Kees Cook <keescook@chromium.org>
+Link: https://lore.kernel.org/r/20210830181528.1569-2-yu-cheng.yu@intel.com
+
+===
+
+v1:
+Moved kernel parameters documentation to other patch
+---
+ Documentation/x86/cet.rst   | 141 ++++++++++++++++++++++++++++++++++++
+ Documentation/x86/index.rst |   1 +
+ 2 files changed, 142 insertions(+)
+ create mode 100644 Documentation/x86/cet.rst
+
+diff --git a/Documentation/x86/cet.rst b/Documentation/x86/cet.rst
+new file mode 100644
+index 000000000000..18e314bac92e
+--- /dev/null
++++ b/Documentation/x86/cet.rst
+@@ -0,0 +1,141 @@
++.. SPDX-License-Identifier: GPL-2.0
++
++=========================================
++Control-flow Enforcement Technology (CET)
++=========================================
++
++[1] Overview
++============
++
++Control-flow Enforcement Technology (CET) is an x86 processor feature
++that provides protection against return/jump-oriented programming (ROP)
++attacks. The HW feature itself can be set up to protect both applications
++and the kernel. Only user-mode protection is implemented in the 64-bit
++kernel, including shadow stack support for running legacy 32-bit
++applications.
++
++CET introduces Shadow Stack and Indirect Branch Tracking. Shadow stack is
++a secondary stack allocated from memory and cannot be directly modified by
++applications. When executing a CALL instruction, the processor pushes the
++return address to both the normal stack and the shadow stack. Upon
++function return, the processor pops the shadow stack copy and compares it
++to the normal stack copy. If the two differ, the processor raises a
++control-protection fault. Indirect branch tracking verifies indirect
++CALL/JMP targets are intended as marked by the compiler with 'ENDBR'
++opcodes. Not all CPU's have both Shadow Stack and Indirect Branch Tracking
++and only Shadow Stack is currently supported in the kernel.
++
++The Kconfig options is X86_SHADOW_STACK, and it can be disabled with
++no_user_shstk.
++
++To build a CET-enabled kernel, Binutils v2.31 and GCC v8.1 or LLVM v10.0.1
++or later are required. To build a CET-enabled application, GLIBC v2.28 or
++later is also required.
++
++At run time, /proc/cpuinfo shows CET features if the processor supports
++CET.
++
++[2] Application Enabling
++========================
++
++An application's CET capability is marked in its ELF header and can be
++verified from readelf/llvm-readelf output:
++
++    readelf -n <application> | grep -a SHSTK2
++        properties: x86 feature: WRSS, SHSTK2
++
++The CET related elf features are IBT, SHSTK, SHSTK2 and WRSS. currently
++only "SHSTK2" and "WRSS" are suppored by the kernel. "SHSTK" binaries
++target a never upstream kernel ABI and so its presense does not result
++in Shadow Stack support getting enabled for the binary. IBT is currently
++not supported.
++
++[3] Backward Compatibility
++==========================
++
++GLIBC provides a few CET tunables via the GLIBC_TUNABLES environment
++variable:
++
++GLIBC_TUNABLES=glibc.tune.hwcaps=-SHSTK2,-WRSS
++    Turn off SHSTK2/WRSS.
++
++GLIBC_TUNABLES=glibc.tune.x86_shstk=<on, permissive>
++    This controls how dlopen() handles SHSTK2 legacy libraries::
++
++        on         - continue with SHSTK2 enabled;
++        permissive - continue with SHSTK2 off.
++
++Details can be found in the GLIBC manual pages.
++
++[4] CET arch_prctl()'s
++======================
++
++Elf features are enabled automatically if found in the binary header, but
++their enablement state can also be controlled using the below arch_prctl's.
++
++arch_prctl(ARCH_X86_FEATURE_1_STATUS, u64 *args)
++    Get feature status.
++
++    The parameter 'args' is a pointer to a user buffer. The kernel returns
++    the following information:
++
++    *args = elf feature enable state (GNU_PROPERTY_X86_FEATURE_1_FOO bits)
++    *(args + 1) = shadow stack base address
++    *(args + 2) = shadow stack size
++
++    32-bit binaries use the same interface, but only lower 32-bits of each
++    item.
++
++arch_prctl(ARCH_X86_FEATURE_1_DISABLE, unsigned int features)
++    Disable features specified in 'features'. Return -EPERM if any of the
++    passed feature are locked.
++
++arch_ptrtl(ARCH_X86_FEATURE_1_ENABLE, unsigned int features)
++    Enable feature specified in 'features'. Return -EPERM if any of the
++    passed feature are locked.
++
++arch_prctl(ARCH_X86_FEATURE_1_LOCK, unsigned int features)
++    Lock in all features at their current enabled or disabled status.
++
++Note:
++  Currently only WRSS can be enabled with ARCH_X86_FEATURE_1_ENABLE,
++  SHSTK2 is enabled automatically if found in the elf header.
++
++[5] The implementation of the Shadow Stack
++==========================================
++
++Shadow Stack size
++-----------------
++
++A task's shadow stack is allocated from memory to a fixed size of
++MIN(RLIMIT_STACK, 4 GB). In other words, the shadow stack is allocated to
++the maximum size of the normal stack, but capped to 4 GB. However,
++a compat-mode application's address space is smaller, each of its thread's
++shadow stack size is MIN(1/4 RLIMIT_STACK, 4 GB).
++
++Signal
++------
++
++The main program and its signal handlers use the same shadow stack.
++Because the shadow stack stores only return addresses, a large shadow
++stack covers the condition that both the program stack and the signal
++alternate stack run out.
++
++The kernel creates a restore token for the shadow stack restoring address
++and verifies that token when restoring from the signal handler.
++
++Fork
++----
++
++The shadow stack's vma has VM_SHADOW_STACK flag set; its PTEs are required
++to be read-only and dirty. When a shadow stack PTE is not RO and dirty, a
++shadow access triggers a page fault with the shadow stack access bit set
++in the page fault error code.
++
++When a task forks a child, its shadow stack PTEs are copied and both the
++parent's and the child's shadow stack PTEs are cleared of the dirty bit.
++Upon the next shadow stack access, the resulting shadow stack page fault
++is handled by page copy/re-use.
++
++When a pthread child is created, the kernel allocates a new shadow stack
++for the new thread.
+diff --git a/Documentation/x86/index.rst b/Documentation/x86/index.rst
+index f498f1d36cd3..b5f083a61eab 100644
+--- a/Documentation/x86/index.rst
++++ b/Documentation/x86/index.rst
+@@ -21,6 +21,7 @@ x86-specific Documentation
+    tlb
+    mtrr
+    pat
++   cet
+    intel-iommu
+    intel_txt
+    amd-memory-encryption
+-- 
+2.34.1
+
diff --git a/0002-x86-cet-shstk-Add-Kconfig-option-for-Shadow-Stack.patch b/0002-x86-cet-shstk-Add-Kconfig-option-for-Shadow-Stack.patch
new file mode 100644
index 000000000..a61d45dff
--- /dev/null
+++ b/0002-x86-cet-shstk-Add-Kconfig-option-for-Shadow-Stack.patch
@@ -0,0 +1,83 @@
+From 13ac82b892c18996c3320bcfd8caca290f95cee5 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:14:58 -0700
+Subject: [PATCH 02/39] x86/cet/shstk: Add Kconfig option for Shadow Stack
+
+Shadow Stack provides protection against function return address
+corruption.  It is active when the processor supports it, the kernel has
+CONFIG_X86_SHADOW_STACK enabled, and the application is built for the
+feature.  This is only implemented for the 64-bit kernel.  When it is
+enabled, legacy non-Shadow Stack applications continue to work, but without
+protection.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Cc: Kees Cook <keescook@chromium.org>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+===
+
+Yu-cheng v25:
+- Remove X86_CET and use X86_SHADOW_STACK directly.
+
+Yu-cheng v24:
+- Update for the splitting X86_CET to X86_SHADOW_STACK and X86_IBT.
+---
+ arch/x86/Kconfig           | 22 ++++++++++++++++++++++
+ arch/x86/Kconfig.assembler |  5 +++++
+ 2 files changed, 27 insertions(+)
+
+diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
+index 5c2ccb85f2ef..b82c3f61ab23 100644
+--- a/arch/x86/Kconfig
++++ b/arch/x86/Kconfig
+@@ -26,6 +26,7 @@ config X86_64
+ 	depends on 64BIT
+ 	# Options that are inherently 64-bit kernel only:
+ 	select ARCH_HAS_GIGANTIC_PAGE
++	select ARCH_HAS_SHADOW_STACK
+ 	select ARCH_SUPPORTS_INT128 if CC_HAS_INT128
+ 	select ARCH_USE_CMPXCHG_LOCKREF
+ 	select HAVE_ARCH_SOFT_DIRTY
+@@ -1927,6 +1928,27 @@ config X86_SGX
+ 
+ 	  If unsure, say N.
+ 
++config ARCH_HAS_SHADOW_STACK
++	def_bool n
++
++config X86_SHADOW_STACK
++	prompt "Intel Shadow Stack"
++	def_bool n
++	depends on AS_WRUSS
++	depends on ARCH_HAS_SHADOW_STACK
++	select ARCH_USES_HIGH_VMA_FLAGS
++	help
++	  Shadow Stack protection is a hardware feature that detects function
++	  return address corruption.  This helps mitigate ROP attacks.
++	  Applications must be enabled to use it, and old userspace does not
++	  get protection "for free".
++	  Support for this feature is present on Tiger Lake family of
++	  processors released in 2020 or later.  Enabling this feature
++	  increases kernel text size by 3.7 KB.
++	  See Documentation/x86/intel_cet.rst for more information.
++
++	  If unsure, say N.
++
+ config EFI
+ 	bool "EFI runtime service support"
+ 	depends on ACPI
+diff --git a/arch/x86/Kconfig.assembler b/arch/x86/Kconfig.assembler
+index 26b8c08e2fc4..00c79dd93651 100644
+--- a/arch/x86/Kconfig.assembler
++++ b/arch/x86/Kconfig.assembler
+@@ -19,3 +19,8 @@ config AS_TPAUSE
+ 	def_bool $(as-instr,tpause %ecx)
+ 	help
+ 	  Supported by binutils >= 2.31.1 and LLVM integrated assembler >= V7
++
++config AS_WRUSS
++	def_bool $(as-instr,wrussq %rax$(comma)(%rbx))
++	help
++	  Supported by binutils >= 2.31 and LLVM integrated assembler
+-- 
+2.34.1
+
diff --git a/0003-x86-cpufeatures-Add-CET-CPU-feature-flags-for-Contro.patch b/0003-x86-cpufeatures-Add-CET-CPU-feature-flags-for-Contro.patch
new file mode 100644
index 000000000..65e3c1a48
--- /dev/null
+++ b/0003-x86-cpufeatures-Add-CET-CPU-feature-flags-for-Contro.patch
@@ -0,0 +1,82 @@
+From 6c67337dacb4a9c0636473d3e71e81aa381228e9 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:14:59 -0700
+Subject: [PATCH 03/39] x86/cpufeatures: Add CET CPU feature flags for
+ Control-flow Enforcement Technology (CET)
+
+Add CPU feature flags for Control-flow Enforcement Technology (CET).
+
+CPUID.(EAX=7,ECX=0):ECX[bit 7] Shadow stack
+CPUID.(EAX=7,ECX=0):EDX[bit 20] Indirect Branch Tracking
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Cc: Kees Cook <keescook@chromium.org>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+
+===
+
+Yu-cheng v25:
+- Make X86_FEATURE_IBT depend on X86_FEATURE_SHSTK.
+
+Yu-cheng v24:
+- Update for splitting CONFIG_X86_CET to CONFIG_X86_SHADOW_STACK and CONFIG_X86_IBT.
+- Move DISABLE_IBT definition to the IBT series.
+---
+ arch/x86/include/asm/cpufeatures.h       | 1 +
+ arch/x86/include/asm/disabled-features.h | 8 +++++++-
+ arch/x86/kernel/cpu/cpuid-deps.c         | 1 +
+ 3 files changed, 9 insertions(+), 1 deletion(-)
+
+diff --git a/arch/x86/include/asm/cpufeatures.h b/arch/x86/include/asm/cpufeatures.h
+index d5b5f2ab87a0..5120ef01e765 100644
+--- a/arch/x86/include/asm/cpufeatures.h
++++ b/arch/x86/include/asm/cpufeatures.h
+@@ -352,6 +352,7 @@
+ #define X86_FEATURE_OSPKE		(16*32+ 4) /* OS Protection Keys Enable */
+ #define X86_FEATURE_WAITPKG		(16*32+ 5) /* UMONITOR/UMWAIT/TPAUSE Instructions */
+ #define X86_FEATURE_AVX512_VBMI2	(16*32+ 6) /* Additional AVX512 Vector Bit Manipulation Instructions */
++#define X86_FEATURE_SHSTK		(16*32+ 7) /* Shadow Stack */
+ #define X86_FEATURE_GFNI		(16*32+ 8) /* Galois Field New Instructions */
+ #define X86_FEATURE_VAES		(16*32+ 9) /* Vector AES */
+ #define X86_FEATURE_VPCLMULQDQ		(16*32+10) /* Carry-Less Multiplication Double Quadword */
+diff --git a/arch/x86/include/asm/disabled-features.h b/arch/x86/include/asm/disabled-features.h
+index 8f28fafa98b3..b7728f7afb2b 100644
+--- a/arch/x86/include/asm/disabled-features.h
++++ b/arch/x86/include/asm/disabled-features.h
+@@ -65,6 +65,12 @@
+ # define DISABLE_SGX	(1 << (X86_FEATURE_SGX & 31))
+ #endif
+ 
++#ifdef CONFIG_X86_SHADOW_STACK
++#define DISABLE_SHSTK	0
++#else
++#define DISABLE_SHSTK	(1 << (X86_FEATURE_SHSTK & 31))
++#endif
++
+ /*
+  * Make sure to add features to the correct mask
+  */
+@@ -85,7 +91,7 @@
+ #define DISABLED_MASK14	0
+ #define DISABLED_MASK15	0
+ #define DISABLED_MASK16	(DISABLE_PKU|DISABLE_OSPKE|DISABLE_LA57|DISABLE_UMIP| \
+-			 DISABLE_ENQCMD)
++			 DISABLE_ENQCMD|DISABLE_SHSTK)
+ #define DISABLED_MASK17	0
+ #define DISABLED_MASK18	0
+ #define DISABLED_MASK19	0
+diff --git a/arch/x86/kernel/cpu/cpuid-deps.c b/arch/x86/kernel/cpu/cpuid-deps.c
+index c881bcafba7d..bf1b55a1ba21 100644
+--- a/arch/x86/kernel/cpu/cpuid-deps.c
++++ b/arch/x86/kernel/cpu/cpuid-deps.c
+@@ -78,6 +78,7 @@ static const struct cpuid_dep cpuid_deps[] = {
+ 	{ X86_FEATURE_XFD,			X86_FEATURE_XSAVES    },
+ 	{ X86_FEATURE_XFD,			X86_FEATURE_XGETBV1   },
+ 	{ X86_FEATURE_AMX_TILE,			X86_FEATURE_XFD       },
++	{ X86_FEATURE_SHSTK,			X86_FEATURE_XSAVES    },
+ 	{}
+ };
+ 
+-- 
+2.34.1
+
diff --git a/0004-x86-cpufeatures-Introduce-CPU-setup-and-option-parsi.patch b/0004-x86-cpufeatures-Introduce-CPU-setup-and-option-parsi.patch
new file mode 100644
index 000000000..a0e257bc3
--- /dev/null
+++ b/0004-x86-cpufeatures-Introduce-CPU-setup-and-option-parsi.patch
@@ -0,0 +1,97 @@
+From f080ccbca93e38fa3a760ba79d279eaabcd9ab40 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:00 -0700
+Subject: [PATCH 04/39] x86/cpufeatures: Introduce CPU setup and option parsing
+ for CET
+
+Introduce CPU setup and boot option parsing for CET features.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Cc: Kees Cook <keescook@chromium.org>
+Link: https://lore.kernel.org/r/20210830181528.1569-5-yu-cheng.yu@intel.com
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+
+===
+
+v1:
+Moved kernel-parameters.txt changes to separate patch
+
+Yu-cheng v25:
+- Remove software-defined X86_FEATURE_CET.
+
+Yu-cheng v24:
+- Update #ifdef placement to reflect Kconfig changes of splitting shadow stack and ibt.
+---
+ Documentation/admin-guide/kernel-parameters.txt |  4 ++++
+ arch/x86/include/uapi/asm/processor-flags.h     |  2 ++
+ arch/x86/kernel/cpu/common.c                    | 12 ++++++++++++
+ 3 files changed, 18 insertions(+)
+
+diff --git a/Documentation/admin-guide/kernel-parameters.txt b/Documentation/admin-guide/kernel-parameters.txt
+index 2fba82431efb..1c776599e3d3 100644
+--- a/Documentation/admin-guide/kernel-parameters.txt
++++ b/Documentation/admin-guide/kernel-parameters.txt
+@@ -3380,6 +3380,10 @@
+ 			noexec=on: enable non-executable mappings (default)
+ 			noexec=off: disable non-executable mappings
+ 
++	no_user_shstk	[X86-64] Disable Shadow Stack for user-mode
++			applications.  Disabling shadow stack also disables
++			IBT.
++
+ 	nosmap		[X86,PPC]
+ 			Disable SMAP (Supervisor Mode Access Prevention)
+ 			even if it is supported by processor.
+diff --git a/arch/x86/include/uapi/asm/processor-flags.h b/arch/x86/include/uapi/asm/processor-flags.h
+index bcba3c643e63..a8df907e8017 100644
+--- a/arch/x86/include/uapi/asm/processor-flags.h
++++ b/arch/x86/include/uapi/asm/processor-flags.h
+@@ -130,6 +130,8 @@
+ #define X86_CR4_SMAP		_BITUL(X86_CR4_SMAP_BIT)
+ #define X86_CR4_PKE_BIT		22 /* enable Protection Keys support */
+ #define X86_CR4_PKE		_BITUL(X86_CR4_PKE_BIT)
++#define X86_CR4_CET_BIT		23 /* enable Control-flow Enforcement */
++#define X86_CR4_CET		_BITUL(X86_CR4_CET_BIT)
+ 
+ /*
+  * x86-64 Task Priority Register, CR8
+diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
+index 0083464de5e3..e92a27509306 100644
+--- a/arch/x86/kernel/cpu/common.c
++++ b/arch/x86/kernel/cpu/common.c
+@@ -515,6 +515,14 @@ static __init int setup_disable_pku(char *arg)
+ __setup("nopku", setup_disable_pku);
+ #endif /* CONFIG_X86_64 */
+ 
++static __always_inline void setup_cet(struct cpuinfo_x86 *c)
++{
++	if (!cpu_feature_enabled(X86_FEATURE_SHSTK))
++		return;
++
++	cr4_set_bits(X86_CR4_CET);
++}
++
+ /*
+  * Some CPU features depend on higher CPUID levels, which may not always
+  * be available due to CPUID level capping or broken virtualization
+@@ -1261,6 +1269,9 @@ static void __init cpu_parse_early_param(void)
+ 	if (cmdline_find_option_bool(boot_command_line, "noxsaves"))
+ 		setup_clear_cpu_cap(X86_FEATURE_XSAVES);
+ 
++	if (cmdline_find_option_bool(boot_command_line, "no_user_shstk"))
++		setup_clear_cpu_cap(X86_FEATURE_SHSTK);
++
+ 	arglen = cmdline_find_option(boot_command_line, "clearcpuid", arg, sizeof(arg));
+ 	if (arglen <= 0)
+ 		return;
+@@ -1632,6 +1643,7 @@ static void identify_cpu(struct cpuinfo_x86 *c)
+ 
+ 	x86_init_rdrand(c);
+ 	setup_pku(c);
++	setup_cet(c);
+ 
+ 	/*
+ 	 * Clear/Set all flags overridden by options, need do it
+-- 
+2.34.1
+
diff --git a/0005-x86-fpu-xstate-Introduce-CET-MSR-and-XSAVES-supervis.patch b/0005-x86-fpu-xstate-Introduce-CET-MSR-and-XSAVES-supervis.patch
new file mode 100644
index 000000000..c7d06bfaa
--- /dev/null
+++ b/0005-x86-fpu-xstate-Introduce-CET-MSR-and-XSAVES-supervis.patch
@@ -0,0 +1,212 @@
+From 09638056926b3d7cfed3fde3303d899b4391639b Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:01 -0700
+Subject: [PATCH 05/39] x86/fpu/xstate: Introduce CET MSR and XSAVES supervisor
+ states
+
+Control-flow Enforcement Technology (CET) introduces these MSRs:
+
+    MSR_IA32_U_CET (user-mode CET settings),
+    MSR_IA32_PL3_SSP (user-mode shadow stack pointer),
+
+    MSR_IA32_PL0_SSP (kernel-mode shadow stack pointer),
+    MSR_IA32_PL1_SSP (Privilege Level 1 shadow stack pointer),
+    MSR_IA32_PL2_SSP (Privilege Level 2 shadow stack pointer),
+    MSR_IA32_S_CET (kernel-mode CET settings),
+    MSR_IA32_INT_SSP_TAB (exception shadow stack table).
+
+The two user-mode MSRs belong to XFEATURE_CET_USER.  The first three of
+kernel-mode MSRs belong to XFEATURE_CET_KERNEL.  Both XSAVES states are
+supervisor states.  This means that there is no direct, unprivileged access
+to these states, making it harder for an attacker to subvert CET.
+
+For sigreturn and future ptrace() support, shadow stack address and MSR
+reserved bits are checked before written to the supervisor states.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Cc: Kees Cook <keescook@chromium.org>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+
+===
+
+Yu-cheng v29:
+- Move CET MSR definition up in msr-index.h.
+
+Yu-cheng v28:
+- Add XFEATURE_MASK_CET_USER to XFEATURES_INIT_FPSTATE_HANDLED.
+
+Yu-cheng v25:
+- Update xsave_cpuid_features[].  Now CET XSAVES features depend on
+  X86_FEATURE_SHSTK (vs. the software-defined X86_FEATURE_CET).
+---
+ arch/x86/include/asm/fpu/types.h  | 23 +++++++++++++++++++++--
+ arch/x86/include/asm/fpu/xstate.h |  6 ++++--
+ arch/x86/include/asm/msr-index.h  | 20 ++++++++++++++++++++
+ arch/x86/kernel/fpu/xstate.c      | 13 ++++++++++++-
+ 4 files changed, 57 insertions(+), 5 deletions(-)
+
+diff --git a/arch/x86/include/asm/fpu/types.h b/arch/x86/include/asm/fpu/types.h
+index 3c06c82ab355..ecfdd2c4a586 100644
+--- a/arch/x86/include/asm/fpu/types.h
++++ b/arch/x86/include/asm/fpu/types.h
+@@ -115,8 +115,8 @@ enum xfeature {
+ 	XFEATURE_PT_UNIMPLEMENTED_SO_FAR,
+ 	XFEATURE_PKRU,
+ 	XFEATURE_PASID,
+-	XFEATURE_RSRVD_COMP_11,
+-	XFEATURE_RSRVD_COMP_12,
++	XFEATURE_CET_USER,
++	XFEATURE_CET_KERNEL,
+ 	XFEATURE_RSRVD_COMP_13,
+ 	XFEATURE_RSRVD_COMP_14,
+ 	XFEATURE_LBR,
+@@ -138,6 +138,8 @@ enum xfeature {
+ #define XFEATURE_MASK_PT		(1 << XFEATURE_PT_UNIMPLEMENTED_SO_FAR)
+ #define XFEATURE_MASK_PKRU		(1 << XFEATURE_PKRU)
+ #define XFEATURE_MASK_PASID		(1 << XFEATURE_PASID)
++#define XFEATURE_MASK_CET_USER		(1 << XFEATURE_CET_USER)
++#define XFEATURE_MASK_CET_KERNEL	(1 << XFEATURE_CET_KERNEL)
+ #define XFEATURE_MASK_LBR		(1 << XFEATURE_LBR)
+ #define XFEATURE_MASK_XTILE_CFG		(1 << XFEATURE_XTILE_CFG)
+ #define XFEATURE_MASK_XTILE_DATA	(1 << XFEATURE_XTILE_DATA)
+@@ -252,6 +254,23 @@ struct pkru_state {
+ 	u32				pad;
+ } __packed;
+ 
++/*
++ * State component 11 is Control-flow Enforcement user states
++ */
++struct cet_user_state {
++	u64 user_cet;			/* user control-flow settings */
++	u64 user_ssp;			/* user shadow stack pointer */
++};
++
++/*
++ * State component 12 is Control-flow Enforcement kernel states
++ */
++struct cet_kernel_state {
++	u64 kernel_ssp;			/* kernel shadow stack */
++	u64 pl1_ssp;			/* privilege level 1 shadow stack */
++	u64 pl2_ssp;			/* privilege level 2 shadow stack */
++};
++
+ /*
+  * State component 15: Architectural LBR configuration state.
+  * The size of Arch LBR state depends on the number of LBRs (lbr_depth).
+diff --git a/arch/x86/include/asm/fpu/xstate.h b/arch/x86/include/asm/fpu/xstate.h
+index cd3dd170e23a..d4427b88ee12 100644
+--- a/arch/x86/include/asm/fpu/xstate.h
++++ b/arch/x86/include/asm/fpu/xstate.h
+@@ -50,7 +50,8 @@
+ #define XFEATURE_MASK_USER_DYNAMIC	XFEATURE_MASK_XTILE_DATA
+ 
+ /* All currently supported supervisor features */
+-#define XFEATURE_MASK_SUPERVISOR_SUPPORTED (XFEATURE_MASK_PASID)
++#define XFEATURE_MASK_SUPERVISOR_SUPPORTED (XFEATURE_MASK_PASID | \
++					    XFEATURE_MASK_CET_USER)
+ 
+ /*
+  * A supervisor state component may not always contain valuable information,
+@@ -77,7 +78,8 @@
+  * Unsupported supervisor features. When a supervisor feature in this mask is
+  * supported in the future, move it to the supported supervisor feature mask.
+  */
+-#define XFEATURE_MASK_SUPERVISOR_UNSUPPORTED (XFEATURE_MASK_PT)
++#define XFEATURE_MASK_SUPERVISOR_UNSUPPORTED (XFEATURE_MASK_PT | \
++					      XFEATURE_MASK_CET_KERNEL)
+ 
+ /* All supervisor states including supported and unsupported states. */
+ #define XFEATURE_MASK_SUPERVISOR_ALL (XFEATURE_MASK_SUPERVISOR_SUPPORTED | \
+diff --git a/arch/x86/include/asm/msr-index.h b/arch/x86/include/asm/msr-index.h
+index 01e2650b9585..b757e0397f26 100644
+--- a/arch/x86/include/asm/msr-index.h
++++ b/arch/x86/include/asm/msr-index.h
+@@ -362,6 +362,26 @@
+ 
+ 
+ #define MSR_CORE_PERF_LIMIT_REASONS	0x00000690
++
++/* Control-flow Enforcement Technology MSRs */
++#define MSR_IA32_U_CET			0x000006a0 /* user mode cet setting */
++#define MSR_IA32_S_CET			0x000006a2 /* kernel mode cet setting */
++#define CET_SHSTK_EN			BIT_ULL(0)
++#define CET_WRSS_EN			BIT_ULL(1)
++#define CET_ENDBR_EN			BIT_ULL(2)
++#define CET_LEG_IW_EN			BIT_ULL(3)
++#define CET_NO_TRACK_EN			BIT_ULL(4)
++#define CET_SUPPRESS_DISABLE		BIT_ULL(5)
++#define CET_RESERVED			(BIT_ULL(6) | BIT_ULL(7) | BIT_ULL(8) | BIT_ULL(9))
++#define CET_SUPPRESS			BIT_ULL(10)
++#define CET_WAIT_ENDBR			BIT_ULL(11)
++
++#define MSR_IA32_PL0_SSP		0x000006a4 /* kernel shadow stack pointer */
++#define MSR_IA32_PL1_SSP		0x000006a5 /* ring-1 shadow stack pointer */
++#define MSR_IA32_PL2_SSP		0x000006a6 /* ring-2 shadow stack pointer */
++#define MSR_IA32_PL3_SSP		0x000006a7 /* user shadow stack pointer */
++#define MSR_IA32_INT_SSP_TAB		0x000006a8 /* exception shadow stack table */
++
+ #define MSR_GFX_PERF_LIMIT_REASONS	0x000006B0
+ #define MSR_RING_PERF_LIMIT_REASONS	0x000006B1
+ 
+diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c
+index d28829403ed0..1355b7119afe 100644
+--- a/arch/x86/kernel/fpu/xstate.c
++++ b/arch/x86/kernel/fpu/xstate.c
+@@ -50,6 +50,8 @@ static const char *xfeature_names[] =
+ 	"Processor Trace (unused)"	,
+ 	"Protection Keys User registers",
+ 	"PASID state",
++	"Control-flow User registers"	,
++	"Control-flow Kernel registers"	,
+ 	"unknown xstate feature"	,
+ 	"unknown xstate feature"	,
+ 	"unknown xstate feature"	,
+@@ -73,6 +75,8 @@ static unsigned short xsave_cpuid_features[] __initdata = {
+ 	[XFEATURE_PT_UNIMPLEMENTED_SO_FAR]	= X86_FEATURE_INTEL_PT,
+ 	[XFEATURE_PKRU]				= X86_FEATURE_PKU,
+ 	[XFEATURE_PASID]			= X86_FEATURE_ENQCMD,
++	[XFEATURE_CET_USER]			= X86_FEATURE_SHSTK,
++	[XFEATURE_CET_KERNEL]			= X86_FEATURE_SHSTK,
+ 	[XFEATURE_XTILE_CFG]			= X86_FEATURE_AMX_TILE,
+ 	[XFEATURE_XTILE_DATA]			= X86_FEATURE_AMX_TILE,
+ };
+@@ -250,6 +254,8 @@ static void __init print_xstate_features(void)
+ 	print_xstate_feature(XFEATURE_MASK_Hi16_ZMM);
+ 	print_xstate_feature(XFEATURE_MASK_PKRU);
+ 	print_xstate_feature(XFEATURE_MASK_PASID);
++	print_xstate_feature(XFEATURE_MASK_CET_USER);
++	print_xstate_feature(XFEATURE_MASK_CET_KERNEL);
+ 	print_xstate_feature(XFEATURE_MASK_XTILE_CFG);
+ 	print_xstate_feature(XFEATURE_MASK_XTILE_DATA);
+ }
+@@ -405,6 +411,7 @@ static __init void os_xrstor_booting(struct xregs_state *xstate)
+ 	 XFEATURE_MASK_BNDREGS |		\
+ 	 XFEATURE_MASK_BNDCSR |			\
+ 	 XFEATURE_MASK_PASID |			\
++	 XFEATURE_MASK_CET_USER |		\
+ 	 XFEATURE_MASK_XTILE)
+ 
+ /*
+@@ -621,6 +628,8 @@ static bool __init check_xstate_against_struct(int nr)
+ 	XCHECK_SZ(sz, nr, XFEATURE_PKRU,      struct pkru_state);
+ 	XCHECK_SZ(sz, nr, XFEATURE_PASID,     struct ia32_pasid_state);
+ 	XCHECK_SZ(sz, nr, XFEATURE_XTILE_CFG, struct xtile_cfg);
++	XCHECK_SZ(sz, nr, XFEATURE_CET_USER,   struct cet_user_state);
++	XCHECK_SZ(sz, nr, XFEATURE_CET_KERNEL, struct cet_kernel_state);
+ 
+ 	/* The tile data size varies between implementations. */
+ 	if (nr == XFEATURE_XTILE_DATA)
+@@ -634,7 +643,9 @@ static bool __init check_xstate_against_struct(int nr)
+ 	if ((nr < XFEATURE_YMM) ||
+ 	    (nr >= XFEATURE_MAX) ||
+ 	    (nr == XFEATURE_PT_UNIMPLEMENTED_SO_FAR) ||
+-	    ((nr >= XFEATURE_RSRVD_COMP_11) && (nr <= XFEATURE_RSRVD_COMP_16))) {
++	    (nr == XFEATURE_RSRVD_COMP_13) ||
++	    (nr == XFEATURE_RSRVD_COMP_14) ||
++	    (nr == XFEATURE_RSRVD_COMP_16)) {
+ 		WARN_ONCE(1, "no structure for xstate: %d\n", nr);
+ 		XSTATE_WARN_ON(1);
+ 		return false;
+-- 
+2.34.1
+
diff --git a/0006-x86-cet-Add-control-protection-fault-handler.patch b/0006-x86-cet-Add-control-protection-fault-handler.patch
new file mode 100644
index 000000000..64dcd7d61
--- /dev/null
+++ b/0006-x86-cet-Add-control-protection-fault-handler.patch
@@ -0,0 +1,247 @@
+From 5122cda3f9700b56753ae184c3d6c3148fa36ecb Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:02 -0700
+Subject: [PATCH 06/39] x86/cet: Add control-protection fault handler
+
+A control-protection fault is triggered when a control-flow transfer
+attempt violates Shadow Stack or Indirect Branch Tracking constraints.
+For example, the return address for a RET instruction differs from the copy
+on the shadow stack; or an indirect JMP instruction, without the NOTRACK
+prefix, arrives at a non-ENDBR opcode.
+
+The control-protection fault handler works in a similar way as the general
+protection fault handler.  It provides the si_code SEGV_CPERR to the signal
+handler.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Cc: Kees Cook <keescook@chromium.org>
+Cc: Michael Kerrisk <mtk.manpages@gmail.com>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+
+===
+
+Yu-cheng v29:
+- Remove pr_emerg() since it is followed by die().
+- Change boot_cpu_has() to cpu_feature_enabled().
+
+Yu-cheng v25:
+- Change CONFIG_X86_CET to CONFIG_X86_SHADOW_STACK.
+- Change X86_FEATURE_CET to X86_FEATURE_SHSTK.
+---
+ arch/arm/kernel/signal.c           |  2 +-
+ arch/arm64/kernel/signal.c         |  2 +-
+ arch/arm64/kernel/signal32.c       |  2 +-
+ arch/sparc/kernel/signal32.c       |  2 +-
+ arch/sparc/kernel/signal_64.c      |  2 +-
+ arch/x86/include/asm/idtentry.h    |  4 ++
+ arch/x86/kernel/idt.c              |  4 ++
+ arch/x86/kernel/signal_compat.c    |  2 +-
+ arch/x86/kernel/traps.c            | 62 ++++++++++++++++++++++++++++++
+ include/uapi/asm-generic/siginfo.h |  3 +-
+ 10 files changed, 78 insertions(+), 7 deletions(-)
+
+diff --git a/arch/arm/kernel/signal.c b/arch/arm/kernel/signal.c
+index a41e27ace391..abe65cefa68f 100644
+--- a/arch/arm/kernel/signal.c
++++ b/arch/arm/kernel/signal.c
+@@ -681,7 +681,7 @@ asmlinkage void do_rseq_syscall(struct pt_regs *regs)
+  */
+ static_assert(NSIGILL	== 11);
+ static_assert(NSIGFPE	== 15);
+-static_assert(NSIGSEGV	== 9);
++static_assert(NSIGSEGV	== 10);
+ static_assert(NSIGBUS	== 5);
+ static_assert(NSIGTRAP	== 6);
+ static_assert(NSIGCHLD	== 6);
+diff --git a/arch/arm64/kernel/signal.c b/arch/arm64/kernel/signal.c
+index 8f6372b44b65..db0834e03f66 100644
+--- a/arch/arm64/kernel/signal.c
++++ b/arch/arm64/kernel/signal.c
+@@ -983,7 +983,7 @@ void __init minsigstksz_setup(void)
+  */
+ static_assert(NSIGILL	== 11);
+ static_assert(NSIGFPE	== 15);
+-static_assert(NSIGSEGV	== 9);
++static_assert(NSIGSEGV	== 10);
+ static_assert(NSIGBUS	== 5);
+ static_assert(NSIGTRAP	== 6);
+ static_assert(NSIGCHLD	== 6);
+diff --git a/arch/arm64/kernel/signal32.c b/arch/arm64/kernel/signal32.c
+index d984282b979f..8776a34c6444 100644
+--- a/arch/arm64/kernel/signal32.c
++++ b/arch/arm64/kernel/signal32.c
+@@ -460,7 +460,7 @@ void compat_setup_restart_syscall(struct pt_regs *regs)
+  */
+ static_assert(NSIGILL	== 11);
+ static_assert(NSIGFPE	== 15);
+-static_assert(NSIGSEGV	== 9);
++static_assert(NSIGSEGV	== 10);
+ static_assert(NSIGBUS	== 5);
+ static_assert(NSIGTRAP	== 6);
+ static_assert(NSIGCHLD	== 6);
+diff --git a/arch/sparc/kernel/signal32.c b/arch/sparc/kernel/signal32.c
+index 6cc124a3bb98..dc50b2a78692 100644
+--- a/arch/sparc/kernel/signal32.c
++++ b/arch/sparc/kernel/signal32.c
+@@ -752,7 +752,7 @@ asmlinkage int do_sys32_sigstack(u32 u_ssptr, u32 u_ossptr, unsigned long sp)
+  */
+ static_assert(NSIGILL	== 11);
+ static_assert(NSIGFPE	== 15);
+-static_assert(NSIGSEGV	== 9);
++static_assert(NSIGSEGV	== 10);
+ static_assert(NSIGBUS	== 5);
+ static_assert(NSIGTRAP	== 6);
+ static_assert(NSIGCHLD	== 6);
+diff --git a/arch/sparc/kernel/signal_64.c b/arch/sparc/kernel/signal_64.c
+index 2a78d2af1265..7fe2bd37bd1a 100644
+--- a/arch/sparc/kernel/signal_64.c
++++ b/arch/sparc/kernel/signal_64.c
+@@ -562,7 +562,7 @@ void do_notify_resume(struct pt_regs *regs, unsigned long orig_i0, unsigned long
+  */
+ static_assert(NSIGILL	== 11);
+ static_assert(NSIGFPE	== 15);
+-static_assert(NSIGSEGV	== 9);
++static_assert(NSIGSEGV	== 10);
+ static_assert(NSIGBUS	== 5);
+ static_assert(NSIGTRAP	== 6);
+ static_assert(NSIGCHLD	== 6);
+diff --git a/arch/x86/include/asm/idtentry.h b/arch/x86/include/asm/idtentry.h
+index 1345088e9902..a90791433152 100644
+--- a/arch/x86/include/asm/idtentry.h
++++ b/arch/x86/include/asm/idtentry.h
+@@ -562,6 +562,10 @@ DECLARE_IDTENTRY_ERRORCODE(X86_TRAP_SS,	exc_stack_segment);
+ DECLARE_IDTENTRY_ERRORCODE(X86_TRAP_GP,	exc_general_protection);
+ DECLARE_IDTENTRY_ERRORCODE(X86_TRAP_AC,	exc_alignment_check);
+ 
++#ifdef CONFIG_X86_SHADOW_STACK
++DECLARE_IDTENTRY_ERRORCODE(X86_TRAP_CP, exc_control_protection);
++#endif
++
+ /* Raw exception entries which need extra work */
+ DECLARE_IDTENTRY_RAW(X86_TRAP_UD,		exc_invalid_op);
+ DECLARE_IDTENTRY_RAW(X86_TRAP_BP,		exc_int3);
+diff --git a/arch/x86/kernel/idt.c b/arch/x86/kernel/idt.c
+index df0fa695bb09..9f1bdaabc246 100644
+--- a/arch/x86/kernel/idt.c
++++ b/arch/x86/kernel/idt.c
+@@ -113,6 +113,10 @@ static const __initconst struct idt_data def_idts[] = {
+ #elif defined(CONFIG_X86_32)
+ 	SYSG(IA32_SYSCALL_VECTOR,	entry_INT80_32),
+ #endif
++
++#ifdef CONFIG_X86_SHADOW_STACK
++	INTG(X86_TRAP_CP,		asm_exc_control_protection),
++#endif
+ };
+ 
+ /*
+diff --git a/arch/x86/kernel/signal_compat.c b/arch/x86/kernel/signal_compat.c
+index b52407c56000..ff50cd978ea5 100644
+--- a/arch/x86/kernel/signal_compat.c
++++ b/arch/x86/kernel/signal_compat.c
+@@ -27,7 +27,7 @@ static inline void signal_compat_build_tests(void)
+ 	 */
+ 	BUILD_BUG_ON(NSIGILL  != 11);
+ 	BUILD_BUG_ON(NSIGFPE  != 15);
+-	BUILD_BUG_ON(NSIGSEGV != 9);
++	BUILD_BUG_ON(NSIGSEGV != 10);
+ 	BUILD_BUG_ON(NSIGBUS  != 5);
+ 	BUILD_BUG_ON(NSIGTRAP != 6);
+ 	BUILD_BUG_ON(NSIGCHLD != 6);
+diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
+index c9d566dcf89a..54b7a146fd5e 100644
+--- a/arch/x86/kernel/traps.c
++++ b/arch/x86/kernel/traps.c
+@@ -39,6 +39,7 @@
+ #include <linux/io.h>
+ #include <linux/hardirq.h>
+ #include <linux/atomic.h>
++#include <linux/nospec.h>
+ 
+ #include <asm/stacktrace.h>
+ #include <asm/processor.h>
+@@ -641,6 +642,67 @@ DEFINE_IDTENTRY_ERRORCODE(exc_general_protection)
+ 	cond_local_irq_disable(regs);
+ }
+ 
++#ifdef CONFIG_X86_SHADOW_STACK
++static const char * const control_protection_err[] = {
++	"unknown",
++	"near-ret",
++	"far-ret/iret",
++	"endbranch",
++	"rstorssp",
++	"setssbsy",
++	"unknown",
++};
++
++static DEFINE_RATELIMIT_STATE(cpf_rate, DEFAULT_RATELIMIT_INTERVAL,
++			      DEFAULT_RATELIMIT_BURST);
++
++/*
++ * When a control protection exception occurs, send a signal to the responsible
++ * application.  Currently, control protection is only enabled for user mode.
++ * This exception should not come from kernel mode.
++ */
++DEFINE_IDTENTRY_ERRORCODE(exc_control_protection)
++{
++	struct task_struct *tsk;
++
++	if (!user_mode(regs)) {
++		die("kernel control protection fault", regs, error_code);
++		panic("Unexpected kernel control protection fault.  Machine halted.");
++	}
++
++	cond_local_irq_enable(regs);
++
++	if (!cpu_feature_enabled(X86_FEATURE_SHSTK))
++		WARN_ONCE(1, "Control protection fault with CET support disabled\n");
++
++	tsk = current;
++	tsk->thread.error_code = error_code;
++	tsk->thread.trap_nr = X86_TRAP_CP;
++
++	/*
++	 * Ratelimit to prevent log spamming.
++	 */
++	if (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&
++	    __ratelimit(&cpf_rate)) {
++		unsigned long ssp;
++		int cpf_type;
++
++		cpf_type = array_index_nospec(error_code, ARRAY_SIZE(control_protection_err));
++
++		rdmsrl(MSR_IA32_PL3_SSP, ssp);
++		pr_emerg("%s[%d] control protection ip:%lx sp:%lx ssp:%lx error:%lx(%s)",
++			 tsk->comm, task_pid_nr(tsk),
++			 regs->ip, regs->sp, ssp, error_code,
++			 control_protection_err[cpf_type]);
++		print_vma_addr(KERN_CONT " in ", regs->ip);
++		pr_cont("\n");
++	}
++
++	force_sig_fault(SIGSEGV, SEGV_CPERR, (void __user *)0);
++	cond_local_irq_disable(regs);
++}
++#endif
++
+ static bool do_int3(struct pt_regs *regs)
+ {
+ 	int res;
+diff --git a/include/uapi/asm-generic/siginfo.h b/include/uapi/asm-generic/siginfo.h
+index 3ba180f550d7..081f4b37d22c 100644
+--- a/include/uapi/asm-generic/siginfo.h
++++ b/include/uapi/asm-generic/siginfo.h
+@@ -240,7 +240,8 @@ typedef struct siginfo {
+ #define SEGV_ADIPERR	7	/* Precise MCD exception */
+ #define SEGV_MTEAERR	8	/* Asynchronous ARM MTE error */
+ #define SEGV_MTESERR	9	/* Synchronous ARM MTE exception */
+-#define NSIGSEGV	9
++#define SEGV_CPERR	10	/* Control protection fault */
++#define NSIGSEGV	10
+ 
+ /*
+  * SIGBUS si_codes
+-- 
+2.34.1
+
diff --git a/0007-x86-mm-Remove-_PAGE_DIRTY-from-kernel-RO-pages.patch b/0007-x86-mm-Remove-_PAGE_DIRTY-from-kernel-RO-pages.patch
new file mode 100644
index 000000000..92e57ea27
--- /dev/null
+++ b/0007-x86-mm-Remove-_PAGE_DIRTY-from-kernel-RO-pages.patch
@@ -0,0 +1,67 @@
+From 388aa9ac726e2601b1bf6e19057219a623a73597 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:03 -0700
+Subject: [PATCH 07/39] x86/mm: Remove _PAGE_DIRTY from kernel RO pages
+
+The x86 family of processors do not directly create read-only and Dirty
+PTEs.  These PTEs are created by software.  One such case is that kernel
+read-only pages are historically setup as Dirty.
+
+New processors that support Shadow Stack regard read-only and Dirty PTEs as
+shadow stack pages.  This results in ambiguity between shadow stack and
+kernel read-only pages.  To resolve this, removed Dirty from kernel read-
+only pages.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
+Cc: "H. Peter Anvin" <hpa@zytor.com>
+Cc: Kees Cook <keescook@chromium.org>
+Cc: Thomas Gleixner <tglx@linutronix.de>
+Cc: Dave Hansen <dave.hansen@linux.intel.com>
+Cc: Christoph Hellwig <hch@lst.de>
+Cc: Andy Lutomirski <luto@kernel.org>
+Cc: Ingo Molnar <mingo@redhat.com>
+Cc: Borislav Petkov <bp@alien8.de>
+Cc: Peter Zijlstra <peterz@infradead.org>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+
+===
+---
+ arch/x86/include/asm/pgtable_types.h | 6 +++---
+ arch/x86/mm/pat/set_memory.c         | 2 +-
+ 2 files changed, 4 insertions(+), 4 deletions(-)
+
+diff --git a/arch/x86/include/asm/pgtable_types.h b/arch/x86/include/asm/pgtable_types.h
+index 40497a9020c6..3781a79b6388 100644
+--- a/arch/x86/include/asm/pgtable_types.h
++++ b/arch/x86/include/asm/pgtable_types.h
+@@ -190,10 +190,10 @@ enum page_cache_mode {
+ #define _KERNPG_TABLE		 (__PP|__RW|   0|___A|   0|___D|   0|   0| _ENC)
+ #define _PAGE_TABLE_NOENC	 (__PP|__RW|_USR|___A|   0|___D|   0|   0)
+ #define _PAGE_TABLE		 (__PP|__RW|_USR|___A|   0|___D|   0|   0| _ENC)
+-#define __PAGE_KERNEL_RO	 (__PP|   0|   0|___A|__NX|___D|   0|___G)
+-#define __PAGE_KERNEL_ROX	 (__PP|   0|   0|___A|   0|___D|   0|___G)
++#define __PAGE_KERNEL_RO	 (__PP|   0|   0|___A|__NX|   0|   0|___G)
++#define __PAGE_KERNEL_ROX	 (__PP|   0|   0|___A|   0|   0|   0|___G)
+ #define __PAGE_KERNEL_NOCACHE	 (__PP|__RW|   0|___A|__NX|___D|   0|___G| __NC)
+-#define __PAGE_KERNEL_VVAR	 (__PP|   0|_USR|___A|__NX|___D|   0|___G)
++#define __PAGE_KERNEL_VVAR	 (__PP|   0|_USR|___A|__NX|   0|   0|___G)
+ #define __PAGE_KERNEL_LARGE	 (__PP|__RW|   0|___A|__NX|___D|_PSE|___G)
+ #define __PAGE_KERNEL_LARGE_EXEC (__PP|__RW|   0|___A|   0|___D|_PSE|___G)
+ #define __PAGE_KERNEL_WP	 (__PP|__RW|   0|___A|__NX|___D|   0|___G| __WP)
+diff --git a/arch/x86/mm/pat/set_memory.c b/arch/x86/mm/pat/set_memory.c
+index b4072115c8ef..844bb30280b7 100644
+--- a/arch/x86/mm/pat/set_memory.c
++++ b/arch/x86/mm/pat/set_memory.c
+@@ -1943,7 +1943,7 @@ int set_memory_nx(unsigned long addr, int numpages)
+ 
+ int set_memory_ro(unsigned long addr, int numpages)
+ {
+-	return change_page_attr_clear(&addr, numpages, __pgprot(_PAGE_RW), 0);
++	return change_page_attr_clear(&addr, numpages, __pgprot(_PAGE_RW | _PAGE_DIRTY), 0);
+ }
+ 
+ int set_memory_rw(unsigned long addr, int numpages)
+-- 
+2.34.1
+
diff --git a/0008-x86-mm-Move-pmd_write-pud_write-up-in-the-file.patch b/0008-x86-mm-Move-pmd_write-pud_write-up-in-the-file.patch
new file mode 100644
index 000000000..de4af52ab
--- /dev/null
+++ b/0008-x86-mm-Move-pmd_write-pud_write-up-in-the-file.patch
@@ -0,0 +1,68 @@
+From b75b299102b51fe16e4bbb7a57ecc90a84f0497a Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:04 -0700
+Subject: [PATCH 08/39] x86/mm: Move pmd_write(), pud_write() up in the file
+
+To prepare the introduction of _PAGE_COW, move pmd_write() and
+pud_write() up in the file, so that they can be used by other
+helpers below.  No functional changes.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+---
+ arch/x86/include/asm/pgtable.h | 24 ++++++++++++------------
+ 1 file changed, 12 insertions(+), 12 deletions(-)
+
+diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
+index 448cd01eb3ec..0ddeda0bc0c0 100644
+--- a/arch/x86/include/asm/pgtable.h
++++ b/arch/x86/include/asm/pgtable.h
+@@ -156,6 +156,18 @@ static inline int pte_write(pte_t pte)
+ 	return pte_flags(pte) & _PAGE_RW;
+ }
+ 
++#define pmd_write pmd_write
++static inline int pmd_write(pmd_t pmd)
++{
++	return pmd_flags(pmd) & _PAGE_RW;
++}
++
++#define pud_write pud_write
++static inline int pud_write(pud_t pud)
++{
++	return pud_flags(pud) & _PAGE_RW;
++}
++
+ static inline int pte_huge(pte_t pte)
+ {
+ 	return pte_flags(pte) & _PAGE_PSE;
+@@ -1099,12 +1111,6 @@ extern int pmdp_clear_flush_young(struct vm_area_struct *vma,
+ 				  unsigned long address, pmd_t *pmdp);
+ 
+ 
+-#define pmd_write pmd_write
+-static inline int pmd_write(pmd_t pmd)
+-{
+-	return pmd_flags(pmd) & _PAGE_RW;
+-}
+-
+ #define __HAVE_ARCH_PMDP_HUGE_GET_AND_CLEAR
+ static inline pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm, unsigned long addr,
+ 				       pmd_t *pmdp)
+@@ -1126,12 +1132,6 @@ static inline void pmdp_set_wrprotect(struct mm_struct *mm,
+ 	clear_bit(_PAGE_BIT_RW, (unsigned long *)pmdp);
+ }
+ 
+-#define pud_write pud_write
+-static inline int pud_write(pud_t pud)
+-{
+-	return pud_flags(pud) & _PAGE_RW;
+-}
+-
+ #ifndef pmdp_establish
+ #define pmdp_establish pmdp_establish
+ static inline pmd_t pmdp_establish(struct vm_area_struct *vma,
+-- 
+2.34.1
+
diff --git a/0009-x86-mm-Introduce-_PAGE_COW.patch b/0009-x86-mm-Introduce-_PAGE_COW.patch
new file mode 100644
index 000000000..8ce31163b
--- /dev/null
+++ b/0009-x86-mm-Introduce-_PAGE_COW.patch
@@ -0,0 +1,452 @@
+From bd2da8cacb660bbe6902f336d3cb5e71f92264cb Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:05 -0700
+Subject: [PATCH 09/39] x86/mm: Introduce _PAGE_COW
+
+There is essentially no room left in the x86 hardware PTEs on some OSes
+(not Linux).  That left the hardware architects looking for a way to
+represent a new memory type (shadow stack) within the existing bits.
+They chose to repurpose a lightly-used state: Write=0, Dirty=1.
+
+The reason it's lightly used is that Dirty=1 is normally set by hardware
+and cannot normally be set by hardware on a Write=0 PTE.  Software must
+normally be involved to create one of these PTEs, so software can simply
+opt to not create them.
+
+In places where Linux normally creates Write=0, Dirty=1, it can use the
+software-defined _PAGE_COW in place of the hardware _PAGE_DIRTY.  In other
+words, whenever Linux needs to create Write=0, Dirty=1, it instead creates
+Write=0, Cow=1, except for shadow stack, which is Write=0, Dirty=1.  This
+clearly separates shadow stack from other data, and results in the
+following:
+
+(a) A modified, copy-on-write (COW) page: (Write=0, Cow=1)
+(b) A R/O page that has been COW'ed: (Write=0, Cow=1)
+    The user page is in a R/O VMA, and get_user_pages() needs a writable
+    copy.  The page fault handler creates a copy of the page and sets
+    the new copy's PTE as Write=0 and Cow=1.
+(c) A shadow stack PTE: (Write=0, Dirty=1)
+(d) A shared shadow stack PTE: (Write=0, Cow=1)
+    When a shadow stack page is being shared among processes (this happens
+    at fork()), its PTE is made Dirty=0, so the next shadow stack access
+    causes a fault, and the page is duplicated and Dirty=1 is set again.
+    This is the COW equivalent for shadow stack pages, even though it's
+    copy-on-access rather than copy-on-write.
+(e) A page where the processor observed a Write=1 PTE, started a write, set
+    Dirty=1, but then observed a Write=0 PTE.  That's possible today, but
+    will not happen on processors that support shadow stack.
+
+Define _PAGE_COW and update pte_*() helpers and apply the same changes to
+pmd and pud.
+
+After this, there are six free bits left in the 64-bit PTE, and no more
+free bits in the 32-bit PTE (except for PAE) and Shadow Stack is not
+implemented for the 32-bit kernel.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+---
+ arch/x86/include/asm/pgtable.h       | 196 ++++++++++++++++++++++++---
+ arch/x86/include/asm/pgtable_types.h |  42 +++++-
+ 2 files changed, 217 insertions(+), 21 deletions(-)
+
+diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
+index 0ddeda0bc0c0..9f1ba76ed79a 100644
+--- a/arch/x86/include/asm/pgtable.h
++++ b/arch/x86/include/asm/pgtable.h
+@@ -121,9 +121,20 @@ extern pmdval_t early_pmd_flags;
+  * The following only work if pte_present() is true.
+  * Undefined behaviour if not..
+  */
+-static inline int pte_dirty(pte_t pte)
++static inline bool pte_dirty(pte_t pte)
+ {
+-	return pte_flags(pte) & _PAGE_DIRTY;
++	/*
++	 * A dirty PTE has Dirty=1 or Cow=1.
++	 */
++	return pte_flags(pte) & _PAGE_DIRTY_BITS;
++}
++
++static inline bool pte_shstk(pte_t pte)
++{
++	if (!cpu_feature_enabled(X86_FEATURE_SHSTK))
++		return false;
++
++	return (pte_flags(pte) & (_PAGE_RW | _PAGE_DIRTY)) == _PAGE_DIRTY;
+ }
+ 
+ static inline int pte_young(pte_t pte)
+@@ -131,9 +142,20 @@ static inline int pte_young(pte_t pte)
+ 	return pte_flags(pte) & _PAGE_ACCESSED;
+ }
+ 
+-static inline int pmd_dirty(pmd_t pmd)
++static inline bool pmd_dirty(pmd_t pmd)
++{
++	/*
++	 * A dirty PMD has Dirty=1 or Cow=1.
++	 */
++	return pmd_flags(pmd) & _PAGE_DIRTY_BITS;
++}
++
++static inline bool pmd_shstk(pmd_t pmd)
+ {
+-	return pmd_flags(pmd) & _PAGE_DIRTY;
++	if (!cpu_feature_enabled(X86_FEATURE_SHSTK))
++		return false;
++
++	return (pmd_flags(pmd) & (_PAGE_RW | _PAGE_DIRTY)) == _PAGE_DIRTY;
+ }
+ 
+ static inline int pmd_young(pmd_t pmd)
+@@ -141,9 +163,12 @@ static inline int pmd_young(pmd_t pmd)
+ 	return pmd_flags(pmd) & _PAGE_ACCESSED;
+ }
+ 
+-static inline int pud_dirty(pud_t pud)
++static inline bool pud_dirty(pud_t pud)
+ {
+-	return pud_flags(pud) & _PAGE_DIRTY;
++	/*
++	 * A dirty PUD has Dirty=1 or Cow=1.
++	 */
++	return pud_flags(pud) & _PAGE_DIRTY_BITS;
+ }
+ 
+ static inline int pud_young(pud_t pud)
+@@ -153,13 +178,23 @@ static inline int pud_young(pud_t pud)
+ 
+ static inline int pte_write(pte_t pte)
+ {
+-	return pte_flags(pte) & _PAGE_RW;
++	/*
++	 * Shadow stack pages are always writable - but not by normal
++	 * instructions, and only by shadow stack operations.  Therefore,
++	 * the W=0,D=1 test with pte_shstk().
++	 */
++	return (pte_flags(pte) & _PAGE_RW) || pte_shstk(pte);
+ }
+ 
+ #define pmd_write pmd_write
+ static inline int pmd_write(pmd_t pmd)
+ {
+-	return pmd_flags(pmd) & _PAGE_RW;
++	/*
++	 * Shadow stack pages are always writable - but not by normal
++	 * instructions, and only by shadow stack operations.  Therefore,
++	 * the W=0,D=1 test with pmd_shstk().
++	 */
++	return (pmd_flags(pmd) & _PAGE_RW) || pmd_shstk(pmd);
+ }
+ 
+ #define pud_write pud_write
+@@ -297,6 +332,24 @@ static inline pte_t pte_clear_flags(pte_t pte, pteval_t clear)
+ 	return native_make_pte(v & ~clear);
+ }
+ 
++static inline pte_t pte_mkcow(pte_t pte)
++{
++	if (!cpu_feature_enabled(X86_FEATURE_SHSTK))
++		return pte;
++
++	pte = pte_clear_flags(pte, _PAGE_DIRTY);
++	return pte_set_flags(pte, _PAGE_COW);
++}
++
++static inline pte_t pte_clear_cow(pte_t pte)
++{
++	if (!cpu_feature_enabled(X86_FEATURE_SHSTK))
++		return pte;
++
++	pte = pte_set_flags(pte, _PAGE_DIRTY);
++	return pte_clear_flags(pte, _PAGE_COW);
++}
++
+ #ifdef CONFIG_HAVE_ARCH_USERFAULTFD_WP
+ static inline int pte_uffd_wp(pte_t pte)
+ {
+@@ -316,7 +369,7 @@ static inline pte_t pte_clear_uffd_wp(pte_t pte)
+ 
+ static inline pte_t pte_mkclean(pte_t pte)
+ {
+-	return pte_clear_flags(pte, _PAGE_DIRTY);
++	return pte_clear_flags(pte, _PAGE_DIRTY_BITS);
+ }
+ 
+ static inline pte_t pte_mkold(pte_t pte)
+@@ -326,7 +379,16 @@ static inline pte_t pte_mkold(pte_t pte)
+ 
+ static inline pte_t pte_wrprotect(pte_t pte)
+ {
+-	return pte_clear_flags(pte, _PAGE_RW);
++	pte = pte_clear_flags(pte, _PAGE_RW);
++
++	/*
++	 * Blindly clearing _PAGE_RW might accidentally create
++	 * a shadow stack PTE (RW=0, Dirty=1).  Move the hardware
++	 * dirty value to the software bit.
++	 */
++	if (pte_dirty(pte))
++		pte = pte_mkcow(pte);
++	return pte;
+ }
+ 
+ static inline pte_t pte_mkexec(pte_t pte)
+@@ -336,7 +398,18 @@ static inline pte_t pte_mkexec(pte_t pte)
+ 
+ static inline pte_t pte_mkdirty(pte_t pte)
+ {
+-	return pte_set_flags(pte, _PAGE_DIRTY | _PAGE_SOFT_DIRTY);
++	pteval_t dirty = _PAGE_DIRTY;
++
++	/* Avoid creating (HW)Dirty=1, Write=0 PTEs */
++	if (cpu_feature_enabled(X86_FEATURE_SHSTK) && !pte_write(pte))
++		dirty = _PAGE_COW;
++
++	return pte_set_flags(pte, dirty | _PAGE_SOFT_DIRTY);
++}
++
++static inline pte_t pte_mkwrite_shstk(pte_t pte)
++{
++	return pte_clear_cow(pte);
+ }
+ 
+ static inline pte_t pte_mkyoung(pte_t pte)
+@@ -346,7 +419,12 @@ static inline pte_t pte_mkyoung(pte_t pte)
+ 
+ static inline pte_t pte_mkwrite(pte_t pte)
+ {
+-	return pte_set_flags(pte, _PAGE_RW);
++	pte = pte_set_flags(pte, _PAGE_RW);
++
++	if (pte_dirty(pte))
++		pte = pte_clear_cow(pte);
++
++	return pte;
+ }
+ 
+ static inline pte_t pte_mkhuge(pte_t pte)
+@@ -393,6 +471,24 @@ static inline pmd_t pmd_clear_flags(pmd_t pmd, pmdval_t clear)
+ 	return native_make_pmd(v & ~clear);
+ }
+ 
++static inline pmd_t pmd_mkcow(pmd_t pmd)
++{
++	if (!cpu_feature_enabled(X86_FEATURE_SHSTK))
++		return pmd;
++
++	pmd = pmd_clear_flags(pmd, _PAGE_DIRTY);
++	return pmd_set_flags(pmd, _PAGE_COW);
++}
++
++static inline pmd_t pmd_clear_cow(pmd_t pmd)
++{
++	if (!cpu_feature_enabled(X86_FEATURE_SHSTK))
++		return pmd;
++
++	pmd = pmd_set_flags(pmd, _PAGE_DIRTY);
++	return pmd_clear_flags(pmd, _PAGE_COW);
++}
++
+ #ifdef CONFIG_HAVE_ARCH_USERFAULTFD_WP
+ static inline int pmd_uffd_wp(pmd_t pmd)
+ {
+@@ -417,17 +513,36 @@ static inline pmd_t pmd_mkold(pmd_t pmd)
+ 
+ static inline pmd_t pmd_mkclean(pmd_t pmd)
+ {
+-	return pmd_clear_flags(pmd, _PAGE_DIRTY);
++	return pmd_clear_flags(pmd, _PAGE_DIRTY_BITS);
+ }
+ 
+ static inline pmd_t pmd_wrprotect(pmd_t pmd)
+ {
+-	return pmd_clear_flags(pmd, _PAGE_RW);
++	pmd = pmd_clear_flags(pmd, _PAGE_RW);
++	/*
++	 * Blindly clearing _PAGE_RW might accidentally create
++	 * a shadow stack PMD (RW=0, Dirty=1).  Move the hardware
++	 * dirty value to the software bit.
++	 */
++	if (pmd_dirty(pmd))
++		pmd = pmd_mkcow(pmd);
++	return pmd;
+ }
+ 
+ static inline pmd_t pmd_mkdirty(pmd_t pmd)
+ {
+-	return pmd_set_flags(pmd, _PAGE_DIRTY | _PAGE_SOFT_DIRTY);
++	pmdval_t dirty = _PAGE_DIRTY;
++
++	/* Avoid creating (HW)Dirty=1, Write=0 PMDs */
++	if (cpu_feature_enabled(X86_FEATURE_SHSTK) && !pmd_write(pmd))
++		dirty = _PAGE_COW;
++
++	return pmd_set_flags(pmd, dirty | _PAGE_SOFT_DIRTY);
++}
++
++static inline pmd_t pmd_mkwrite_shstk(pmd_t pmd)
++{
++	return pmd_clear_cow(pmd);
+ }
+ 
+ static inline pmd_t pmd_mkdevmap(pmd_t pmd)
+@@ -447,7 +562,11 @@ static inline pmd_t pmd_mkyoung(pmd_t pmd)
+ 
+ static inline pmd_t pmd_mkwrite(pmd_t pmd)
+ {
+-	return pmd_set_flags(pmd, _PAGE_RW);
++	pmd = pmd_set_flags(pmd, _PAGE_RW);
++
++	if (pmd_dirty(pmd))
++		pmd = pmd_clear_cow(pmd);
++	return pmd;
+ }
+ 
+ static inline pud_t pud_set_flags(pud_t pud, pudval_t set)
+@@ -464,6 +583,24 @@ static inline pud_t pud_clear_flags(pud_t pud, pudval_t clear)
+ 	return native_make_pud(v & ~clear);
+ }
+ 
++static inline pud_t pud_mkcow(pud_t pud)
++{
++	if (!cpu_feature_enabled(X86_FEATURE_SHSTK))
++		return pud;
++
++	pud = pud_clear_flags(pud, _PAGE_DIRTY);
++	return pud_set_flags(pud, _PAGE_COW);
++}
++
++static inline pud_t pud_clear_cow(pud_t pud)
++{
++	if (!cpu_feature_enabled(X86_FEATURE_SHSTK))
++		return pud;
++
++	pud = pud_set_flags(pud, _PAGE_DIRTY);
++	return pud_clear_flags(pud, _PAGE_COW);
++}
++
+ static inline pud_t pud_mkold(pud_t pud)
+ {
+ 	return pud_clear_flags(pud, _PAGE_ACCESSED);
+@@ -471,17 +608,32 @@ static inline pud_t pud_mkold(pud_t pud)
+ 
+ static inline pud_t pud_mkclean(pud_t pud)
+ {
+-	return pud_clear_flags(pud, _PAGE_DIRTY);
++	return pud_clear_flags(pud, _PAGE_DIRTY_BITS);
+ }
+ 
+ static inline pud_t pud_wrprotect(pud_t pud)
+ {
+-	return pud_clear_flags(pud, _PAGE_RW);
++	pud = pud_clear_flags(pud, _PAGE_RW);
++
++	/*
++	 * Blindly clearing _PAGE_RW might accidentally create
++	 * a shadow stack PUD (RW=0, Dirty=1).  Move the hardware
++	 * dirty value to the software bit.
++	 */
++	if (pud_dirty(pud))
++		pud = pud_mkcow(pud);
++	return pud;
+ }
+ 
+ static inline pud_t pud_mkdirty(pud_t pud)
+ {
+-	return pud_set_flags(pud, _PAGE_DIRTY | _PAGE_SOFT_DIRTY);
++	pudval_t dirty = _PAGE_DIRTY;
++
++	/* Avoid creating (HW)Dirty=1, Write=0 PUDs */
++	if (cpu_feature_enabled(X86_FEATURE_SHSTK) && !pud_write(pud))
++		dirty = _PAGE_COW;
++
++	return pud_set_flags(pud, dirty | _PAGE_SOFT_DIRTY);
+ }
+ 
+ static inline pud_t pud_mkdevmap(pud_t pud)
+@@ -501,7 +653,11 @@ static inline pud_t pud_mkyoung(pud_t pud)
+ 
+ static inline pud_t pud_mkwrite(pud_t pud)
+ {
+-	return pud_set_flags(pud, _PAGE_RW);
++	pud = pud_set_flags(pud, _PAGE_RW);
++
++	if (pud_dirty(pud))
++		pud = pud_clear_cow(pud);
++	return pud;
+ }
+ 
+ #ifdef CONFIG_HAVE_ARCH_SOFT_DIRTY
+diff --git a/arch/x86/include/asm/pgtable_types.h b/arch/x86/include/asm/pgtable_types.h
+index 3781a79b6388..1bfab70ff9ac 100644
+--- a/arch/x86/include/asm/pgtable_types.h
++++ b/arch/x86/include/asm/pgtable_types.h
+@@ -21,7 +21,8 @@
+ #define _PAGE_BIT_SOFTW2	10	/* " */
+ #define _PAGE_BIT_SOFTW3	11	/* " */
+ #define _PAGE_BIT_PAT_LARGE	12	/* On 2MB or 1GB pages */
+-#define _PAGE_BIT_SOFTW4	58	/* available for programmer */
++#define _PAGE_BIT_SOFTW4	57	/* available for programmer */
++#define _PAGE_BIT_SOFTW5	58	/* available for programmer */
+ #define _PAGE_BIT_PKEY_BIT0	59	/* Protection Keys, bit 1/4 */
+ #define _PAGE_BIT_PKEY_BIT1	60	/* Protection Keys, bit 2/4 */
+ #define _PAGE_BIT_PKEY_BIT2	61	/* Protection Keys, bit 3/4 */
+@@ -34,6 +35,15 @@
+ #define _PAGE_BIT_SOFT_DIRTY	_PAGE_BIT_SOFTW3 /* software dirty tracking */
+ #define _PAGE_BIT_DEVMAP	_PAGE_BIT_SOFTW4
+ 
++/*
++ * Indicates a copy-on-write page.
++ */
++#ifdef CONFIG_X86_SHADOW_STACK
++#define _PAGE_BIT_COW		_PAGE_BIT_SOFTW5 /* copy-on-write */
++#else
++#define _PAGE_BIT_COW		0
++#endif
++
+ /* If _PAGE_BIT_PRESENT is clear, we use these: */
+ /* - if the user mapped it with PROT_NONE; pte_present gives true */
+ #define _PAGE_BIT_PROTNONE	_PAGE_BIT_GLOBAL
+@@ -115,6 +125,36 @@
+ #define _PAGE_DEVMAP	(_AT(pteval_t, 0))
+ #endif
+ 
++/*
++ * The hardware requires shadow stack to be read-only and Dirty.
++ * _PAGE_COW is a software-only bit used to separate copy-on-write PTEs
++ * from shadow stack PTEs:
++ * (a) A modified, copy-on-write (COW) page: (Write=0, Cow=1)
++ * (b) A R/O page that has been COW'ed: (Write=0, Cow=1)
++ *     The user page is in a R/O VMA, and get_user_pages() needs a
++ *     writable copy.  The page fault handler creates a copy of the page
++ *     and sets the new copy's PTE as Write=0, Cow=1.
++ * (c) A shadow stack PTE: (Write=0, Dirty=1)
++ * (d) A shared (copy-on-access) shadow stack PTE: (Write=0, Cow=1)
++ *     When a shadow stack page is being shared among processes (this
++ *     happens at fork()), its PTE is cleared of _PAGE_DIRTY, so the next
++ *     shadow stack access causes a fault, and the page is duplicated and
++ *     _PAGE_DIRTY is set again.  This is the COW equivalent for shadow
++ *     stack pages, even though it's copy-on-access rather than
++ *     copy-on-write.
++ * (e) A page where the processor observed a Write=1 PTE, started a write,
++ *     set Dirty=1, but then observed a Write=0 PTE (changed by another
++ *     thread).  That's possible today, but will not happen on processors
++ *     that support shadow stack.
++ */
++#ifdef CONFIG_X86_SHADOW_STACK
++#define _PAGE_COW	(_AT(pteval_t, 1) << _PAGE_BIT_COW)
++#else
++#define _PAGE_COW	(_AT(pteval_t, 0))
++#endif
++
++#define _PAGE_DIRTY_BITS (_PAGE_DIRTY | _PAGE_COW)
++
+ #define _PAGE_PROTNONE	(_AT(pteval_t, 1) << _PAGE_BIT_PROTNONE)
+ 
+ /*
+-- 
+2.34.1
+
diff --git a/0010-drm-i915-gvt-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_BITS.patch b/0010-drm-i915-gvt-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_BITS.patch
new file mode 100644
index 000000000..c0624d39b
--- /dev/null
+++ b/0010-drm-i915-gvt-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_BITS.patch
@@ -0,0 +1,39 @@
+From 3335e47f40a6cbce75bf10cc85c307f3b5217e5a Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:06 -0700
+Subject: [PATCH 10/39] drm/i915/gvt: Change _PAGE_DIRTY to _PAGE_DIRTY_BITS
+
+After the introduction of _PAGE_COW, a modified page's PTE can have either
+_PAGE_DIRTY or _PAGE_COW.  Change _PAGE_DIRTY to _PAGE_DIRTY_BITS.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+Reviewed-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
+Cc: David Airlie <airlied@linux.ie>
+Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
+Cc: Jani Nikula <jani.nikula@linux.intel.com>
+Cc: Daniel Vetter <daniel@ffwll.ch>
+Cc: Rodrigo Vivi <rodrigo.vivi@intel.com>
+Cc: Zhenyu Wang <zhenyuw@linux.intel.com>
+Cc: Zhi Wang <zhi.a.wang@intel.com>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+---
+ drivers/gpu/drm/i915/gvt/gtt.c | 2 +-
+ 1 file changed, 1 insertion(+), 1 deletion(-)
+
+diff --git a/drivers/gpu/drm/i915/gvt/gtt.c b/drivers/gpu/drm/i915/gvt/gtt.c
+index 53d0cb327539..581840d7617e 100644
+--- a/drivers/gpu/drm/i915/gvt/gtt.c
++++ b/drivers/gpu/drm/i915/gvt/gtt.c
+@@ -1210,7 +1210,7 @@ static int split_2MB_gtt_entry(struct intel_vgpu *vgpu,
+ 	}
+ 
+ 	/* Clear dirty field. */
+-	se->val64 &= ~_PAGE_DIRTY;
++	se->val64 &= ~_PAGE_DIRTY_BITS;
+ 
+ 	ops->clear_pse(se);
+ 	ops->clear_ips(se);
+-- 
+2.34.1
+
diff --git a/0011-x86-mm-Update-pte_modify-for-_PAGE_COW.patch b/0011-x86-mm-Update-pte_modify-for-_PAGE_COW.patch
new file mode 100644
index 000000000..53af9d197
--- /dev/null
+++ b/0011-x86-mm-Update-pte_modify-for-_PAGE_COW.patch
@@ -0,0 +1,92 @@
+From ee32940441d8d5b094acdd85748ff197997dc851 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:07 -0700
+Subject: [PATCH 11/39] x86/mm: Update pte_modify for _PAGE_COW
+
+The read-only and Dirty PTE has been used to indicate copy-on-write pages.
+However, newer x86 processors also regard a read-only and Dirty PTE as a
+shadow stack page.  In order to separate the two, the software-defined
+_PAGE_COW is created to replace _PAGE_DIRTY for the copy-on-write case, and
+pte_*() are updated.
+
+Pte_modify() changes a PTE to 'newprot', but it doesn't use the pte_*().
+Introduce fixup_dirty_pte(), which sets a dirty PTE, based on _PAGE_RW,
+to either _PAGE_DIRTY or _PAGE_COW.
+
+Apply the same changes to pmd_modify().
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+---
+ arch/x86/include/asm/pgtable.h | 37 ++++++++++++++++++++++++++++++++++
+ 1 file changed, 37 insertions(+)
+
+diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
+index 9f1ba76ed79a..cf7316e968df 100644
+--- a/arch/x86/include/asm/pgtable.h
++++ b/arch/x86/include/asm/pgtable.h
+@@ -771,6 +771,23 @@ static inline pmd_t pmd_mkinvalid(pmd_t pmd)
+ 
+ static inline u64 flip_protnone_guard(u64 oldval, u64 val, u64 mask);
+ 
++static inline pteval_t fixup_dirty_pte(pteval_t pteval)
++{
++	pte_t pte = __pte(pteval);
++
++	/*
++	 * Fix up potential shadow stack page flags because the RO, Dirty
++	 * PTE is special.
++	 */
++	if (cpu_feature_enabled(X86_FEATURE_SHSTK)) {
++		if (pte_dirty(pte)) {
++			pte = pte_mkclean(pte);
++			pte = pte_mkdirty(pte);
++		}
++	}
++	return pte_val(pte);
++}
++
+ static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
+ {
+ 	pteval_t val = pte_val(pte), oldval = val;
+@@ -781,16 +798,36 @@ static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
+ 	 */
+ 	val &= _PAGE_CHG_MASK;
+ 	val |= check_pgprot(newprot) & ~_PAGE_CHG_MASK;
++	val = fixup_dirty_pte(val);
+ 	val = flip_protnone_guard(oldval, val, PTE_PFN_MASK);
+ 	return __pte(val);
+ }
+ 
++static inline int pmd_write(pmd_t pmd);
++static inline pmdval_t fixup_dirty_pmd(pmdval_t pmdval)
++{
++	pmd_t pmd = __pmd(pmdval);
++
++	/*
++	 * Fix up potential shadow stack page flags because the RO, Dirty
++	 * PMD is special.
++	 */
++	if (cpu_feature_enabled(X86_FEATURE_SHSTK)) {
++		if (pmd_dirty(pmd)) {
++			pmd = pmd_mkclean(pmd);
++			pmd = pmd_mkdirty(pmd);
++		}
++	}
++	return pmd_val(pmd);
++}
++
+ static inline pmd_t pmd_modify(pmd_t pmd, pgprot_t newprot)
+ {
+ 	pmdval_t val = pmd_val(pmd), oldval = val;
+ 
+ 	val &= _HPAGE_CHG_MASK;
+ 	val |= check_pgprot(newprot) & ~_HPAGE_CHG_MASK;
++	val = fixup_dirty_pmd(val);
+ 	val = flip_protnone_guard(oldval, val, PHYSICAL_PMD_PAGE_MASK);
+ 	return __pmd(val);
+ }
+-- 
+2.34.1
+
diff --git a/0012-x86-mm-Update-ptep_set_wrprotect-and-pmdp_set_wrprot.patch b/0012-x86-mm-Update-ptep_set_wrprotect-and-pmdp_set_wrprot.patch
new file mode 100644
index 000000000..d49036d5f
--- /dev/null
+++ b/0012-x86-mm-Update-ptep_set_wrprotect-and-pmdp_set_wrprot.patch
@@ -0,0 +1,95 @@
+From 41eca8f64ab146bf9d546adf23fa6b80d22e4344 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:08 -0700
+Subject: [PATCH 12/39] x86/mm: Update ptep_set_wrprotect() and
+ pmdp_set_wrprotect() for transition from _PAGE_DIRTY to _PAGE_COW
+
+When Shadow Stack is introduced, [R/O + _PAGE_DIRTY] PTE is reserved for
+shadow stack.  Copy-on-write PTEs have [R/O + _PAGE_COW].
+
+When a PTE goes from [R/W + _PAGE_DIRTY] to [R/O + _PAGE_COW], it could
+become a transient shadow stack PTE in two cases:
+
+The first case is that some processors can start a write but end up seeing
+a read-only PTE by the time they get to the Dirty bit, creating a transient
+shadow stack PTE.  However, this will not occur on processors supporting
+Shadow Stack, and a TLB flush is not necessary.
+
+The second case is that when _PAGE_DIRTY is replaced with _PAGE_COW non-
+atomically, a transient shadow stack PTE can be created as a result.
+Thus, prevent that with cmpxchg.
+
+Dave Hansen, Jann Horn, Andy Lutomirski, and Peter Zijlstra provided many
+insights to the issue.  Jann Horn provided the cmpxchg solution.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+Reviewed-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+===
+
+Yu-cheng v30:
+- Replace (pmdval_t) cast with CONFIG_PGTABLE_LEVELES > 2 (Borislav Petkov).
+---
+ arch/x86/include/asm/pgtable.h | 38 ++++++++++++++++++++++++++++++++++
+ 1 file changed, 38 insertions(+)
+
+diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
+index cf7316e968df..7c0542997790 100644
+--- a/arch/x86/include/asm/pgtable.h
++++ b/arch/x86/include/asm/pgtable.h
+@@ -1278,6 +1278,24 @@ static inline pte_t ptep_get_and_clear_full(struct mm_struct *mm,
+ static inline void ptep_set_wrprotect(struct mm_struct *mm,
+ 				      unsigned long addr, pte_t *ptep)
+ {
++	/*
++	 * If Shadow Stack is enabled, pte_wrprotect() moves _PAGE_DIRTY
++	 * to _PAGE_COW (see comments at pte_wrprotect()).
++	 * When a thread reads a RW=1, Dirty=0 PTE and before changing it
++	 * to RW=0, Dirty=0, another thread could have written to the page
++	 * and the PTE is RW=1, Dirty=1 now.  Use try_cmpxchg() to detect
++	 * PTE changes and update old_pte, then try again.
++	 */
++	if (cpu_feature_enabled(X86_FEATURE_SHSTK)) {
++		pte_t old_pte, new_pte;
++
++		old_pte = READ_ONCE(*ptep);
++		do {
++			new_pte = pte_wrprotect(old_pte);
++		} while (!try_cmpxchg(&ptep->pte, &old_pte.pte, new_pte.pte));
++
++		return;
++	}
+ 	clear_bit(_PAGE_BIT_RW, (unsigned long *)&ptep->pte);
+ }
+ 
+@@ -1322,6 +1340,26 @@ static inline pud_t pudp_huge_get_and_clear(struct mm_struct *mm,
+ static inline void pmdp_set_wrprotect(struct mm_struct *mm,
+ 				      unsigned long addr, pmd_t *pmdp)
+ {
++#if CONFIG_PGTABLE_LEVELS > 2
++	/*
++	 * If Shadow Stack is enabled, pmd_wrprotect() moves _PAGE_DIRTY
++	 * to _PAGE_COW (see comments at pmd_wrprotect()).
++	 * When a thread reads a RW=1, Dirty=0 PMD and before changing it
++	 * to RW=0, Dirty=0, another thread could have written to the page
++	 * and the PMD is RW=1, Dirty=1 now.  Use try_cmpxchg() to detect
++	 * PMD changes and update old_pmd, then try again.
++	 */
++	if (cpu_feature_enabled(X86_FEATURE_SHSTK)) {
++		pmd_t old_pmd, new_pmd;
++
++		old_pmd = READ_ONCE(*pmdp);
++		do {
++			new_pmd = pmd_wrprotect(old_pmd);
++		} while (!try_cmpxchg(&pmdp->pmd, &old_pmd.pmd, new_pmd.pmd));
++
++		return;
++	}
++#endif
+ 	clear_bit(_PAGE_BIT_RW, (unsigned long *)pmdp);
+ }
+ 
+-- 
+2.34.1
+
diff --git a/0013-mm-Move-VM_UFFD_MINOR_BIT-from-37-to-38.patch b/0013-mm-Move-VM_UFFD_MINOR_BIT-from-37-to-38.patch
new file mode 100644
index 000000000..4da7a7348
--- /dev/null
+++ b/0013-mm-Move-VM_UFFD_MINOR_BIT-from-37-to-38.patch
@@ -0,0 +1,33 @@
+From bce66e68322cff3db2f0055eebfcb0f2dacf79b9 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:09 -0700
+Subject: [PATCH 13/39] mm: Move VM_UFFD_MINOR_BIT from 37 to 38
+
+To introduce VM_SHADOW_STACK as VM_HIGH_ARCH_BIT (37), and make all
+VM_HIGH_ARCH_BITs stay together, move VM_UFFD_MINOR_BIT from 37 to 38.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Axel Rasmussen <axelrasmussen@google.com>
+Cc: Peter Xu <peterx@redhat.com>
+Cc: Mike Kravetz <mike.kravetz@oracle.com>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+---
+ include/linux/mm.h | 2 +-
+ 1 file changed, 1 insertion(+), 1 deletion(-)
+
+diff --git a/include/linux/mm.h b/include/linux/mm.h
+index a7e4a9e7d807..33240dc8789d 100644
+--- a/include/linux/mm.h
++++ b/include/linux/mm.h
+@@ -359,7 +359,7 @@ extern unsigned int kobjsize(const void *objp);
+ #endif
+ 
+ #ifdef CONFIG_HAVE_ARCH_USERFAULTFD_MINOR
+-# define VM_UFFD_MINOR_BIT	37
++# define VM_UFFD_MINOR_BIT	38
+ # define VM_UFFD_MINOR		BIT(VM_UFFD_MINOR_BIT)	/* UFFD minor faults */
+ #else /* !CONFIG_HAVE_ARCH_USERFAULTFD_MINOR */
+ # define VM_UFFD_MINOR		VM_NONE
+-- 
+2.34.1
+
diff --git a/0014-mm-Introduce-VM_SHADOW_STACK-for-shadow-stack-memory.patch b/0014-mm-Introduce-VM_SHADOW_STACK-for-shadow-stack-memory.patch
new file mode 100644
index 000000000..0f46a102d
--- /dev/null
+++ b/0014-mm-Introduce-VM_SHADOW_STACK-for-shadow-stack-memory.patch
@@ -0,0 +1,94 @@
+From 7a3cef491c97f07f18a9a9dd0866faa58ccc435c Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:10 -0700
+Subject: [PATCH 14/39] mm: Introduce VM_SHADOW_STACK for shadow stack memory
+
+A shadow stack PTE must be read-only and have _PAGE_DIRTY set.  However,
+read-only and Dirty PTEs also exist for copy-on-write (COW) pages.  These
+two cases are handled differently for page faults.  Introduce
+VM_SHADOW_STACK to track shadow stack VMAs.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
+Cc: Kees Cook <keescook@chromium.org>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+---
+ Documentation/filesystems/proc.rst | 1 +
+ arch/x86/mm/mmap.c                 | 2 ++
+ fs/proc/task_mmu.c                 | 3 +++
+ include/linux/mm.h                 | 8 ++++++++
+ 4 files changed, 14 insertions(+)
+
+diff --git a/Documentation/filesystems/proc.rst b/Documentation/filesystems/proc.rst
+index 8d7f141c6fc7..8545eb6f8f17 100644
+--- a/Documentation/filesystems/proc.rst
++++ b/Documentation/filesystems/proc.rst
+@@ -553,6 +553,7 @@ encoded manner. The codes are the following:
+     mt    arm64 MTE allocation tags are enabled
+     um    userfaultfd missing tracking
+     uw    userfaultfd wr-protect tracking
++    ss    shadow stack page
+     ==    =======================================
+ 
+ Note that there is no guarantee that every flag and associated mnemonic will
+diff --git a/arch/x86/mm/mmap.c b/arch/x86/mm/mmap.c
+index c90c20904a60..f3f52c5e2fd6 100644
+--- a/arch/x86/mm/mmap.c
++++ b/arch/x86/mm/mmap.c
+@@ -165,6 +165,8 @@ unsigned long get_mmap_base(int is_legacy)
+ 
+ const char *arch_vma_name(struct vm_area_struct *vma)
+ {
++	if (vma->vm_flags & VM_SHADOW_STACK)
++		return "[shadow stack]";
+ 	return NULL;
+ }
+ 
+diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c
+index ad667dbc96f5..8fcade61fd7e 100644
+--- a/fs/proc/task_mmu.c
++++ b/fs/proc/task_mmu.c
+@@ -668,6 +668,9 @@ static void show_smap_vma_flags(struct seq_file *m, struct vm_area_struct *vma)
+ #ifdef CONFIG_HAVE_ARCH_USERFAULTFD_MINOR
+ 		[ilog2(VM_UFFD_MINOR)]	= "ui",
+ #endif /* CONFIG_HAVE_ARCH_USERFAULTFD_MINOR */
++#ifdef CONFIG_ARCH_HAS_SHADOW_STACK
++		[ilog2(VM_SHADOW_STACK)] = "ss",
++#endif
+ 	};
+ 	size_t i;
+ 
+diff --git a/include/linux/mm.h b/include/linux/mm.h
+index 33240dc8789d..983bae8cb2e2 100644
+--- a/include/linux/mm.h
++++ b/include/linux/mm.h
+@@ -308,11 +308,13 @@ extern unsigned int kobjsize(const void *objp);
+ #define VM_HIGH_ARCH_BIT_2	34	/* bit only usable on 64-bit architectures */
+ #define VM_HIGH_ARCH_BIT_3	35	/* bit only usable on 64-bit architectures */
+ #define VM_HIGH_ARCH_BIT_4	36	/* bit only usable on 64-bit architectures */
++#define VM_HIGH_ARCH_BIT_5	37	/* bit only usable on 64-bit architectures */
+ #define VM_HIGH_ARCH_0	BIT(VM_HIGH_ARCH_BIT_0)
+ #define VM_HIGH_ARCH_1	BIT(VM_HIGH_ARCH_BIT_1)
+ #define VM_HIGH_ARCH_2	BIT(VM_HIGH_ARCH_BIT_2)
+ #define VM_HIGH_ARCH_3	BIT(VM_HIGH_ARCH_BIT_3)
+ #define VM_HIGH_ARCH_4	BIT(VM_HIGH_ARCH_BIT_4)
++#define VM_HIGH_ARCH_5	BIT(VM_HIGH_ARCH_BIT_5)
+ #endif /* CONFIG_ARCH_USES_HIGH_VMA_FLAGS */
+ 
+ #ifdef CONFIG_ARCH_HAS_PKEYS
+@@ -328,6 +330,12 @@ extern unsigned int kobjsize(const void *objp);
+ #endif
+ #endif /* CONFIG_ARCH_HAS_PKEYS */
+ 
++#ifdef CONFIG_X86_SHADOW_STACK
++# define VM_SHADOW_STACK	VM_HIGH_ARCH_5
++#else
++# define VM_SHADOW_STACK	VM_NONE
++#endif
++
+ #if defined(CONFIG_X86)
+ # define VM_PAT		VM_ARCH_1	/* PAT reserves whole VMA at once (x86) */
+ #elif defined(CONFIG_PPC)
+-- 
+2.34.1
+
diff --git a/0015-x86-mm-Check-Shadow-Stack-page-fault-errors.patch b/0015-x86-mm-Check-Shadow-Stack-page-fault-errors.patch
new file mode 100644
index 000000000..62540c17e
--- /dev/null
+++ b/0015-x86-mm-Check-Shadow-Stack-page-fault-errors.patch
@@ -0,0 +1,100 @@
+From ca86e7105b5f7c1949440ccaa4e675784da3b007 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:11 -0700
+Subject: [PATCH 15/39] x86/mm: Check Shadow Stack page fault errors
+
+Shadow stack accesses are those that are performed by the CPU where it
+expects to encounter a shadow stack mapping.  These accesses are performed
+implicitly by CALL/RET at the site of the shadow stack pointer.  These
+accesses are made explicitly by shadow stack management instructions like
+WRUSSQ.
+
+Shadow stacks accesses to shadow-stack mapping can see faults in normal,
+valid operation just like regular accesses to regular mappings.  Shadow
+stacks need some of the same features like delayed allocation, swap and
+copy-on-write.
+
+Shadow stack accesses can also result in errors, such as when a shadow
+stack overflows, or if a shadow stack access occurs to a non-shadow-stack
+mapping.
+
+In handling a shadow stack page fault, verify it occurs within a shadow
+stack mapping.  It is always an error otherwise.  For valid shadow stack
+accesses, set FAULT_FLAG_WRITE to effect copy-on-write.  Because clearing
+_PAGE_DIRTY (vs. _PAGE_RW) is used to trigger the fault, shadow stack read
+fault and shadow stack write fault are not differentiated and both are
+handled as a write access.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+Reviewed-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+===
+
+Yu-cheng v30:
+- Update Subject line and add a verb.
+---
+ arch/x86/include/asm/trap_pf.h |  2 ++
+ arch/x86/mm/fault.c            | 19 +++++++++++++++++++
+ 2 files changed, 21 insertions(+)
+
+diff --git a/arch/x86/include/asm/trap_pf.h b/arch/x86/include/asm/trap_pf.h
+index 10b1de500ab1..afa524325e55 100644
+--- a/arch/x86/include/asm/trap_pf.h
++++ b/arch/x86/include/asm/trap_pf.h
+@@ -11,6 +11,7 @@
+  *   bit 3 ==				1: use of reserved bit detected
+  *   bit 4 ==				1: fault was an instruction fetch
+  *   bit 5 ==				1: protection keys block access
++ *   bit 6 ==				1: shadow stack access fault
+  *   bit 15 ==				1: SGX MMU page-fault
+  */
+ enum x86_pf_error_code {
+@@ -20,6 +21,7 @@ enum x86_pf_error_code {
+ 	X86_PF_RSVD	=		1 << 3,
+ 	X86_PF_INSTR	=		1 << 4,
+ 	X86_PF_PK	=		1 << 5,
++	X86_PF_SHSTK	=		1 << 6,
+ 	X86_PF_SGX	=		1 << 15,
+ };
+ 
+diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
+index 4bfed53e210e..54515c66aa81 100644
+--- a/arch/x86/mm/fault.c
++++ b/arch/x86/mm/fault.c
+@@ -1107,6 +1107,17 @@ access_error(unsigned long error_code, struct vm_area_struct *vma)
+ 				       (error_code & X86_PF_INSTR), foreign))
+ 		return 1;
+ 
++	/*
++	 * Verify a shadow stack access is within a shadow stack VMA.
++	 * It is always an error otherwise.  Normal data access to a
++	 * shadow stack area is checked in the case followed.
++	 */
++	if (error_code & X86_PF_SHSTK) {
++		if (!(vma->vm_flags & VM_SHADOW_STACK))
++			return 1;
++		return 0;
++	}
++
+ 	if (error_code & X86_PF_WRITE) {
+ 		/* write, present and write, not present: */
+ 		if (unlikely(!(vma->vm_flags & VM_WRITE)))
+@@ -1300,6 +1311,14 @@ void do_user_addr_fault(struct pt_regs *regs,
+ 
+ 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
+ 
++	/*
++	 * Clearing _PAGE_DIRTY is used to detect shadow stack access.
++	 * This method cannot distinguish shadow stack read vs. write.
++	 * For valid shadow stack accesses, set FAULT_FLAG_WRITE to effect
++	 * copy-on-write.
++	 */
++	if (error_code & X86_PF_SHSTK)
++		flags |= FAULT_FLAG_WRITE;
+ 	if (error_code & X86_PF_WRITE)
+ 		flags |= FAULT_FLAG_WRITE;
+ 	if (error_code & X86_PF_INSTR)
+-- 
+2.34.1
+
diff --git a/0016-x86-mm-Update-maybe_mkwrite-for-shadow-stack.patch b/0016-x86-mm-Update-maybe_mkwrite-for-shadow-stack.patch
new file mode 100644
index 000000000..c0b1617ec
--- /dev/null
+++ b/0016-x86-mm-Update-maybe_mkwrite-for-shadow-stack.patch
@@ -0,0 +1,130 @@
+From 56860b2083f6276b4040fd411d004ef644ffacdb Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:12 -0700
+Subject: [PATCH 16/39] x86/mm: Update maybe_mkwrite() for shadow stack
+
+When serving a page fault, maybe_mkwrite() makes a PTE writable if its vma
+has VM_WRITE.
+
+A shadow stack vma has VM_SHADOW_STACK.  Its PTEs have _PAGE_DIRTY, but not
+_PAGE_WRITE.  In fork(), _PAGE_DIRTY is cleared to cause copy-on-write,
+and in the page fault handler, _PAGE_DIRTY is restored and the shadow stack
+page is writable again.
+
+Introduce an x86 version of maybe_mkwrite(), which sets proper PTE bits
+according to VM flags.
+
+Apply the same changes to maybe_pmd_mkwrite().
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
+Cc: Kees Cook <keescook@chromium.org>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+
+===
+
+Yu-cheng v29:
+- Remove likely()'s.
+---
+ arch/x86/include/asm/pgtable.h |  6 ++++++
+ arch/x86/mm/pgtable.c          | 20 ++++++++++++++++++++
+ include/linux/mm.h             |  2 ++
+ mm/huge_memory.c               |  2 ++
+ 4 files changed, 30 insertions(+)
+
+diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
+index 7c0542997790..8dee420e382e 100644
+--- a/arch/x86/include/asm/pgtable.h
++++ b/arch/x86/include/asm/pgtable.h
+@@ -280,6 +280,9 @@ static inline int pmd_trans_huge(pmd_t pmd)
+ 	return (pmd_val(pmd) & (_PAGE_PSE|_PAGE_DEVMAP)) == _PAGE_PSE;
+ }
+ 
++#define maybe_pmd_mkwrite maybe_pmd_mkwrite
++extern pmd_t maybe_pmd_mkwrite(pmd_t pmd, struct vm_area_struct *vma);
++
+ #ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD
+ static inline int pud_trans_huge(pud_t pud)
+ {
+@@ -1634,6 +1637,9 @@ static inline bool arch_faults_on_old_pte(void)
+ 	return false;
+ }
+ 
++#define maybe_mkwrite maybe_mkwrite
++extern pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma);
++
+ #endif	/* __ASSEMBLY__ */
+ 
+ #endif /* _ASM_X86_PGTABLE_H */
+diff --git a/arch/x86/mm/pgtable.c b/arch/x86/mm/pgtable.c
+index 3481b35cb4ec..c22c8e9c37e8 100644
+--- a/arch/x86/mm/pgtable.c
++++ b/arch/x86/mm/pgtable.c
+@@ -610,6 +610,26 @@ int pmdp_clear_flush_young(struct vm_area_struct *vma,
+ }
+ #endif
+ 
++pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)
++{
++	if (vma->vm_flags & VM_WRITE)
++		pte = pte_mkwrite(pte);
++	else if (vma->vm_flags & VM_SHADOW_STACK)
++		pte = pte_mkwrite_shstk(pte);
++	return pte;
++}
++
++#ifdef CONFIG_TRANSPARENT_HUGEPAGE
++pmd_t maybe_pmd_mkwrite(pmd_t pmd, struct vm_area_struct *vma)
++{
++	if (vma->vm_flags & VM_WRITE)
++		pmd = pmd_mkwrite(pmd);
++	else if (vma->vm_flags & VM_SHADOW_STACK)
++		pmd = pmd_mkwrite_shstk(pmd);
++	return pmd;
++}
++#endif /* CONFIG_TRANSPARENT_HUGEPAGE */
++
+ /**
+  * reserve_top_address - reserves a hole in the top of kernel address space
+  * @reserve - size of hole to reserve
+diff --git a/include/linux/mm.h b/include/linux/mm.h
+index 983bae8cb2e2..998b9a1a5c18 100644
+--- a/include/linux/mm.h
++++ b/include/linux/mm.h
+@@ -993,12 +993,14 @@ void free_compound_page(struct page *page);
+  * pte_mkwrite.  But get_user_pages can cause write faults for mappings
+  * that do not have writing enabled, when used by access_process_vm.
+  */
++#ifndef maybe_mkwrite
+ static inline pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)
+ {
+ 	if (likely(vma->vm_flags & VM_WRITE))
+ 		pte = pte_mkwrite(pte);
+ 	return pte;
+ }
++#endif
+ 
+ vm_fault_t do_set_pmd(struct vm_fault *vmf, struct page *page);
+ void do_set_pte(struct vm_fault *vmf, struct page *page, unsigned long addr);
+diff --git a/mm/huge_memory.c b/mm/huge_memory.c
+index e5483347291c..b6039623c7b8 100644
+--- a/mm/huge_memory.c
++++ b/mm/huge_memory.c
+@@ -491,12 +491,14 @@ static int __init setup_transparent_hugepage(char *str)
+ }
+ __setup("transparent_hugepage=", setup_transparent_hugepage);
+ 
++#ifndef maybe_pmd_mkwrite
+ pmd_t maybe_pmd_mkwrite(pmd_t pmd, struct vm_area_struct *vma)
+ {
+ 	if (likely(vma->vm_flags & VM_WRITE))
+ 		pmd = pmd_mkwrite(pmd);
+ 	return pmd;
+ }
++#endif
+ 
+ #ifdef CONFIG_MEMCG
+ static inline struct deferred_split *get_deferred_split_queue(struct page *page)
+-- 
+2.34.1
+
diff --git a/0017-mm-Fixup-places-that-call-pte_mkwrite-directly.patch b/0017-mm-Fixup-places-that-call-pte_mkwrite-directly.patch
new file mode 100644
index 000000000..26ba532cb
--- /dev/null
+++ b/0017-mm-Fixup-places-that-call-pte_mkwrite-directly.patch
@@ -0,0 +1,107 @@
+From 22ffef7517cda606dcccd8d0a8c3dcca18ff3dab Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:13 -0700
+Subject: [PATCH 17/39] mm: Fixup places that call pte_mkwrite() directly
+
+When serving a page fault, maybe_mkwrite() makes a PTE writable if it is in
+a writable vma.  A shadow stack vma is writable, but its PTEs need
+_PAGE_DIRTY to be set to become writable.  For this reason, maybe_mkwrite()
+has been updated.
+
+There are a few places that call pte_mkwrite() directly, but have the
+same result as from maybe_mkwrite().  These sites need to be updated for
+shadow stack as well.  Thus, change them to maybe_mkwrite():
+
+- do_anonymous_page() and migrate_vma_insert_page() check VM_WRITE directly
+  and call pte_mkwrite(), which is the same as maybe_mkwrite().  Change
+  them to maybe_mkwrite().
+
+- In do_numa_page(), if the numa entry was writable, then pte_mkwrite()
+  is called directly.  Fix it by doing maybe_mkwrite().  Make the same
+  changes to do_huge_pmd_numa_page().
+
+- In change_pte_range(), pte_mkwrite() is called directly.  Replace it with
+  maybe_mkwrite().
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
+Cc: Kees Cook <keescook@chromium.org>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+
+===
+
+Yu-cheng v25:
+- Apply same changes to do_huge_pmd_numa_page() as to do_numa_page().
+---
+ mm/huge_memory.c | 2 +-
+ mm/memory.c      | 5 ++---
+ mm/migrate.c     | 3 +--
+ mm/mprotect.c    | 2 +-
+ 4 files changed, 5 insertions(+), 7 deletions(-)
+
+diff --git a/mm/huge_memory.c b/mm/huge_memory.c
+index b6039623c7b8..7136026d6c1e 100644
+--- a/mm/huge_memory.c
++++ b/mm/huge_memory.c
+@@ -1489,7 +1489,7 @@ vm_fault_t do_huge_pmd_numa_page(struct vm_fault *vmf)
+ 	pmd = pmd_modify(oldpmd, vma->vm_page_prot);
+ 	pmd = pmd_mkyoung(pmd);
+ 	if (was_writable)
+-		pmd = pmd_mkwrite(pmd);
++		pmd = maybe_pmd_mkwrite(pmd, vma);
+ 	set_pmd_at(vma->vm_mm, haddr, vmf->pmd, pmd);
+ 	update_mmu_cache_pmd(vma, vmf->address, vmf->pmd);
+ 	spin_unlock(vmf->ptl);
+diff --git a/mm/memory.c b/mm/memory.c
+index 8f1de811a1dc..ae23b4c0a916 100644
+--- a/mm/memory.c
++++ b/mm/memory.c
+@@ -3774,8 +3774,7 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
+ 
+ 	entry = mk_pte(page, vma->vm_page_prot);
+ 	entry = pte_sw_mkyoung(entry);
+-	if (vma->vm_flags & VM_WRITE)
+-		entry = pte_mkwrite(pte_mkdirty(entry));
++	entry = maybe_mkwrite(pte_mkdirty(entry), vma);
+ 
+ 	vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,
+ 			&vmf->ptl);
+@@ -4409,7 +4408,7 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)
+ 	pte = pte_modify(old_pte, vma->vm_page_prot);
+ 	pte = pte_mkyoung(pte);
+ 	if (was_writable)
+-		pte = pte_mkwrite(pte);
++		pte = maybe_mkwrite(pte, vma);
+ 	ptep_modify_prot_commit(vma, vmf->address, vmf->pte, old_pte, pte);
+ 	update_mmu_cache(vma, vmf->address, vmf->pte);
+ 	pte_unmap_unlock(vmf->pte, vmf->ptl);
+diff --git a/mm/migrate.c b/mm/migrate.c
+index cf25b00f03c8..511610ff4a90 100644
+--- a/mm/migrate.c
++++ b/mm/migrate.c
+@@ -2781,8 +2781,7 @@ static void migrate_vma_insert_page(struct migrate_vma *migrate,
+ 		}
+ 	} else {
+ 		entry = mk_pte(page, vma->vm_page_prot);
+-		if (vma->vm_flags & VM_WRITE)
+-			entry = pte_mkwrite(pte_mkdirty(entry));
++		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
+ 	}
+ 
+ 	ptep = pte_offset_map_lock(mm, pmdp, addr, &ptl);
+diff --git a/mm/mprotect.c b/mm/mprotect.c
+index e552f5e0ccbd..7b8ac6f6b997 100644
+--- a/mm/mprotect.c
++++ b/mm/mprotect.c
+@@ -135,7 +135,7 @@ static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
+ 			if (dirty_accountable && pte_dirty(ptent) &&
+ 					(pte_soft_dirty(ptent) ||
+ 					 !(vma->vm_flags & VM_SOFTDIRTY))) {
+-				ptent = pte_mkwrite(ptent);
++				ptent = maybe_mkwrite(ptent, vma);
+ 			}
+ 			ptep_modify_prot_commit(vma, addr, pte, oldpte, ptent);
+ 			pages++;
+-- 
+2.34.1
+
diff --git a/0018-mm-Add-guard-pages-around-a-shadow-stack.patch b/0018-mm-Add-guard-pages-around-a-shadow-stack.patch
new file mode 100644
index 000000000..b47c4a930
--- /dev/null
+++ b/0018-mm-Add-guard-pages-around-a-shadow-stack.patch
@@ -0,0 +1,137 @@
+From 31180de610fed0f75a633f0bf3566533a527ccba Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:14 -0700
+Subject: [PATCH 18/39] mm: Add guard pages around a shadow stack.
+
+INCSSP(Q/D) increments shadow stack pointer and 'pops and discards' the
+first and the last elements in the range, effectively touches those memory
+areas.
+
+The maximum moving distance by INCSSPQ is 255 * 8 = 2040 bytes and
+255 * 4 = 1020 bytes by INCSSPD.  Both ranges are far from PAGE_SIZE.
+Thus, putting a gap page on both ends of a shadow stack prevents INCSSP,
+CALL, and RET from going beyond.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
+Cc: Kees Cook <keescook@chromium.org>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+
+===
+
+Yu-cheng v25:
+- Move SHADOW_STACK_GUARD_GAP to arch/x86/mm/mmap.c.
+
+Yu-cheng v24:
+- Instead changing vm_*_gap(), create x86-specific versions.
+---
+ arch/x86/include/asm/page_types.h |  7 +++++
+ arch/x86/mm/mmap.c                | 46 +++++++++++++++++++++++++++++++
+ include/linux/mm.h                |  4 +++
+ 3 files changed, 57 insertions(+)
+
+diff --git a/arch/x86/include/asm/page_types.h b/arch/x86/include/asm/page_types.h
+index a506a411474d..e1533fdc08b4 100644
+--- a/arch/x86/include/asm/page_types.h
++++ b/arch/x86/include/asm/page_types.h
+@@ -73,6 +73,13 @@ bool pfn_range_is_mapped(unsigned long start_pfn, unsigned long end_pfn);
+ 
+ extern void initmem_init(void);
+ 
++#define vm_start_gap vm_start_gap
++struct vm_area_struct;
++extern unsigned long vm_start_gap(struct vm_area_struct *vma);
++
++#define vm_end_gap vm_end_gap
++extern unsigned long vm_end_gap(struct vm_area_struct *vma);
++
+ #endif	/* !__ASSEMBLY__ */
+ 
+ #endif	/* _ASM_X86_PAGE_DEFS_H */
+diff --git a/arch/x86/mm/mmap.c b/arch/x86/mm/mmap.c
+index f3f52c5e2fd6..81f9325084d3 100644
+--- a/arch/x86/mm/mmap.c
++++ b/arch/x86/mm/mmap.c
+@@ -250,3 +250,49 @@ bool pfn_modify_allowed(unsigned long pfn, pgprot_t prot)
+ 		return false;
+ 	return true;
+ }
++
++/*
++ * Shadow stack pointer is moved by CALL, RET, and INCSSP(Q/D).  INCSSPQ
++ * moves shadow stack pointer up to 255 * 8 = ~2 KB (~1KB for INCSSPD) and
++ * touches the first and the last element in the range, which triggers a
++ * page fault if the range is not in a shadow stack.  Because of this,
++ * creating 4-KB guard pages around a shadow stack prevents these
++ * instructions from going beyond.
++ */
++#define SHADOW_STACK_GUARD_GAP PAGE_SIZE
++
++unsigned long vm_start_gap(struct vm_area_struct *vma)
++{
++	unsigned long vm_start = vma->vm_start;
++	unsigned long gap = 0;
++
++	if (vma->vm_flags & VM_GROWSDOWN)
++		gap = stack_guard_gap;
++	else if (vma->vm_flags & VM_SHADOW_STACK)
++		gap = SHADOW_STACK_GUARD_GAP;
++
++	if (gap != 0) {
++		vm_start -= gap;
++		if (vm_start > vma->vm_start)
++			vm_start = 0;
++	}
++	return vm_start;
++}
++
++unsigned long vm_end_gap(struct vm_area_struct *vma)
++{
++	unsigned long vm_end = vma->vm_end;
++	unsigned long gap = 0;
++
++	if (vma->vm_flags & VM_GROWSUP)
++		gap = stack_guard_gap;
++	else if (vma->vm_flags & VM_SHADOW_STACK)
++		gap = SHADOW_STACK_GUARD_GAP;
++
++	if (gap != 0) {
++		vm_end += gap;
++		if (vm_end < vma->vm_end)
++			vm_end = -PAGE_SIZE;
++	}
++	return vm_end;
++}
+diff --git a/include/linux/mm.h b/include/linux/mm.h
+index 998b9a1a5c18..649a14869d0b 100644
+--- a/include/linux/mm.h
++++ b/include/linux/mm.h
+@@ -2836,6 +2836,7 @@ struct vm_area_struct *vma_lookup(struct mm_struct *mm, unsigned long addr)
+ 	return vma;
+ }
+ 
++#ifndef vm_start_gap
+ static inline unsigned long vm_start_gap(struct vm_area_struct *vma)
+ {
+ 	unsigned long vm_start = vma->vm_start;
+@@ -2847,7 +2848,9 @@ static inline unsigned long vm_start_gap(struct vm_area_struct *vma)
+ 	}
+ 	return vm_start;
+ }
++#endif
+ 
++#ifndef vm_end_gap
+ static inline unsigned long vm_end_gap(struct vm_area_struct *vma)
+ {
+ 	unsigned long vm_end = vma->vm_end;
+@@ -2859,6 +2862,7 @@ static inline unsigned long vm_end_gap(struct vm_area_struct *vma)
+ 	}
+ 	return vm_end;
+ }
++#endif
+ 
+ static inline unsigned long vma_pages(struct vm_area_struct *vma)
+ {
+-- 
+2.34.1
+
diff --git a/0019-mm-mmap-Add-shadow-stack-pages-to-memory-accounting.patch b/0019-mm-mmap-Add-shadow-stack-pages-to-memory-accounting.patch
new file mode 100644
index 000000000..e26090f21
--- /dev/null
+++ b/0019-mm-mmap-Add-shadow-stack-pages-to-memory-accounting.patch
@@ -0,0 +1,102 @@
+From 4f765c1fe504ca6deebf380136d50b83eff782b9 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:15 -0700
+Subject: [PATCH 19/39] mm/mmap: Add shadow stack pages to memory accounting
+
+Account shadow stack pages to stack memory.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
+Cc: Kees Cook <keescook@chromium.org>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+
+===
+
+Yu-cheng v26:
+- Remove redundant #ifdef CONFIG_MMU.
+
+Yu-cheng v25:
+- Remove #ifdef CONFIG_ARCH_HAS_SHADOW_STACK for is_shadow_stack_mapping().
+
+Yu-cheng v24:
+- Change arch_shadow_stack_mapping() to is_shadow_stack_mapping().
+- Change VM_SHSTK to VM_SHADOW_STACK.
+---
+ arch/x86/include/asm/pgtable.h | 3 +++
+ arch/x86/mm/pgtable.c          | 5 +++++
+ include/linux/pgtable.h        | 8 ++++++++
+ mm/mmap.c                      | 5 +++++
+ 4 files changed, 21 insertions(+)
+
+diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
+index 8dee420e382e..83ef43886a2a 100644
+--- a/arch/x86/include/asm/pgtable.h
++++ b/arch/x86/include/asm/pgtable.h
+@@ -1640,6 +1640,9 @@ static inline bool arch_faults_on_old_pte(void)
+ #define maybe_mkwrite maybe_mkwrite
+ extern pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma);
+ 
++#define is_shadow_stack_mapping is_shadow_stack_mapping
++extern bool is_shadow_stack_mapping(vm_flags_t vm_flags);
++
+ #endif	/* __ASSEMBLY__ */
+ 
+ #endif /* _ASM_X86_PGTABLE_H */
+diff --git a/arch/x86/mm/pgtable.c b/arch/x86/mm/pgtable.c
+index c22c8e9c37e8..61a364b9ae0a 100644
+--- a/arch/x86/mm/pgtable.c
++++ b/arch/x86/mm/pgtable.c
+@@ -884,3 +884,8 @@ int pmd_free_pte_page(pmd_t *pmd, unsigned long addr)
+ 
+ #endif /* CONFIG_X86_64 */
+ #endif	/* CONFIG_HAVE_ARCH_HUGE_VMAP */
++
++bool is_shadow_stack_mapping(vm_flags_t vm_flags)
++{
++	return vm_flags & VM_SHADOW_STACK;
++}
+diff --git a/include/linux/pgtable.h b/include/linux/pgtable.h
+index e24d2c992b11..5ebe4677e13f 100644
+--- a/include/linux/pgtable.h
++++ b/include/linux/pgtable.h
+@@ -903,6 +903,14 @@ static inline void ptep_modify_prot_commit(struct vm_area_struct *vma,
+ 	__ptep_modify_prot_commit(vma, addr, ptep, pte);
+ }
+ #endif /* __HAVE_ARCH_PTEP_MODIFY_PROT_TRANSACTION */
++
++#ifndef is_shadow_stack_mapping
++static inline bool is_shadow_stack_mapping(vm_flags_t vm_flags)
++{
++	return false;
++}
++#endif
++
+ #endif /* CONFIG_MMU */
+ 
+ /*
+diff --git a/mm/mmap.c b/mm/mmap.c
+index bfb0ea164a90..080e02810d44 100644
+--- a/mm/mmap.c
++++ b/mm/mmap.c
+@@ -1709,6 +1709,9 @@ static inline int accountable_mapping(struct file *file, vm_flags_t vm_flags)
+ 	if (file && is_file_hugepages(file))
+ 		return 0;
+ 
++	if (is_shadow_stack_mapping(vm_flags))
++		return 1;
++
+ 	return (vm_flags & (VM_NORESERVE | VM_SHARED | VM_WRITE)) == VM_WRITE;
+ }
+ 
+@@ -3339,6 +3342,8 @@ void vm_stat_account(struct mm_struct *mm, vm_flags_t flags, long npages)
+ 		mm->stack_vm += npages;
+ 	else if (is_data_mapping(flags))
+ 		mm->data_vm += npages;
++	else if (is_shadow_stack_mapping(flags))
++		mm->stack_vm += npages;
+ }
+ 
+ static vm_fault_t special_mapping_fault(struct vm_fault *vmf);
+-- 
+2.34.1
+
diff --git a/0020-mm-Update-can_follow_write_pte-for-shadow-stack.patch b/0020-mm-Update-can_follow_write_pte-for-shadow-stack.patch
new file mode 100644
index 000000000..407d8c1f0
--- /dev/null
+++ b/0020-mm-Update-can_follow_write_pte-for-shadow-stack.patch
@@ -0,0 +1,109 @@
+From 557750bc9aaa540004ffff124adf61919f897bb7 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:16 -0700
+Subject: [PATCH 20/39] mm: Update can_follow_write_pte() for shadow stack
+
+Can_follow_write_pte() ensures a read-only page is COWed by checking the
+FOLL_COW flag, and uses pte_dirty() to validate the flag is still valid.
+
+Like a writable data page, a shadow stack page is writable, and becomes
+read-only during copy-on-write, but it is always dirty.  Thus, in the
+can_follow_write_pte() check, it belongs to the writable page case and
+should be excluded from the read-only page pte_dirty() check.  Apply
+the same changes to can_follow_write_pmd().
+
+While at it, also split the long line into smaller ones.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
+Cc: Kees Cook <keescook@chromium.org>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+
+===
+
+Yu-cheng v26:
+- Instead of passing vm_flags, pass down vma pointer to can_follow_write_*().
+
+Yu-cheng v25:
+- Split long line into smaller ones.
+
+Yu-cheng v24:
+- Change arch_shadow_stack_mapping() to is_shadow_stack_mapping().
+---
+ mm/gup.c         | 16 ++++++++++++----
+ mm/huge_memory.c | 16 ++++++++++++----
+ 2 files changed, 24 insertions(+), 8 deletions(-)
+
+diff --git a/mm/gup.c b/mm/gup.c
+index 2c51e9748a6a..670059feb728 100644
+--- a/mm/gup.c
++++ b/mm/gup.c
+@@ -464,10 +464,18 @@ static int follow_pfn_pte(struct vm_area_struct *vma, unsigned long address,
+  * FOLL_FORCE can write to even unwritable pte's, but only
+  * after we've gone through a COW cycle and they are dirty.
+  */
+-static inline bool can_follow_write_pte(pte_t pte, unsigned int flags)
++static inline bool can_follow_write_pte(pte_t pte, unsigned int flags,
++					struct vm_area_struct *vma)
+ {
+-	return pte_write(pte) ||
+-		((flags & FOLL_FORCE) && (flags & FOLL_COW) && pte_dirty(pte));
++	if (pte_write(pte))
++		return true;
++	if ((flags & (FOLL_FORCE | FOLL_COW)) != (FOLL_FORCE | FOLL_COW))
++		return false;
++	if (!pte_dirty(pte))
++		return false;
++	if (is_shadow_stack_mapping(vma->vm_flags))
++		return false;
++	return true;
+ }
+ 
+ static struct page *follow_page_pte(struct vm_area_struct *vma,
+@@ -510,7 +518,7 @@ static struct page *follow_page_pte(struct vm_area_struct *vma,
+ 	}
+ 	if ((flags & FOLL_NUMA) && pte_protnone(pte))
+ 		goto no_page;
+-	if ((flags & FOLL_WRITE) && !can_follow_write_pte(pte, flags)) {
++	if ((flags & FOLL_WRITE) && !can_follow_write_pte(pte, flags, vma)) {
+ 		pte_unmap_unlock(ptep, ptl);
+ 		return NULL;
+ 	}
+diff --git a/mm/huge_memory.c b/mm/huge_memory.c
+index 7136026d6c1e..84948ea39b10 100644
+--- a/mm/huge_memory.c
++++ b/mm/huge_memory.c
+@@ -1346,10 +1346,18 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf)
+  * FOLL_FORCE can write to even unwritable pmd's, but only
+  * after we've gone through a COW cycle and they are dirty.
+  */
+-static inline bool can_follow_write_pmd(pmd_t pmd, unsigned int flags)
++static inline bool can_follow_write_pmd(pmd_t pmd, unsigned int flags,
++					struct vm_area_struct *vma)
+ {
+-	return pmd_write(pmd) ||
+-	       ((flags & FOLL_FORCE) && (flags & FOLL_COW) && pmd_dirty(pmd));
++	if (pmd_write(pmd))
++		return true;
++	if ((flags & (FOLL_FORCE | FOLL_COW)) != (FOLL_FORCE | FOLL_COW))
++		return false;
++	if (!pmd_dirty(pmd))
++		return false;
++	if (is_shadow_stack_mapping(vma->vm_flags))
++		return false;
++	return true;
+ }
+ 
+ struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,
+@@ -1362,7 +1370,7 @@ struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,
+ 
+ 	assert_spin_locked(pmd_lockptr(mm, pmd));
+ 
+-	if (flags & FOLL_WRITE && !can_follow_write_pmd(*pmd, flags))
++	if (flags & FOLL_WRITE && !can_follow_write_pmd(*pmd, flags, vma))
+ 		goto out;
+ 
+ 	/* Avoid dumping huge zero page */
+-- 
+2.34.1
+
diff --git a/0021-mm-mprotect-Exclude-shadow-stack-from-preserve_write.patch b/0021-mm-mprotect-Exclude-shadow-stack-from-preserve_write.patch
new file mode 100644
index 000000000..a6a8e5f0c
--- /dev/null
+++ b/0021-mm-mprotect-Exclude-shadow-stack-from-preserve_write.patch
@@ -0,0 +1,68 @@
+From e851e515b127188707009217852f95a964026f1a Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:17 -0700
+Subject: [PATCH 21/39] mm/mprotect: Exclude shadow stack from preserve_write
+
+In change_pte_range(), when a PTE is changed for prot_numa, _PAGE_RW is
+preserved to avoid the additional write fault after the NUMA hinting fault.
+However, pte_write() now includes both normal writable and shadow stack
+(RW=0, Dirty=1) PTEs, but the latter does not have _PAGE_RW and has no need
+to preserve it.
+
+Exclude shadow stack from preserve_write test, and apply the same change to
+change_huge_pmd().
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+===
+
+v25:
+- Move is_shadow_stack_mapping() to a separate line.
+
+v24:
+- Change arch_shadow_stack_mapping() to is_shadow_stack_mapping().
+---
+ mm/huge_memory.c | 7 +++++++
+ mm/mprotect.c    | 7 +++++++
+ 2 files changed, 14 insertions(+)
+
+diff --git a/mm/huge_memory.c b/mm/huge_memory.c
+index 84948ea39b10..b6fb57020c94 100644
+--- a/mm/huge_memory.c
++++ b/mm/huge_memory.c
+@@ -1750,6 +1750,13 @@ int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,
+ 		return 0;
+ 
+ 	preserve_write = prot_numa && pmd_write(*pmd);
++
++	/*
++	 * Preserve only normal writable huge PMD, but not shadow
++	 * stack (RW=0, Dirty=1).
++	 */
++	if (is_shadow_stack_mapping(vma->vm_flags))
++		preserve_write = false;
+ 	ret = 1;
+ 
+ #ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION
+diff --git a/mm/mprotect.c b/mm/mprotect.c
+index 7b8ac6f6b997..b589b0625ca4 100644
+--- a/mm/mprotect.c
++++ b/mm/mprotect.c
+@@ -77,6 +77,13 @@ static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
+ 			pte_t ptent;
+ 			bool preserve_write = prot_numa && pte_write(oldpte);
+ 
++			/*
++			 * Preserve only normal writable PTE, but not shadow
++			 * stack (RW=0, Dirty=1).
++			 */
++			if (is_shadow_stack_mapping(vma->vm_flags))
++				preserve_write = false;
++
+ 			/*
+ 			 * Avoid trapping faults against the zero or KSM
+ 			 * pages. See similar comment in change_huge_pmd.
+-- 
+2.34.1
+
diff --git a/0022-mm-Re-introduce-vm_flags-to-do_mmap.patch b/0022-mm-Re-introduce-vm_flags-to-do_mmap.patch
new file mode 100644
index 000000000..40c767050
--- /dev/null
+++ b/0022-mm-Re-introduce-vm_flags-to-do_mmap.patch
@@ -0,0 +1,153 @@
+From fb5446372fb6965dd1553c894db9283ed22a8d0b Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:18 -0700
+Subject: [PATCH 22/39] mm: Re-introduce vm_flags to do_mmap()
+
+There was no more caller passing vm_flags to do_mmap(), and vm_flags was
+removed from the function's input by:
+
+    commit 45e55300f114 ("mm: remove unnecessary wrapper function do_mmap_pgoff()").
+
+There is a new user now.  Shadow stack allocation passes VM_SHADOW_STACK to
+do_mmap().  Thus, re-introduce vm_flags to do_mmap().
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Peter Collingbourne <pcc@google.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+Reviewed-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
+Cc: Andrew Morton <akpm@linux-foundation.org>
+Cc: Oleg Nesterov <oleg@redhat.com>
+Cc: linux-mm@kvack.org
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+===
+---
+ fs/aio.c           |  2 +-
+ include/linux/mm.h |  3 ++-
+ ipc/shm.c          |  2 +-
+ mm/mmap.c          | 10 +++++-----
+ mm/nommu.c         |  4 ++--
+ mm/util.c          |  2 +-
+ 6 files changed, 12 insertions(+), 11 deletions(-)
+
+diff --git a/fs/aio.c b/fs/aio.c
+index f6f1cbffef9e..587e4640890e 100644
+--- a/fs/aio.c
++++ b/fs/aio.c
+@@ -527,7 +527,7 @@ static int aio_setup_ring(struct kioctx *ctx, unsigned int nr_events)
+ 
+ 	ctx->mmap_base = do_mmap(ctx->aio_ring_file, 0, ctx->mmap_size,
+ 				 PROT_READ | PROT_WRITE,
+-				 MAP_SHARED, 0, &unused, NULL);
++				 MAP_SHARED, 0, 0, &unused, NULL);
+ 	mmap_write_unlock(mm);
+ 	if (IS_ERR((void *)ctx->mmap_base)) {
+ 		ctx->mmap_size = 0;
+diff --git a/include/linux/mm.h b/include/linux/mm.h
+index 649a14869d0b..104abedc7187 100644
+--- a/include/linux/mm.h
++++ b/include/linux/mm.h
+@@ -2728,7 +2728,8 @@ extern unsigned long mmap_region(struct file *file, unsigned long addr,
+ 	struct list_head *uf);
+ extern unsigned long do_mmap(struct file *file, unsigned long addr,
+ 	unsigned long len, unsigned long prot, unsigned long flags,
+-	unsigned long pgoff, unsigned long *populate, struct list_head *uf);
++	vm_flags_t vm_flags, unsigned long pgoff, unsigned long *populate,
++	struct list_head *uf);
+ extern int __do_munmap(struct mm_struct *, unsigned long, size_t,
+ 		       struct list_head *uf, bool downgrade);
+ extern int do_munmap(struct mm_struct *, unsigned long, size_t,
+diff --git a/ipc/shm.c b/ipc/shm.c
+index b3048ebd5c31..f236b3e14ec4 100644
+--- a/ipc/shm.c
++++ b/ipc/shm.c
+@@ -1646,7 +1646,7 @@ long do_shmat(int shmid, char __user *shmaddr, int shmflg,
+ 			goto invalid;
+ 	}
+ 
+-	addr = do_mmap(file, addr, size, prot, flags, 0, &populate, NULL);
++	addr = do_mmap(file, addr, size, prot, flags, 0, 0, &populate, NULL);
+ 	*raddr = addr;
+ 	err = 0;
+ 	if (IS_ERR_VALUE(addr))
+diff --git a/mm/mmap.c b/mm/mmap.c
+index 080e02810d44..861dc8046bf1 100644
+--- a/mm/mmap.c
++++ b/mm/mmap.c
+@@ -1403,11 +1403,11 @@ static inline bool file_mmap_ok(struct file *file, struct inode *inode,
+  */
+ unsigned long do_mmap(struct file *file, unsigned long addr,
+ 			unsigned long len, unsigned long prot,
+-			unsigned long flags, unsigned long pgoff,
+-			unsigned long *populate, struct list_head *uf)
++			unsigned long flags, vm_flags_t vm_flags,
++			unsigned long pgoff, unsigned long *populate,
++			struct list_head *uf)
+ {
+ 	struct mm_struct *mm = current->mm;
+-	vm_flags_t vm_flags;
+ 	int pkey = 0;
+ 
+ 	*populate = 0;
+@@ -1467,7 +1467,7 @@ unsigned long do_mmap(struct file *file, unsigned long addr,
+ 	 * to. we assume access permissions have been handled by the open
+ 	 * of the memory object, so we don't do any here.
+ 	 */
+-	vm_flags = calc_vm_prot_bits(prot, pkey) | calc_vm_flag_bits(flags) |
++	vm_flags |= calc_vm_prot_bits(prot, pkey) | calc_vm_flag_bits(flags) |
+ 			mm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;
+ 
+ 	if (flags & MAP_LOCKED)
+@@ -3005,7 +3005,7 @@ SYSCALL_DEFINE5(remap_file_pages, unsigned long, start, unsigned long, size,
+ 
+ 	file = get_file(vma->vm_file);
+ 	ret = do_mmap(vma->vm_file, start, size,
+-			prot, flags, pgoff, &populate, NULL);
++			prot, flags, 0, pgoff, &populate, NULL);
+ 	fput(file);
+ out:
+ 	mmap_write_unlock(mm);
+diff --git a/mm/nommu.c b/mm/nommu.c
+index 55a9e48a7a02..a6e0243cd69b 100644
+--- a/mm/nommu.c
++++ b/mm/nommu.c
+@@ -1057,6 +1057,7 @@ unsigned long do_mmap(struct file *file,
+ 			unsigned long len,
+ 			unsigned long prot,
+ 			unsigned long flags,
++			vm_flags_t vm_flags,
+ 			unsigned long pgoff,
+ 			unsigned long *populate,
+ 			struct list_head *uf)
+@@ -1064,7 +1065,6 @@ unsigned long do_mmap(struct file *file,
+ 	struct vm_area_struct *vma;
+ 	struct vm_region *region;
+ 	struct rb_node *rb;
+-	vm_flags_t vm_flags;
+ 	unsigned long capabilities, result;
+ 	int ret;
+ 
+@@ -1083,7 +1083,7 @@ unsigned long do_mmap(struct file *file,
+ 
+ 	/* we've determined that we can make the mapping, now translate what we
+ 	 * now know into VMA flags */
+-	vm_flags = determine_vm_flags(file, prot, flags, capabilities);
++	vm_flags |= determine_vm_flags(file, prot, flags, capabilities);
+ 
+ 	/* we're going to need to record the mapping */
+ 	region = kmem_cache_zalloc(vm_region_jar, GFP_KERNEL);
+diff --git a/mm/util.c b/mm/util.c
+index 741ba32a43ac..59c3a117705a 100644
+--- a/mm/util.c
++++ b/mm/util.c
+@@ -516,7 +516,7 @@ unsigned long vm_mmap_pgoff(struct file *file, unsigned long addr,
+ 	if (!ret) {
+ 		if (mmap_write_lock_killable(mm))
+ 			return -EINTR;
+-		ret = do_mmap(file, addr, len, prot, flag, pgoff, &populate,
++		ret = do_mmap(file, addr, len, prot, flag, 0, pgoff, &populate,
+ 			      &uf);
+ 		mmap_write_unlock(mm);
+ 		userfaultfd_unmap_complete(mm, &uf);
+-- 
+2.34.1
+
diff --git a/0023-x86-fpu-Add-helpers-for-modifying-supervisor-xstate.patch b/0023-x86-fpu-Add-helpers-for-modifying-supervisor-xstate.patch
new file mode 100644
index 000000000..c536133bf
--- /dev/null
+++ b/0023-x86-fpu-Add-helpers-for-modifying-supervisor-xstate.patch
@@ -0,0 +1,215 @@
+From e0d675bb6bbdcbae342333adfb07bddcd92d8d7a Mon Sep 17 00:00:00 2001
+From: Rick Edgecombe <rick.p.edgecombe@intel.com>
+Date: Fri, 29 Oct 2021 16:11:39 -0700
+Subject: [PATCH 23/39] x86/fpu: Add helpers for modifying supervisor xstate
+
+Add helpers that can be used to modify supervisor xstate safely for the
+current task.
+
+State for supervisors xstate based features can be live and
+accesses via MSR's, or saved in memory in an xsave buffer. When the
+kernel needs to modify this state it needs to be sure to operate on it
+in the right place, so the modifications don't get clobbered.
+
+In the past supervisor xstate features have used get_xsave_addr()
+directly, and performed open coded logic to determine whether to operate
+on the saved state or the MSR's directly. This has posed two problems:
+ 1. It has functionally been got wrong more than once.
+ 2. To reduce code, less common path's are not optimized. Determination
+    of which path's are less common is based on assumptions about far away
+    code that could change.
+
+In addition, now that get_xsave_addr() is not available outside of the
+core fpu code, there isn't even a way for these supervisor features to
+modify the in memory state.
+
+To resolve these problems, add some helpers that encapsulate the correct
+logic to operate on the correct copy of the state. Map the MSR's to the
+struct field location in a case statements in __get_xsave_member().
+
+Use the helpers like this, to write to either the MSR or saves state:
+void *xstate;
+
+xstate = start_update_xsave_msrs(XFEATURE_FOO);
+r = xsave_rdmsrl(state, MSR_IA32_FOO_1, &val)
+if (r)
+	xsave_wrmsrl(state, MSR_IA32_FOO_2, FOO_ENABLE);
+end_update_xsave_msrs();
+
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+
+===
+
+v1:
+- New patch
+---
+ arch/x86/include/asm/fpu/api.h |   6 ++
+ arch/x86/kernel/fpu/xstate.c   | 133 +++++++++++++++++++++++++++++++++
+ 2 files changed, 139 insertions(+)
+
+diff --git a/arch/x86/include/asm/fpu/api.h b/arch/x86/include/asm/fpu/api.h
+index c2767a6a387e..600b51b9ea61 100644
+--- a/arch/x86/include/asm/fpu/api.h
++++ b/arch/x86/include/asm/fpu/api.h
+@@ -13,6 +13,7 @@
+ #include <linux/bottom_half.h>
+ 
+ #include <asm/fpu/types.h>
++#include <linux/sched.h>
+ 
+ /*
+  * Use kernel_fpu_begin/end() if you intend to use FPU in kernel context. It
+@@ -154,4 +155,9 @@ static inline bool fpstate_is_confidential(struct fpu_guest *gfpu)
+ struct task_struct;
+ extern long fpu_xstate_prctl(struct task_struct *tsk, int option, unsigned long arg2);
+ 
++void *start_update_xsave_msrs(int xfeature_nr);
++void end_update_xsave_msrs(void);
++int xsave_rdmsrl(void *state, unsigned int msr, unsigned long long *p);
++int xsave_wrmsrl(void *state, u32 msr, u64 val);
++int xsave_set_clear_bits_msrl(void *state, u32 msr, u64 set, u64 clear);
+ #endif /* _ASM_X86_FPU_API_H */
+diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c
+index 1355b7119afe..e6be3b4fb3cc 100644
+--- a/arch/x86/kernel/fpu/xstate.c
++++ b/arch/x86/kernel/fpu/xstate.c
+@@ -1826,3 +1826,136 @@ int proc_pid_arch_status(struct seq_file *m, struct pid_namespace *ns,
+ 	return 0;
+ }
+ #endif /* CONFIG_PROC_PID_ARCH_STATUS */
++
++static u64 *__get_xsave_member(void *xstate, u32 msr)
++{
++	switch (msr) {
++	default:
++		WARN_ONCE(1, "x86/fpu: unsupported xstate msr (%u)\n", msr);
++		return NULL;
++	}
++}
++
++/*
++ * Return a pointer to the xstate for the feature if it should be used, or NULL
++ * if the MSRs should be written to directly. To do this safely using the
++ * associated read/write helpers are required.
++ */
++void *start_update_xsave_msrs(int xfeature_nr)
++{
++	void *xstate;
++
++	/*
++	 * fpregs_lock() only disables preemption (mostly). So modifing state
++	 * in an interrupt could screw up some in progress fpregs operation,
++	 * but appear to work. Warn about it.
++	 */
++	WARN_ON_ONCE(!in_task());
++	WARN_ON_ONCE(current->flags & PF_KTHREAD);
++
++	fpregs_lock();
++
++	fpregs_assert_state_consistent();
++
++	/*
++	 * If the registers don't need to be reloaded. Go ahead and operate on the
++	 * registers.
++	 */
++	if (!test_thread_flag(TIF_NEED_FPU_LOAD))
++		return NULL;
++
++	xstate = get_xsave_addr(&current->thread.fpu.fpstate->regs.xsave, xfeature_nr);
++
++	/*
++	 * If regs are in the init state, they can't be retrieved from
++	 * init_fpstate due to the init optimization, but are not nessarily
++	 * zero. The only option is to restore to make everything live and
++	 * operate on registers. This will clear TIF_NEED_FPU_LOAD.
++	 *
++	 * Otherwise, if not in the init state but TIF_NEED_FPU_LOAD is set,
++	 * operate on the buffer. The registers will be restored before going
++	 * to userspace in any case, but the task might get preempted before
++	 * then, so this possibly saves an xsave.
++	 */
++	if (!xstate)
++		fpregs_restore_userregs();
++	return xstate;
++}
++
++void end_update_xsave_msrs(void)
++{
++	fpregs_unlock();
++}
++
++/*
++ * When TIF_NEED_FPU_LOAD is set and fpregs_state_valid() is true, the saved
++ * state and fp state match. In this case, the kernel has some good options -
++ * it can skip the restore before returning to userspace or it could skip
++ * an xsave if preempted before then.
++ *
++ * But if this correspondence is broken by either a write to the in-memory
++ * buffer or the registers, the kernel needs to be notified so it doesn't miss
++ * an xsave or restore. __xsave_msrl_prepare_write() peforms this check and
++ * notifies the kernel if needed. Use before writes only, to not take away
++ * the kernel's options when not required.
++ *
++ * If TIF_NEED_FPU_LOAD is set, then the logic in start_update_xsave_msrs()
++ * must have resulted in targeting the in-memory state, so invaliding the
++ * registers is the right thing to do.
++ */
++static void __xsave_msrl_prepare_write(void)
++{
++	if (test_thread_flag(TIF_NEED_FPU_LOAD) &&
++	    fpregs_state_valid(&current->thread.fpu, smp_processor_id()))
++		__fpu_invalidate_fpregs_state(&current->thread.fpu);
++}
++
++int xsave_rdmsrl(void *xstate, unsigned int msr, unsigned long long *p)
++{
++	u64 *member_ptr;
++
++	if (!xstate)
++		return rdmsrl_safe(msr, p);
++
++	member_ptr = __get_xsave_member(xstate, msr);
++	if (!member_ptr)
++		return 1;
++
++	*p = *member_ptr;
++
++	return 0;
++}
++
++int xsave_wrmsrl(void *xstate, u32 msr, u64 val)
++{
++	u64 *member_ptr;
++
++	__xsave_msrl_prepare_write();
++	if (!xstate)
++		return wrmsrl_safe(msr, val);
++
++	member_ptr = __get_xsave_member(xstate, msr);
++	if (!member_ptr)
++		return 1;
++
++	*member_ptr = val;
++
++	return 0;
++}
++
++int xsave_set_clear_bits_msrl(void *xstate, u32 msr, u64 set, u64 clear)
++{
++	u64 val, new_val;
++	int ret;
++
++	ret = xsave_rdmsrl(xstate, msr, &val);
++	if (ret)
++		return ret;
++
++	new_val = (val & ~clear) | set;
++
++	if (new_val != val)
++		return xsave_wrmsrl(xstate, msr, new_val);
++
++	return 0;
++}
+-- 
+2.34.1
+
diff --git a/0024-x86-cet-shstk-Add-user-mode-shadow-stack-support.patch b/0024-x86-cet-shstk-Add-user-mode-shadow-stack-support.patch
new file mode 100644
index 000000000..5b2eba19a
--- /dev/null
+++ b/0024-x86-cet-shstk-Add-user-mode-shadow-stack-support.patch
@@ -0,0 +1,270 @@
+From 321c02b5e43f9f9a7aa6e1688895e99fae1c0789 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:19 -0700
+Subject: [PATCH 24/39] x86/cet/shstk: Add user-mode shadow stack support
+
+Introduce basic shadow stack enabling/disabling/allocation routines.
+A task's shadow stack is allocated from memory with VM_SHADOW_STACK flag
+and has a fixed size of min(RLIMIT_STACK, 4GB).
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Cc: Kees Cook <keescook@chromium.org>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+
+===
+
+v1:
+- Switch to xsave helpers
+
+Yu-cheng v30:
+- Remove superfluous comments for struct thread_shstk.
+- Replace 'populate' with 'unused'.
+
+Yu-cheng v28:
+- Update shstk_setup() with wrmsrl_safe(), returns success when shadow
+  stack feature is not present (since this is a setup function).
+
+Yu-cheng v27:
+- Change 'struct cet_status' to 'struct thread_shstk', and change member
+  types from unsigned long to u64.
+- Re-order local variables in reverse order of length.
+- WARN_ON_ONCE() when vm_munmap() fails.
+---
+ arch/x86/include/asm/cet.h       |  27 ++++++
+ arch/x86/include/asm/processor.h |   5 ++
+ arch/x86/kernel/Makefile         |   1 +
+ arch/x86/kernel/fpu/xstate.c     |   4 +
+ arch/x86/kernel/shstk.c          | 137 +++++++++++++++++++++++++++++++
+ 5 files changed, 174 insertions(+)
+ create mode 100644 arch/x86/include/asm/cet.h
+ create mode 100644 arch/x86/kernel/shstk.c
+
+diff --git a/arch/x86/include/asm/cet.h b/arch/x86/include/asm/cet.h
+new file mode 100644
+index 000000000000..bf1f1bf4d734
+--- /dev/null
++++ b/arch/x86/include/asm/cet.h
+@@ -0,0 +1,27 @@
++/* SPDX-License-Identifier: GPL-2.0 */
++#ifndef _ASM_X86_CET_H
++#define _ASM_X86_CET_H
++
++#ifndef __ASSEMBLY__
++#include <linux/types.h>
++
++struct task_struct;
++
++struct thread_shstk {
++	u64	base;
++	u64	size;
++};
++
++#ifdef CONFIG_X86_SHADOW_STACK
++void shstk_setup(void);
++void shstk_free(struct task_struct *p);
++void shstk_disable(void);
++#else
++static inline void shstk_setup(void) { return 0; }
++static inline void shstk_free(struct task_struct *p) {}
++static inline void shstk_disable(void) {}
++#endif
++
++#endif /* __ASSEMBLY__ */
++
++#endif /* _ASM_X86_CET_H */
+diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
+index 355d38c0cf60..ae41076f3a29 100644
+--- a/arch/x86/include/asm/processor.h
++++ b/arch/x86/include/asm/processor.h
+@@ -27,6 +27,7 @@ struct vm86;
+ #include <asm/unwind_hints.h>
+ #include <asm/vmxfeatures.h>
+ #include <asm/vdso/processor.h>
++#include <asm/cet.h>
+ 
+ #include <linux/personality.h>
+ #include <linux/cache.h>
+@@ -528,6 +529,10 @@ struct thread_struct {
+ 	 */
+ 	u32			pkru;
+ 
++#ifdef CONFIG_X86_SHADOW_STACK
++	struct thread_shstk	shstk;
++#endif
++
+ 	/* Floating point and extended processor state */
+ 	struct fpu		fpu;
+ 	/*
+diff --git a/arch/x86/kernel/Makefile b/arch/x86/kernel/Makefile
+index 2ff3e600f426..b97610b46e61 100644
+--- a/arch/x86/kernel/Makefile
++++ b/arch/x86/kernel/Makefile
+@@ -153,6 +153,7 @@ obj-$(CONFIG_AMD_MEM_ENCRYPT)		+= sev.o
+ 
+ obj-$(CONFIG_ARCH_HAS_CC_PLATFORM)	+= cc_platform.o
+ 
++obj-$(CONFIG_X86_SHADOW_STACK)		+= shstk.o
+ ###
+ # 64 bit specific files
+ ifeq ($(CONFIG_X86_64),y)
+diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c
+index e6be3b4fb3cc..a0dc4c2664e9 100644
+--- a/arch/x86/kernel/fpu/xstate.c
++++ b/arch/x86/kernel/fpu/xstate.c
+@@ -1830,6 +1830,10 @@ int proc_pid_arch_status(struct seq_file *m, struct pid_namespace *ns,
+ static u64 *__get_xsave_member(void *xstate, u32 msr)
+ {
+ 	switch (msr) {
++	case MSR_IA32_PL3_SSP:
++		return &((struct cet_user_state *)xstate)->user_ssp;
++	case MSR_IA32_U_CET:
++		return &((struct cet_user_state *)xstate)->user_cet;
+ 	default:
+ 		WARN_ONCE(1, "x86/fpu: unsupported xstate msr (%u)\n", msr);
+ 		return NULL;
+diff --git a/arch/x86/kernel/shstk.c b/arch/x86/kernel/shstk.c
+new file mode 100644
+index 000000000000..f3bf3f4dfbad
+--- /dev/null
++++ b/arch/x86/kernel/shstk.c
+@@ -0,0 +1,137 @@
++// SPDX-License-Identifier: GPL-2.0
++/*
++ * shstk.c - Intel shadow stack support
++ *
++ * Copyright (c) 2021, Intel Corporation.
++ * Yu-cheng Yu <yu-cheng.yu@intel.com>
++ */
++
++#include <linux/types.h>
++#include <linux/mm.h>
++#include <linux/mman.h>
++#include <linux/slab.h>
++#include <linux/uaccess.h>
++#include <linux/sched/signal.h>
++#include <linux/compat.h>
++#include <linux/sizes.h>
++#include <linux/user.h>
++#include <asm/msr.h>
++#include <asm/fpu/internal.h>
++#include <asm/fpu/xstate.h>
++#include <asm/fpu/types.h>
++#include <asm/cet.h>
++#include <asm/special_insns.h>
++#include <asm/fpu/api.h>
++
++static unsigned long alloc_shstk(unsigned long size)
++{
++	int flags = MAP_ANONYMOUS | MAP_PRIVATE;
++	struct mm_struct *mm = current->mm;
++	unsigned long addr, unused;
++
++	mmap_write_lock(mm);
++	addr = do_mmap(NULL, 0, size, PROT_READ, flags, VM_SHADOW_STACK, 0,
++		       &unused, NULL);
++	mmap_write_unlock(mm);
++
++	return addr;
++}
++
++static void unmap_shadow_stack(u64 base, u64 size)
++{
++	while (1) {
++		int r;
++
++		r = vm_munmap(base, size);
++
++		/*
++		 * vm_munmap() returns -EINTR when mmap_lock is held by
++		 * something else, and that lock should not be held for a
++		 * long time.  Retry it for the case.
++		 */
++		if (r == -EINTR) {
++			cond_resched();
++			continue;
++		}
++
++		/*
++		 * For all other types of vm_munmap() failure, either the
++		 * system is out of memory or there is bug.
++		 */
++		WARN_ON_ONCE(r);
++		break;
++	}
++}
++
++void shstk_setup(void)
++{
++	struct thread_shstk *shstk = &current->thread.shstk;
++	unsigned long addr, size;
++	void *xstate;
++	int err;
++
++	if (!cpu_feature_enabled(X86_FEATURE_SHSTK) ||
++	    shstk->size ||
++	    shstk->base)
++		return;
++
++	size = PAGE_ALIGN(min_t(unsigned long long, rlimit(RLIMIT_STACK), SZ_4G));
++	addr = alloc_shstk(size);
++	if (IS_ERR_VALUE(addr))
++		return;
++
++	xstate = start_update_xsave_msrs(XFEATURE_CET_USER);
++	err = xsave_wrmsrl(xstate, MSR_IA32_PL3_SSP, addr + size);
++	if (!err)
++		err = xsave_wrmsrl(xstate, MSR_IA32_U_CET, CET_SHSTK_EN);
++	end_update_xsave_msrs();
++
++	if (err) {
++		/*
++		 * Don't leak shadow stack if something went wrong with writing the
++		 * msrs. Warn about it because things may be in a weird state.
++		 */
++		WARN_ON_ONCE(1);
++		unmap_shadow_stack(addr, size);
++		return;
++	}
++
++	shstk->base = addr;
++	shstk->size = size;
++}
++
++void shstk_free(struct task_struct *tsk)
++{
++	struct thread_shstk *shstk = &tsk->thread.shstk;
++
++	if (!cpu_feature_enabled(X86_FEATURE_SHSTK) ||
++	    !shstk->size ||
++	    !shstk->base)
++		return;
++
++	if (!tsk->mm)
++		return;
++
++	unmap_shadow_stack(shstk->base, shstk->size);
++
++	shstk->base = 0;
++	shstk->size = 0;
++}
++
++void shstk_disable(void)
++{
++	struct thread_shstk *shstk = &current->thread.shstk;
++	void *xstate;
++
++	if (!cpu_feature_enabled(X86_FEATURE_SHSTK) ||
++	    !shstk->size ||
++	    !shstk->base)
++		return;
++
++	xstate = start_update_xsave_msrs(XFEATURE_CET_USER);
++	WARN_ON_ONCE(xsave_set_clear_bits_msrl(xstate, MSR_IA32_U_CET, 0, CET_SHSTK_EN));
++	WARN_ON_ONCE(xsave_wrmsrl(xstate, MSR_IA32_PL3_SSP, 0));
++	end_update_xsave_msrs();
++
++	shstk_free(current);
++}
+-- 
+2.34.1
+
diff --git a/0025-x86-process-Change-copy_thread-argument-arg-to-stack.patch b/0025-x86-process-Change-copy_thread-argument-arg-to-stack.patch
new file mode 100644
index 000000000..3984c37af
--- /dev/null
+++ b/0025-x86-process-Change-copy_thread-argument-arg-to-stack.patch
@@ -0,0 +1,53 @@
+From 9d1cc27f6ab4acaa0b68178ade3a0430abac2465 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:20 -0700
+Subject: [PATCH 25/39] x86/process: Change copy_thread() argument 'arg' to
+ 'stack_size'
+
+The single call site of copy_thread() passes stack size in 'arg'.  To make
+this clear and in preparation of using this argument for shadow stack
+allocation, change 'arg' to 'stack_size'.  No functional changes.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+---
+ arch/x86/kernel/process.c | 9 +++++----
+ 1 file changed, 5 insertions(+), 4 deletions(-)
+
+diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
+index 04143a653a8a..1d557bea15c1 100644
+--- a/arch/x86/kernel/process.c
++++ b/arch/x86/kernel/process.c
+@@ -130,8 +130,9 @@ static int set_new_tls(struct task_struct *p, unsigned long tls)
+ 		return do_set_thread_area_64(p, ARCH_SET_FS, tls);
+ }
+ 
+-int copy_thread(unsigned long clone_flags, unsigned long sp, unsigned long arg,
+-		struct task_struct *p, unsigned long tls)
++int copy_thread(unsigned long clone_flags, unsigned long sp,
++		unsigned long stack_size, struct task_struct *p,
++		unsigned long tls)
+ {
+ 	struct inactive_task_frame *frame;
+ 	struct fork_frame *fork_frame;
+@@ -175,7 +176,7 @@ int copy_thread(unsigned long clone_flags, unsigned long sp, unsigned long arg,
+ 	if (unlikely(p->flags & PF_KTHREAD)) {
+ 		p->thread.pkru = pkru_get_init_value();
+ 		memset(childregs, 0, sizeof(struct pt_regs));
+-		kthread_frame_init(frame, sp, arg);
++		kthread_frame_init(frame, sp, stack_size);
+ 		return 0;
+ 	}
+ 
+@@ -208,7 +209,7 @@ int copy_thread(unsigned long clone_flags, unsigned long sp, unsigned long arg,
+ 		 */
+ 		childregs->sp = 0;
+ 		childregs->ip = 0;
+-		kthread_frame_init(frame, sp, arg);
++		kthread_frame_init(frame, sp, stack_size);
+ 		return 0;
+ 	}
+ 
+-- 
+2.34.1
+
diff --git a/0026-x86-fpu-Add-unsafe-xsave-buffer-helpers.patch b/0026-x86-fpu-Add-unsafe-xsave-buffer-helpers.patch
new file mode 100644
index 000000000..4af756ad9
--- /dev/null
+++ b/0026-x86-fpu-Add-unsafe-xsave-buffer-helpers.patch
@@ -0,0 +1,96 @@
+From a1b21434f4c50f6c5cb5b04cb81954fcf4c3cfb7 Mon Sep 17 00:00:00 2001
+From: Rick Edgecombe <rick.p.edgecombe@intel.com>
+Date: Fri, 3 Dec 2021 10:30:38 -0800
+Subject: [PATCH 26/39] x86/fpu: Add unsafe xsave buffer helpers
+
+CET will need to modify the xsave buffer of a new FPU that was just
+created in the process of copying a thread. In this case the normal
+helpers will not work, because they operate on the current thread's FPU.
+
+So add unsafe helpers to allow for this kind of modifcations. Make the
+unsafe helpers operate on the MSR like the safe helpers for symmetry and
+to avoid exposing the underling xsave structures. Don't add a read
+helper because it is not needed at this time.
+
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+---
+ arch/x86/include/asm/fpu/api.h |  9 ++++++---
+ arch/x86/kernel/fpu/xstate.c   | 27 ++++++++++++++++++++++-----
+ 2 files changed, 28 insertions(+), 8 deletions(-)
+
+diff --git a/arch/x86/include/asm/fpu/api.h b/arch/x86/include/asm/fpu/api.h
+index 600b51b9ea61..e31d8f372e8a 100644
+--- a/arch/x86/include/asm/fpu/api.h
++++ b/arch/x86/include/asm/fpu/api.h
+@@ -157,7 +157,10 @@ extern long fpu_xstate_prctl(struct task_struct *tsk, int option, unsigned long
+ 
+ void *start_update_xsave_msrs(int xfeature_nr);
+ void end_update_xsave_msrs(void);
+-int xsave_rdmsrl(void *state, unsigned int msr, unsigned long long *p);
+-int xsave_wrmsrl(void *state, u32 msr, u64 val);
+-int xsave_set_clear_bits_msrl(void *state, u32 msr, u64 set, u64 clear);
++int xsave_rdmsrl(void *xstate, unsigned int msr, unsigned long long *p);
++int xsave_wrmsrl(void *xstate, u32 msr, u64 val);
++int xsave_set_clear_bits_msrl(void *xstate, u32 msr, u64 set, u64 clear);
++
++void *get_xsave_buffer_unsafe(struct fpu *fpu, int xfeature_nr);
++int xsave_wrmsrl_unsafe(void *xstate, u32 msr, u64 val);
+ #endif /* _ASM_X86_FPU_API_H */
+diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c
+index a0dc4c2664e9..a68f6fc4346a 100644
+--- a/arch/x86/kernel/fpu/xstate.c
++++ b/arch/x86/kernel/fpu/xstate.c
+@@ -1840,6 +1840,17 @@ static u64 *__get_xsave_member(void *xstate, u32 msr)
+ 	}
+ }
+ 
++/*
++ * Operate on the xsave buffer directly. It makes no gaurantees that the
++ * buffer will stay valid now or in the futre. This function is pretty
++ * much only useful when the caller knows the fpu's thread can't be
++ * scheduled or otherwise operated on concurrently.
++ */
++void *get_xsave_buffer_unsafe(struct fpu *fpu, int xfeature_nr)
++{
++	return get_xsave_addr(&fpu->fpstate->regs.xsave, xfeature_nr);
++}
++
+ /*
+  * Return a pointer to the xstate for the feature if it should be used, or NULL
+  * if the MSRs should be written to directly. To do this safely using the
+@@ -1930,14 +1941,11 @@ int xsave_rdmsrl(void *xstate, unsigned int msr, unsigned long long *p)
+ 	return 0;
+ }
+ 
+-int xsave_wrmsrl(void *xstate, u32 msr, u64 val)
++
++int xsave_wrmsrl_unsafe(void *xstate, u32 msr, u64 val)
+ {
+ 	u64 *member_ptr;
+ 
+-	__xsave_msrl_prepare_write();
+-	if (!xstate)
+-		return wrmsrl_safe(msr, val);
+-
+ 	member_ptr = __get_xsave_member(xstate, msr);
+ 	if (!member_ptr)
+ 		return 1;
+@@ -1947,6 +1955,15 @@ int xsave_wrmsrl(void *xstate, u32 msr, u64 val)
+ 	return 0;
+ }
+ 
++int xsave_wrmsrl(void *xstate, u32 msr, u64 val)
++{
++	__xsave_msrl_prepare_write();
++	if (!xstate)
++		return wrmsrl_safe(msr, val);
++
++	return xsave_wrmsrl_unsafe(xstate, msr, val);
++}
++
+ int xsave_set_clear_bits_msrl(void *xstate, u32 msr, u64 set, u64 clear)
+ {
+ 	u64 val, new_val;
+-- 
+2.34.1
+
diff --git a/0027-x86-cet-shstk-Handle-thread-shadow-stack.patch b/0027-x86-cet-shstk-Handle-thread-shadow-stack.patch
new file mode 100644
index 000000000..ee4b21fdd
--- /dev/null
+++ b/0027-x86-cet-shstk-Handle-thread-shadow-stack.patch
@@ -0,0 +1,223 @@
+From fac3f4b1a172d684f010c40b432e7d5c2333881b Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:21 -0700
+Subject: [PATCH 27/39] x86/cet/shstk: Handle thread shadow stack
+
+When a process is duplicated, but the child shares the address space with
+the parent, there is potential for the threads sharing a single stack to
+cause conflicts for each other. In the normal non-cet case this is handled
+in two ways.
+
+With regular CLONE_VM a new stack is provided by userspace such that the
+parent and child have different stacks.
+
+For vfork, the parent is suspended until the child exits. So as long as
+the child doesn't return from the vfork()/CLONE_VFORK calling function and
+sticks to a limited set of operations, the parent and child can share the
+same stack.
+
+For shadow stack, these scenarios present similar sharing problems. For the
+CLONE_VM case, the child and the parent must have separate shadow stacks.
+Instead of changing clone to take a shadow stack, have the kernel just
+allocate one and switch to it.
+
+Use stack_size passed from clone3() syscall for thread shadow stack size. A
+compat-mode thread shadow stack size is further reduced to 1/4.  This
+allows more threads to run in a 32-bit address space. The clone() does not
+pass stack_size, which was added to clone3(). In that case, use
+RLIMIT_STACK size and cap to 4 GB.
+
+For vfork(), the parent and child can share the same shadow stack, like they
+can share a normal stack. Since the parent is suspended until the child
+terminates, the child will not interfere with the parent while executing as
+long as it doesn't return from the vfork() and overwrite up the shadow stack.
+The child can safely overwrite down the shadow stack, as the parent can
+just overwrite this later. So CET does not add any additional limitations
+for vfork().
+
+Userspace implementing posix vfork() can actually prevent the child from
+returning from vfork(), using CET. Glibc does this by adjusting the shadow
+stack pointer in the child, so that the child receives a #CP if it tries to
+return from vfork() calling function.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+
+===
+v1:
+- Expand commit log
+- Switch to xsave helpers
+
+Yu-cheng v30:
+- Update comments about clone()/clone3(). (Borislav Petkov)
+
+Yu-cheng v29:
+- WARN_ON_ONCE() when get_xsave_addr() returns NULL, and update comments.
+  (Dave Hansen)
+
+Yu-cheng v28:
+- Split out copy_thread() argument name changes to a new patch.
+- Add compatibility for earlier clone(), which does not pass stack_size.
+- Add comment for get_xsave_addr(), explain the handling of null return
+  value.
+---
+ arch/x86/include/asm/cet.h         |  7 ++-
+ arch/x86/include/asm/mmu_context.h |  2 +
+ arch/x86/kernel/process.c          |  6 +++
+ arch/x86/kernel/shstk.c            | 68 +++++++++++++++++++++++++++++-
+ 4 files changed, 81 insertions(+), 2 deletions(-)
+
+diff --git a/arch/x86/include/asm/cet.h b/arch/x86/include/asm/cet.h
+index bf1f1bf4d734..2f1ddaccf78e 100644
+--- a/arch/x86/include/asm/cet.h
++++ b/arch/x86/include/asm/cet.h
+@@ -14,10 +14,15 @@ struct thread_shstk {
+ 
+ #ifdef CONFIG_X86_SHADOW_STACK
+ void shstk_setup(void);
++int shstk_alloc_thread_stack(struct task_struct *p, unsigned long clone_flags,
++			     unsigned long stack_size);
+ void shstk_free(struct task_struct *p);
+ void shstk_disable(void);
+ #else
+-static inline void shstk_setup(void) { return 0; }
++static inline void shstk_setup(void) { }
++static inline int shstk_alloc_thread_stack(struct task_struct *p,
++					   unsigned long clone_flags,
++					   unsigned long stack_size) { return 0; }
+ static inline void shstk_free(struct task_struct *p) {}
+ static inline void shstk_disable(void) {}
+ #endif
+diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h
+index 27516046117a..8e721d2c45d5 100644
+--- a/arch/x86/include/asm/mmu_context.h
++++ b/arch/x86/include/asm/mmu_context.h
+@@ -146,6 +146,8 @@ do {						\
+ #else
+ #define deactivate_mm(tsk, mm)			\
+ do {						\
++	if (!tsk->vfork_done)			\
++		shstk_free(tsk);		\
+ 	load_gs_index(0);			\
+ 	loadsegment(fs, 0);			\
+ } while (0)
+diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
+index 1d557bea15c1..c7bbdd8b296b 100644
+--- a/arch/x86/kernel/process.c
++++ b/arch/x86/kernel/process.c
+@@ -46,6 +46,7 @@
+ #include <asm/proto.h>
+ #include <asm/frame.h>
+ #include <asm/unwind.h>
++#include <asm/cet.h>
+ 
+ #include "process.h"
+ 
+@@ -117,6 +118,7 @@ void exit_thread(struct task_struct *tsk)
+ 
+ 	free_vm86(t);
+ 
++	shstk_free(tsk);
+ 	fpu__drop(fpu);
+ }
+ 
+@@ -217,6 +219,10 @@ int copy_thread(unsigned long clone_flags, unsigned long sp,
+ 	if (clone_flags & CLONE_SETTLS)
+ 		ret = set_new_tls(p, tls);
+ 
++	/* Allocate a new shadow stack for pthread */
++	if (!ret)
++		ret = shstk_alloc_thread_stack(p, clone_flags, stack_size);
++
+ 	if (!ret && unlikely(test_tsk_thread_flag(current, TIF_IO_BITMAP)))
+ 		io_bitmap_share(p);
+ 
+diff --git a/arch/x86/kernel/shstk.c b/arch/x86/kernel/shstk.c
+index f3bf3f4dfbad..90a5cb381663 100644
+--- a/arch/x86/kernel/shstk.c
++++ b/arch/x86/kernel/shstk.c
+@@ -100,6 +100,66 @@ void shstk_setup(void)
+ 	shstk->size = size;
+ }
+ 
++int shstk_alloc_thread_stack(struct task_struct *tsk, unsigned long clone_flags,
++			     unsigned long stack_size)
++{
++	struct thread_shstk *shstk = &tsk->thread.shstk;
++	unsigned long addr;
++	void *xstate;
++
++	/*
++	 * If shadow stack is not enabled on the new thread, skip any
++	 * switch to a new shadow stack.
++	 */
++	if (!shstk->size)
++		return 0;
++
++	/*
++	 * clone() does not pass stack_size, which was added to clone3().
++	 * Use RLIMIT_STACK and cap to 4 GB.
++	 */
++	if (!stack_size)
++		stack_size = min_t(unsigned long long, rlimit(RLIMIT_STACK), SZ_4G);
++
++	/*
++	 * For CLONE_VM, except vfork, the child needs a separate shadow
++	 * stack.
++	 */
++	if ((clone_flags & (CLONE_VFORK | CLONE_VM)) != CLONE_VM)
++		return 0;
++
++
++	/*
++	 * Compat-mode pthreads share a limited address space.
++	 * If each function call takes an average of four slots
++	 * stack space, allocate 1/4 of stack size for shadow stack.
++	 */
++	if (in_compat_syscall())
++		stack_size /= 4;
++
++	/*
++	 * 'tsk' is configured with a shadow stack and the fpu.state is
++	 * up to date since it was just copied from the parent.  There
++	 * must be a valid non-init CET state location in the buffer.
++	 */
++	xstate = get_xsave_buffer_unsafe(&tsk->thread.fpu, XFEATURE_CET_USER);
++	if (WARN_ON_ONCE(!xstate))
++		return -EINVAL;
++
++	stack_size = PAGE_ALIGN(stack_size);
++	addr = alloc_shstk(stack_size);
++	if (IS_ERR_VALUE(addr)) {
++		shstk->base = 0;
++		shstk->size = 0;
++		return PTR_ERR((void *)addr);
++	}
++
++	xsave_wrmsrl_unsafe(xstate, MSR_IA32_PL3_SSP, (u64)(addr + stack_size));
++	shstk->base = addr;
++	shstk->size = stack_size;
++	return 0;
++}
++
+ void shstk_free(struct task_struct *tsk)
+ {
+ 	struct thread_shstk *shstk = &tsk->thread.shstk;
+@@ -109,7 +169,13 @@ void shstk_free(struct task_struct *tsk)
+ 	    !shstk->base)
+ 		return;
+ 
+-	if (!tsk->mm)
++	/*
++	 * When fork() with CLONE_VM fails, the child (tsk) already has a
++	 * shadow stack allocated, and exit_thread() calls this function to
++	 * free it.  In this case the parent (current) and the child share
++	 * the same mm struct.
++	 */
++	if (!tsk->mm || tsk->mm != current->mm)
+ 		return;
+ 
+ 	unmap_shadow_stack(shstk->base, shstk->size);
+-- 
+2.34.1
+
diff --git a/0028-x86-cet-shstk-Introduce-shadow-stack-token-setup-ver.patch b/0028-x86-cet-shstk-Introduce-shadow-stack-token-setup-ver.patch
new file mode 100644
index 000000000..b1664c117
--- /dev/null
+++ b/0028-x86-cet-shstk-Introduce-shadow-stack-token-setup-ver.patch
@@ -0,0 +1,250 @@
+From eeb92feda85127222e2e52b1c253d0bc99d24f55 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:22 -0700
+Subject: [PATCH 28/39] x86/cet/shstk: Introduce shadow stack token
+ setup/verify routines
+
+A shadow stack restore token marks a restore point of the shadow stack, and
+the address in a token must point directly above the token, which is within
+the same shadow stack.  This is distinctively different from other pointers
+on the shadow stack, since those pointers point to executable code area.
+
+Introduce token setup and verify routines.  Also introduce WRUSS, which is
+a kernel-mode instruction but writes directly to user shadow stack.  It is
+used to construct user signal stack as described above.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Cc: Kees Cook <keescook@chromium.org>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+
+===
+v1:
+- Use xsave helpers
+
+Yu-cheng v30:
+- Update commit log, remove description about signals.
+- Update various comments.
+- Remove variable 'ssp' init and adjust return value accordingly.
+- Check get_user_shstk_addr() return value.
+- Replace 'ia32' with 'proc32'.
+
+Yu-cheng v29:
+- Update comments for the use of get_xsave_addr().
+
+Yu-cheng v28:
+- Add comments for get_xsave_addr().
+
+Yu-cheng v27:
+- For shstk_check_rstor_token(), instead of an input param, use current
+  shadow stack pointer.
+- In response to comments, fix/simplify a few syntax/format issues.
+
+Yu-cheng v25:
+- Update inline assembly syntax, use %[].
+- Change token address from (unsigned long) to (u64/u32 __user *).
+- Change -EPERM to -EFAULT.
+---
+ arch/x86/include/asm/cet.h           |   7 ++
+ arch/x86/include/asm/special_insns.h |  30 +++++++
+ arch/x86/kernel/shstk.c              | 122 +++++++++++++++++++++++++++
+ 3 files changed, 159 insertions(+)
+
+diff --git a/arch/x86/include/asm/cet.h b/arch/x86/include/asm/cet.h
+index 2f1ddaccf78e..60c7ee01389e 100644
+--- a/arch/x86/include/asm/cet.h
++++ b/arch/x86/include/asm/cet.h
+@@ -18,6 +18,9 @@ int shstk_alloc_thread_stack(struct task_struct *p, unsigned long clone_flags,
+ 			     unsigned long stack_size);
+ void shstk_free(struct task_struct *p);
+ void shstk_disable(void);
++int shstk_setup_rstor_token(bool proc32, unsigned long restorer,
++			    unsigned long *new_ssp);
++int shstk_check_rstor_token(bool proc32, unsigned long *new_ssp);
+ #else
+ static inline void shstk_setup(void) { }
+ static inline int shstk_alloc_thread_stack(struct task_struct *p,
+@@ -25,6 +28,10 @@ static inline int shstk_alloc_thread_stack(struct task_struct *p,
+ 					   unsigned long stack_size) { return 0; }
+ static inline void shstk_free(struct task_struct *p) {}
+ static inline void shstk_disable(void) {}
++static inline int shstk_setup_rstor_token(bool proc32, unsigned long restorer,
++					  unsigned long *new_ssp) { return 0; }
++static inline int shstk_check_rstor_token(bool proc32,
++					  unsigned long *new_ssp) { return 0; }
+ #endif
+ 
+ #endif /* __ASSEMBLY__ */
+diff --git a/arch/x86/include/asm/special_insns.h b/arch/x86/include/asm/special_insns.h
+index 68c257a3de0d..f45f378ca1fc 100644
+--- a/arch/x86/include/asm/special_insns.h
++++ b/arch/x86/include/asm/special_insns.h
+@@ -222,6 +222,36 @@ static inline void clwb(volatile void *__p)
+ 		: [pax] "a" (p));
+ }
+ 
++#ifdef CONFIG_X86_SHADOW_STACK
++static inline int write_user_shstk_32(u32 __user *addr, u32 val)
++{
++	if (WARN_ONCE(!IS_ENABLED(CONFIG_IA32_EMULATION) &&
++		      !IS_ENABLED(CONFIG_X86_X32),
++		      "%s used but not supported.\n", __func__)) {
++		return -EFAULT;
++	}
++
++	asm_volatile_goto("1: wrussd %[val], (%[addr])\n"
++			  _ASM_EXTABLE(1b, %l[fail])
++			  :: [addr] "r" (addr), [val] "r" (val)
++			  :: fail);
++	return 0;
++fail:
++	return -EFAULT;
++}
++
++static inline int write_user_shstk_64(u64 __user *addr, u64 val)
++{
++	asm_volatile_goto("1: wrussq %[val], (%[addr])\n"
++			  _ASM_EXTABLE(1b, %l[fail])
++			  :: [addr] "r" (addr), [val] "r" (val)
++			  :: fail);
++	return 0;
++fail:
++	return -EFAULT;
++}
++#endif /* CONFIG_X86_SHADOW_STACK */
++
+ #define nop() asm volatile ("nop")
+ 
+ static inline void serialize(void)
+diff --git a/arch/x86/kernel/shstk.c b/arch/x86/kernel/shstk.c
+index 90a5cb381663..f7679cba82fb 100644
+--- a/arch/x86/kernel/shstk.c
++++ b/arch/x86/kernel/shstk.c
+@@ -201,3 +201,125 @@ void shstk_disable(void)
+ 
+ 	shstk_free(current);
+ }
++
++static unsigned long get_user_shstk_addr(void)
++{
++	void *xstate;
++	unsigned long long ssp;
++
++	xstate = start_update_xsave_msrs(XFEATURE_CET_USER);
++
++	xsave_rdmsrl(xstate, MSR_IA32_PL3_SSP, &ssp);
++
++	end_update_xsave_msrs();
++
++	return ssp;
++}
++
++/*
++ * Create a restore token on the shadow stack.  A token is always 8-byte
++ * and aligned to 8.
++ */
++static int create_rstor_token(bool proc32, unsigned long ssp,
++			       unsigned long *token_addr)
++{
++	unsigned long addr;
++
++	/* Aligned to 8 is aligned to 4, so test 8 first */
++	if ((!proc32 && !IS_ALIGNED(ssp, 8)) || !IS_ALIGNED(ssp, 4))
++		return -EINVAL;
++
++	addr = ALIGN_DOWN(ssp, 8) - 8;
++
++	/* Is the token for 64-bit? */
++	if (!proc32)
++		ssp |= BIT(0);
++
++	if (write_user_shstk_64((u64 __user *)addr, (u64)ssp))
++		return -EFAULT;
++
++	*token_addr = addr;
++
++	return 0;
++}
++
++/*
++ * Create a restore token on shadow stack, and then push the user-mode
++ * function return address.
++ */
++int shstk_setup_rstor_token(bool proc32, unsigned long ret_addr,
++			    unsigned long *new_ssp)
++{
++	struct thread_shstk *shstk = &current->thread.shstk;
++	unsigned long ssp, token_addr;
++	int err;
++
++	if (!shstk->size)
++		return 0;
++
++	if (!ret_addr)
++		return -EINVAL;
++
++	ssp = get_user_shstk_addr();
++	if (!ssp)
++		return -EINVAL;
++
++	err = create_rstor_token(proc32, ssp, &token_addr);
++	if (err)
++		return err;
++
++	if (proc32) {
++		ssp = token_addr - sizeof(u32);
++		err = write_user_shstk_32((u32 __user *)ssp, (u32)ret_addr);
++	} else {
++		ssp = token_addr - sizeof(u64);
++		err = write_user_shstk_64((u64 __user *)ssp, (u64)ret_addr);
++	}
++
++	if (!err)
++		*new_ssp = ssp;
++
++	return err;
++}
++
++/*
++ * Verify the user shadow stack has a valid token on it, and then set
++ * *new_ssp according to the token.
++ */
++int shstk_check_rstor_token(bool proc32, unsigned long *new_ssp)
++{
++	unsigned long token_addr;
++	unsigned long token;
++	bool shstk32;
++
++	token_addr = get_user_shstk_addr();
++	if (!token_addr)
++		return -EINVAL;
++
++	if (get_user(token, (unsigned long __user *)token_addr))
++		return -EFAULT;
++
++	/* Is mode flag correct? */
++	shstk32 = !(token & BIT(0));
++	if (proc32 ^ shstk32)
++		return -EINVAL;
++
++	/* Is busy flag set? */
++	if (token & BIT(1))
++		return -EINVAL;
++
++	/* Mask out flags */
++	token &= ~3UL;
++
++	/* Restore address aligned? */
++	if ((!proc32 && !IS_ALIGNED(token, 8)) || !IS_ALIGNED(token, 4))
++		return -EINVAL;
++
++	/* Token placed properly? */
++	if (((ALIGN_DOWN(token, 8) - 8) != token_addr) || token >= TASK_SIZE_MAX)
++		return -EINVAL;
++
++	*new_ssp = token;
++
++	return 0;
++}
+-- 
+2.34.1
+
diff --git a/0029-x86-cet-shstk-Handle-signals-for-shadow-stack.patch b/0029-x86-cet-shstk-Handle-signals-for-shadow-stack.patch
new file mode 100644
index 000000000..dc2eef0fd
--- /dev/null
+++ b/0029-x86-cet-shstk-Handle-signals-for-shadow-stack.patch
@@ -0,0 +1,259 @@
+From b0288e69b35832eee125c84fc19dc2fcce84cb78 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:23 -0700
+Subject: [PATCH 29/39] x86/cet/shstk: Handle signals for shadow stack
+
+When a signal happens the context is pushed to the stack before handling
+it. Since the shadow stack only track's return addresses, there isn't any
+state that needs to be pushed. However, there are still a few things that
+need to be done which will be kernel ABI for shadow stacks.
+
+One is to make sure the restoter address is written to shadow stack, since
+the signal handler (if not changing ucontext) returns to the restorer, and
+the restorer calls sigreturn. So add the restorer on the shadow stack
+before handling the signal, so there is not a conflict when the signal
+handler returns to the restorer.
+
+The other thing to do is to place a restore token on the thread's shadow
+stack before handling the signal and check it during sigreturn. This
+is an extra layer of protection to hamper attackers calling sigreturn
+manually as in SROP-like attacks.
+
+So, when handling a signal push
+ - a shadow stack restore token pointing to the current shadow
+stack address
+ - the restorer address below the restore token.
+
+In sigreturn, verify the restore token and pop the shadow stack.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Cc: Andy Lutomirski <luto@kernel.org>
+Cc: Cyrill Gorcunov <gorcunov@gmail.com>
+Cc: Florian Weimer <fweimer@redhat.com>
+Cc: H. Peter Anvin <hpa@zytor.com>
+Cc: Kees Cook <keescook@chromium.org>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+
+===
+
+v1:
+- Use xsave helpers
+- Expand commit log
+
+Yu-cheng v27:
+- Eliminate saving shadow stack pointer to signal context.
+
+Yu-cheng v25:
+- Update commit log/comments for the sc_ext struct.
+- Use restorer address already calculated.
+- Change CONFIG_X86_CET to CONFIG_X86_SHADOW_STACK.
+- Change X86_FEATURE_CET to X86_FEATURE_SHSTK.
+- Eliminate writing to MSR_IA32_U_CET for shadow stack.
+- Change wrmsrl() to wrmsrl_safe() and handle error.
+---
+ arch/x86/ia32/ia32_signal.c | 25 ++++++++++++++++-----
+ arch/x86/include/asm/cet.h  |  4 ++++
+ arch/x86/kernel/shstk.c     | 44 +++++++++++++++++++++++++++++++++++++
+ arch/x86/kernel/signal.c    | 13 +++++++++++
+ 4 files changed, 81 insertions(+), 5 deletions(-)
+
+diff --git a/arch/x86/ia32/ia32_signal.c b/arch/x86/ia32/ia32_signal.c
+index c9c3859322fa..a8d038409d60 100644
+--- a/arch/x86/ia32/ia32_signal.c
++++ b/arch/x86/ia32/ia32_signal.c
+@@ -34,6 +34,7 @@
+ #include <asm/sigframe.h>
+ #include <asm/sighandling.h>
+ #include <asm/smap.h>
++#include <asm/cet.h>
+ 
+ static inline void reload_segments(struct sigcontext_32 *sc)
+ {
+@@ -112,6 +113,10 @@ COMPAT_SYSCALL_DEFINE0(sigreturn)
+ 
+ 	if (!ia32_restore_sigcontext(regs, &frame->sc))
+ 		goto badframe;
++
++	if (restore_signal_shadow_stack())
++		goto badframe;
++
+ 	return regs->ax;
+ 
+ badframe:
+@@ -137,6 +142,9 @@ COMPAT_SYSCALL_DEFINE0(rt_sigreturn)
+ 	if (!ia32_restore_sigcontext(regs, &frame->uc.uc_mcontext))
+ 		goto badframe;
+ 
++	if (restore_signal_shadow_stack())
++		goto badframe;
++
+ 	if (compat_restore_altstack(&frame->uc.uc_stack))
+ 		goto badframe;
+ 
+@@ -261,6 +269,9 @@ int ia32_setup_frame(int sig, struct ksignal *ksig,
+ 			restorer = &frame->retcode;
+ 	}
+ 
++	if (setup_signal_shadow_stack(1, restorer))
++		return -EFAULT;
++
+ 	if (!user_access_begin(frame, sizeof(*frame)))
+ 		return -EFAULT;
+ 
+@@ -318,6 +329,15 @@ int ia32_setup_rt_frame(int sig, struct ksignal *ksig,
+ 
+ 	frame = get_sigframe(ksig, regs, sizeof(*frame), &fp);
+ 
++	if (ksig->ka.sa.sa_flags & SA_RESTORER)
++		restorer = ksig->ka.sa.sa_restorer;
++	else
++		restorer = current->mm->context.vdso +
++			vdso_image_32.sym___kernel_rt_sigreturn;
++
++	if (setup_signal_shadow_stack(1, restorer))
++		return -EFAULT;
++
+ 	if (!user_access_begin(frame, sizeof(*frame)))
+ 		return -EFAULT;
+ 
+@@ -333,11 +353,6 @@ int ia32_setup_rt_frame(int sig, struct ksignal *ksig,
+ 	unsafe_put_user(0, &frame->uc.uc_link, Efault);
+ 	unsafe_compat_save_altstack(&frame->uc.uc_stack, regs->sp, Efault);
+ 
+-	if (ksig->ka.sa.sa_flags & SA_RESTORER)
+-		restorer = ksig->ka.sa.sa_restorer;
+-	else
+-		restorer = current->mm->context.vdso +
+-			vdso_image_32.sym___kernel_rt_sigreturn;
+ 	unsafe_put_user(ptr_to_compat(restorer), &frame->pretcode, Efault);
+ 
+ 	/*
+diff --git a/arch/x86/include/asm/cet.h b/arch/x86/include/asm/cet.h
+index 60c7ee01389e..bc5becc22538 100644
+--- a/arch/x86/include/asm/cet.h
++++ b/arch/x86/include/asm/cet.h
+@@ -21,6 +21,8 @@ void shstk_disable(void);
+ int shstk_setup_rstor_token(bool proc32, unsigned long restorer,
+ 			    unsigned long *new_ssp);
+ int shstk_check_rstor_token(bool proc32, unsigned long *new_ssp);
++int setup_signal_shadow_stack(int proc32, void __user *restorer);
++int restore_signal_shadow_stack(void);
+ #else
+ static inline void shstk_setup(void) { }
+ static inline int shstk_alloc_thread_stack(struct task_struct *p,
+@@ -32,6 +34,8 @@ static inline int shstk_setup_rstor_token(bool proc32, unsigned long restorer,
+ 					  unsigned long *new_ssp) { return 0; }
+ static inline int shstk_check_rstor_token(bool proc32,
+ 					  unsigned long *new_ssp) { return 0; }
++static inline int setup_signal_shadow_stack(int proc32, void __user *restorer) { return 0; }
++static inline int restore_signal_shadow_stack(void) { return 0; }
+ #endif
+ 
+ #endif /* __ASSEMBLY__ */
+diff --git a/arch/x86/kernel/shstk.c b/arch/x86/kernel/shstk.c
+index f7679cba82fb..d21e9d279ffe 100644
+--- a/arch/x86/kernel/shstk.c
++++ b/arch/x86/kernel/shstk.c
+@@ -323,3 +323,47 @@ int shstk_check_rstor_token(bool proc32, unsigned long *new_ssp)
+ 
+ 	return 0;
+ }
++
++int setup_signal_shadow_stack(int proc32, void __user *restorer)
++{
++	struct thread_shstk *shstk = &current->thread.shstk;
++	unsigned long new_ssp;
++	void *xstate;
++	int err;
++
++	if (!cpu_feature_enabled(X86_FEATURE_SHSTK) || !shstk->size)
++		return 0;
++
++	err = shstk_setup_rstor_token(proc32, (unsigned long)restorer,
++				      &new_ssp);
++	if (err)
++		return err;
++
++	xstate = start_update_xsave_msrs(XFEATURE_CET_USER);
++	err = xsave_wrmsrl(xstate, MSR_IA32_PL3_SSP, new_ssp);
++	end_update_xsave_msrs();
++
++	return err;
++}
++
++int restore_signal_shadow_stack(void)
++{
++	struct thread_shstk *shstk = &current->thread.shstk;
++	void *xstate;
++	int proc32 = in_ia32_syscall();
++	unsigned long new_ssp;
++	int err;
++
++	if (!cpu_feature_enabled(X86_FEATURE_SHSTK) || !shstk->size)
++		return 0;
++
++	err = shstk_check_rstor_token(proc32, &new_ssp);
++	if (err)
++		return err;
++
++	xstate = start_update_xsave_msrs(XFEATURE_CET_USER);
++	err = xsave_wrmsrl(xstate, MSR_IA32_PL3_SSP, new_ssp);
++	end_update_xsave_msrs();
++
++	return err;
++}
+diff --git a/arch/x86/kernel/signal.c b/arch/x86/kernel/signal.c
+index ec71e06ae364..e6202fc2a56c 100644
+--- a/arch/x86/kernel/signal.c
++++ b/arch/x86/kernel/signal.c
+@@ -48,6 +48,7 @@
+ #include <asm/syscall.h>
+ #include <asm/sigframe.h>
+ #include <asm/signal.h>
++#include <asm/cet.h>
+ 
+ #ifdef CONFIG_X86_64
+ /*
+@@ -471,6 +472,9 @@ static int __setup_rt_frame(int sig, struct ksignal *ksig,
+ 	frame = get_sigframe(&ksig->ka, regs, sizeof(struct rt_sigframe), &fp);
+ 	uc_flags = frame_uc_flags(regs);
+ 
++	if (setup_signal_shadow_stack(0, ksig->ka.sa.sa_restorer))
++		return -EFAULT;
++
+ 	if (!user_access_begin(frame, sizeof(*frame)))
+ 		return -EFAULT;
+ 
+@@ -576,6 +580,9 @@ static int x32_setup_rt_frame(struct ksignal *ksig,
+ 
+ 	uc_flags = frame_uc_flags(regs);
+ 
++	if (setup_signal_shadow_stack(0, ksig->ka.sa.sa_restorer))
++		return -EFAULT;
++
+ 	if (!user_access_begin(frame, sizeof(*frame)))
+ 		return -EFAULT;
+ 
+@@ -674,6 +681,9 @@ SYSCALL_DEFINE0(rt_sigreturn)
+ 	if (!restore_sigcontext(regs, &frame->uc.uc_mcontext, uc_flags))
+ 		goto badframe;
+ 
++	if (restore_signal_shadow_stack())
++		goto badframe;
++
+ 	if (restore_altstack(&frame->uc.uc_stack))
+ 		goto badframe;
+ 
+@@ -991,6 +1001,9 @@ COMPAT_SYSCALL_DEFINE0(x32_rt_sigreturn)
+ 	if (!restore_sigcontext(regs, &frame->uc.uc_mcontext, uc_flags))
+ 		goto badframe;
+ 
++	if (restore_signal_shadow_stack())
++		goto badframe;
++
+ 	if (compat_restore_altstack(&frame->uc.uc_stack))
+ 		goto badframe;
+ 
+-- 
+2.34.1
+
diff --git a/0030-x86-cet-shstk-Add-arch_prctl-elf-feature-functions.patch b/0030-x86-cet-shstk-Add-arch_prctl-elf-feature-functions.patch
new file mode 100644
index 000000000..ef5512a52
--- /dev/null
+++ b/0030-x86-cet-shstk-Add-arch_prctl-elf-feature-functions.patch
@@ -0,0 +1,218 @@
+From 0656c027a2a5dc91ddb6804470b6a175ab110ea4 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:25 -0700
+Subject: [PATCH 30/39] x86/cet/shstk: Add arch_prctl elf feature functions
+
+Support for some CPU features that adjust the behavior of existing
+instructions is specified in elf headers. The kernel will automatically
+enable these features if it detects the elf headers. However some
+applications may need to disable these CPU features during runtime or
+enable them late. To support per-thread runtime modification of this
+type of configuration of add basic infrastructure for an arch_prctl
+interface that supports disabling, locking and getting status on these
+features.
+
+Today the only user is CET, but keep the names generic because LAM could
+likley use it as well.
+
+arch_prctl(ARCH_X86_FEATURE_1_STATUS, u64 *args)
+    Get feature status.
+
+    The parameter 'args' is a pointer to a user buffer.  The kernel returns
+    the following information:
+
+    *args = shadow stack/IBT status
+    *(args + 1) = shadow stack base address
+    *(args + 2) = shadow stack size
+
+    32-bit binaries use the same interface, but only lower 32-bits of each
+    item.
+
+arch_prctl(ARCH_X86_FEATURE_1_DISABLE, unsigned int features)
+    Disable features specified in 'features'. Return -EPERM if any of the
+    passed feature are locked.
+
+arch_ptrtl(ARCH_X86_FEATURE_1_ENABLE, unsigned int features)
+    Enable feature specified in 'features'. Return -EPERM if any of the
+    passed feature are locked.
+
+arch_prctl(ARCH_X86_FEATURE_1_LOCK, unsigned int features)
+    Lock in all features at their current enabled or disabled status.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+
+===
+
+v1:
+- Changed from ENOSYS and ENOTSUPP error codes per checkpatch
+- Changed interface/filename to be more generic so it can be shared with LAM.
+---
+ arch/x86/include/asm/cet.h          |  6 +++
+ arch/x86/include/asm/processor.h    |  1 +
+ arch/x86/include/uapi/asm/prctl.h   |  5 +++
+ arch/x86/kernel/Makefile            |  2 +-
+ arch/x86/kernel/elf_feature_prctl.c | 67 +++++++++++++++++++++++++++++
+ arch/x86/kernel/process.c           |  2 +-
+ include/uapi/linux/elf.h            |  6 +++
+ 7 files changed, 87 insertions(+), 2 deletions(-)
+ create mode 100644 arch/x86/kernel/elf_feature_prctl.c
+
+diff --git a/arch/x86/include/asm/cet.h b/arch/x86/include/asm/cet.h
+index bc5becc22538..caedbcf4f619 100644
+--- a/arch/x86/include/asm/cet.h
++++ b/arch/x86/include/asm/cet.h
+@@ -38,6 +38,12 @@ static inline int setup_signal_shadow_stack(int proc32, void __user *restorer) {
+ static inline int restore_signal_shadow_stack(void) { return 0; }
+ #endif
+ 
++#ifdef CONFIG_X86_SHADOW_STACK
++int prctl_elf_feature(int option, u64 arg2);
++#else
++static inline int prctl_elf_feature(int option, u64 arg2) { return -EINVAL; }
++#endif
++
+ #endif /* __ASSEMBLY__ */
+ 
+ #endif /* _ASM_X86_CET_H */
+diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
+index ae41076f3a29..49759eb49372 100644
+--- a/arch/x86/include/asm/processor.h
++++ b/arch/x86/include/asm/processor.h
+@@ -531,6 +531,7 @@ struct thread_struct {
+ 
+ #ifdef CONFIG_X86_SHADOW_STACK
+ 	struct thread_shstk	shstk;
++	u64			feat_prctl_locked;
+ #endif
+ 
+ 	/* Floating point and extended processor state */
+diff --git a/arch/x86/include/uapi/asm/prctl.h b/arch/x86/include/uapi/asm/prctl.h
+index 754a07856817..d70750b2db16 100644
+--- a/arch/x86/include/uapi/asm/prctl.h
++++ b/arch/x86/include/uapi/asm/prctl.h
+@@ -18,4 +18,9 @@
+ #define ARCH_MAP_VDSO_32	0x2002
+ #define ARCH_MAP_VDSO_64	0x2003
+ 
++#define ARCH_X86_FEATURE_1_STATUS	0x3001
++#define ARCH_X86_FEATURE_1_DISABLE	0x3002
++#define ARCH_X86_FEATURE_1_LOCK		0x3003
++#define ARCH_X86_FEATURE_1_ENABLE	0x3004
++
+ #endif /* _ASM_X86_PRCTL_H */
+diff --git a/arch/x86/kernel/Makefile b/arch/x86/kernel/Makefile
+index b97610b46e61..79b2f4d05959 100644
+--- a/arch/x86/kernel/Makefile
++++ b/arch/x86/kernel/Makefile
+@@ -153,7 +153,7 @@ obj-$(CONFIG_AMD_MEM_ENCRYPT)		+= sev.o
+ 
+ obj-$(CONFIG_ARCH_HAS_CC_PLATFORM)	+= cc_platform.o
+ 
+-obj-$(CONFIG_X86_SHADOW_STACK)		+= shstk.o
++obj-$(CONFIG_X86_SHADOW_STACK)		+= shstk.o elf_feature_prctl.o
+ ###
+ # 64 bit specific files
+ ifeq ($(CONFIG_X86_64),y)
+diff --git a/arch/x86/kernel/elf_feature_prctl.c b/arch/x86/kernel/elf_feature_prctl.c
+new file mode 100644
+index 000000000000..97074aaff7ba
+--- /dev/null
++++ b/arch/x86/kernel/elf_feature_prctl.c
+@@ -0,0 +1,67 @@
++// SPDX-License-Identifier: GPL-2.0
++
++#include <linux/errno.h>
++#include <linux/uaccess.h>
++#include <linux/prctl.h>
++#include <linux/compat.h>
++#include <linux/mman.h>
++#include <linux/elfcore.h>
++#include <linux/processor.h>
++#include <asm/prctl.h>
++#include <asm/cet.h>
++
++/* See Documentation/x86/intel_cet.rst. */
++
++static int elf_feat_copy_status_to_user(struct thread_shstk *shstk, u64 __user *ubuf)
++{
++	u64 buf[3] = {};
++
++	if (shstk->size) {
++		buf[0] = GNU_PROPERTY_X86_FEATURE_1_SHSTK;
++		buf[1] = shstk->base;
++		buf[2] = shstk->size;
++	}
++
++	return copy_to_user(ubuf, buf, sizeof(buf));
++}
++
++int prctl_elf_feature(int option, u64 arg2)
++{
++	struct thread_struct *thread = &current->thread;
++
++	if (!cpu_feature_enabled(X86_FEATURE_SHSTK))
++		return -EOPNOTSUPP;
++
++	switch (option) {
++	case ARCH_X86_FEATURE_1_STATUS:
++		return elf_feat_copy_status_to_user(&thread->shstk, (u64 __user *)arg2);
++	case ARCH_X86_FEATURE_1_DISABLE:
++		if (arg2 & thread->feat_prctl_locked)
++			return -EPERM;
++		if (arg2 & ~GNU_PROPERTY_X86_FEATURE_1_VALID)
++			return -EINVAL;
++
++		if (arg2 & GNU_PROPERTY_X86_FEATURE_1_SHSTK)
++			shstk_disable();
++		if (arg2 & GNU_PROPERTY_X86_FEATURE_1_WRSS)
++			wrss_control(false);
++		return 0;
++	case ARCH_X86_FEATURE_1_ENABLE:
++		if (arg2 & ~GNU_PROPERTY_X86_FEATURE_1_VALID)
++			return -EINVAL;
++		if (arg2 & thread->feat_prctl_locked)
++			return -EPERM;
++
++		if (arg2 & GNU_PROPERTY_X86_FEATURE_1_WRSS)
++			wrss_control(true);
++		if (arg2 & GNU_PROPERTY_X86_FEATURE_1_SHSTK)
++			shstk_setup();
++		return 0;
++	case ARCH_X86_FEATURE_1_LOCK:
++		thread->feat_prctl_locked |= arg2;
++		return 0;
++
++	default:
++		return -EINVAL;
++	}
++}
+diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
+index c7bbdd8b296b..37c516d0412b 100644
+--- a/arch/x86/kernel/process.c
++++ b/arch/x86/kernel/process.c
+@@ -1003,5 +1003,5 @@ long do_arch_prctl_common(struct task_struct *task, int option,
+ 		return fpu_xstate_prctl(task, option, arg2);
+ 	}
+ 
+-	return -EINVAL;
++	return prctl_elf_feature(option, arg2);
+ }
+diff --git a/include/uapi/linux/elf.h b/include/uapi/linux/elf.h
+index 61bf4774b8f2..d3f1bc99b33a 100644
+--- a/include/uapi/linux/elf.h
++++ b/include/uapi/linux/elf.h
+@@ -456,4 +456,10 @@ typedef struct elf64_note {
+ /* Bits for GNU_PROPERTY_AARCH64_FEATURE_1_BTI */
+ #define GNU_PROPERTY_AARCH64_FEATURE_1_BTI	(1U << 0)
+ 
++/* x86 GNU property bits */
++#define GNU_PROPERTY_X86_FEATURE_1_IBT		0x00000001
++#define GNU_PROPERTY_X86_FEATURE_1_SHSTK	0x00000002
++#define GNU_PROPERTY_X86_FEATURE_1_VALID (GNU_PROPERTY_X86_FEATURE_1_IBT | \
++					  GNU_PROPERTY_X86_FEATURE_1_SHSTK)
++
+ #endif /* _UAPI_LINUX_ELF_H */
+-- 
+2.34.1
+
diff --git a/0031-mm-Update-arch_validate_flags-to-test-vma-anonymous.patch b/0031-mm-Update-arch_validate_flags-to-test-vma-anonymous.patch
new file mode 100644
index 000000000..b8737d7d5
--- /dev/null
+++ b/0031-mm-Update-arch_validate_flags-to-test-vma-anonymous.patch
@@ -0,0 +1,117 @@
+From d510ae13f79d6fa5f5c31d0917a44e5e15daeb72 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 30 Aug 2021 11:15:27 -0700
+Subject: [PATCH 31/39] mm: Update arch_validate_flags() to test vma anonymous
+
+When newer VM flags are being created, such as VM_MTE, it becomes necessary
+for mmap/mprotect to verify if certain flags are being applied to an
+anonymous VMA.
+
+To solve this, one approach is adding a VM flag to track that MAP_ANONYMOUS
+is specified [1], and then using the flag in arch_validate_flags().
+
+Another approach is passing the VMA to arch_validate_flags(), and check
+vma_is_anonymous().
+
+To prepare the introduction of PROT_SHADOW_STACK, which creates a shadow
+stack mapping and can be applied only to an anonymous VMA, update
+arch_validate_flags() to pass in the VMA.
+
+[1] commit 9f3419315f3c ("arm64: mte: Add PROT_MTE support to mmap() and mprotect()"),
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
+Acked-by: Catalin Marinas <catalin.marinas@arm.com>
+Cc: Kees Cook <keescook@chromium.org>
+Cc: Vincenzo Frascino <vincenzo.frascino@arm.com>
+Cc: Will Deacon <will@kernel.org>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+---
+ arch/arm64/include/asm/mman.h | 4 ++--
+ arch/sparc/include/asm/mman.h | 4 ++--
+ include/linux/mman.h          | 2 +-
+ mm/mmap.c                     | 2 +-
+ mm/mprotect.c                 | 2 +-
+ 5 files changed, 7 insertions(+), 7 deletions(-)
+
+diff --git a/arch/arm64/include/asm/mman.h b/arch/arm64/include/asm/mman.h
+index e3e28f7daf62..7c45e7578f78 100644
+--- a/arch/arm64/include/asm/mman.h
++++ b/arch/arm64/include/asm/mman.h
+@@ -74,7 +74,7 @@ static inline bool arch_validate_prot(unsigned long prot,
+ }
+ #define arch_validate_prot(prot, addr) arch_validate_prot(prot, addr)
+ 
+-static inline bool arch_validate_flags(unsigned long vm_flags)
++static inline bool arch_validate_flags(struct vm_area_struct *vma, unsigned long vm_flags)
+ {
+ 	if (!system_supports_mte())
+ 		return true;
+@@ -82,6 +82,6 @@ static inline bool arch_validate_flags(unsigned long vm_flags)
+ 	/* only allow VM_MTE if VM_MTE_ALLOWED has been set previously */
+ 	return !(vm_flags & VM_MTE) || (vm_flags & VM_MTE_ALLOWED);
+ }
+-#define arch_validate_flags(vm_flags) arch_validate_flags(vm_flags)
++#define arch_validate_flags(vma, vm_flags) arch_validate_flags(vma, vm_flags)
+ 
+ #endif /* ! __ASM_MMAN_H__ */
+diff --git a/arch/sparc/include/asm/mman.h b/arch/sparc/include/asm/mman.h
+index 274217e7ed70..0ec4975f167d 100644
+--- a/arch/sparc/include/asm/mman.h
++++ b/arch/sparc/include/asm/mman.h
+@@ -60,11 +60,11 @@ static inline int sparc_validate_prot(unsigned long prot, unsigned long addr)
+ 	return 1;
+ }
+ 
+-#define arch_validate_flags(vm_flags) arch_validate_flags(vm_flags)
++#define arch_validate_flags(vma, vm_flags) arch_validate_flags(vma, vm_flags)
+ /* arch_validate_flags() - Ensure combination of flags is valid for a
+  *	VMA.
+  */
+-static inline bool arch_validate_flags(unsigned long vm_flags)
++static inline bool arch_validate_flags(struct vm_area_struct *vma, unsigned long vm_flags)
+ {
+ 	/* If ADI is being enabled on this VMA, check for ADI
+ 	 * capability on the platform and ensure VMA is suitable
+diff --git a/include/linux/mman.h b/include/linux/mman.h
+index b66e91b8176c..ac1f822268dd 100644
+--- a/include/linux/mman.h
++++ b/include/linux/mman.h
+@@ -117,7 +117,7 @@ static inline bool arch_validate_prot(unsigned long prot, unsigned long addr)
+  *
+  * Returns true if the VM_* flags are valid.
+  */
+-static inline bool arch_validate_flags(unsigned long flags)
++static inline bool arch_validate_flags(struct vm_area_struct *vma, unsigned long flags)
+ {
+ 	return true;
+ }
+diff --git a/mm/mmap.c b/mm/mmap.c
+index 861dc8046bf1..8202d6f91904 100644
+--- a/mm/mmap.c
++++ b/mm/mmap.c
+@@ -1831,7 +1831,7 @@ unsigned long mmap_region(struct file *file, unsigned long addr,
+ 	}
+ 
+ 	/* Allow architectures to sanity-check the vm_flags */
+-	if (!arch_validate_flags(vma->vm_flags)) {
++	if (!arch_validate_flags(vma, vma->vm_flags)) {
+ 		error = -EINVAL;
+ 		if (file)
+ 			goto unmap_and_free_vma;
+diff --git a/mm/mprotect.c b/mm/mprotect.c
+index b589b0625ca4..efad9e264d01 100644
+--- a/mm/mprotect.c
++++ b/mm/mprotect.c
+@@ -624,7 +624,7 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
+ 		}
+ 
+ 		/* Allow architectures to sanity-check the new flags */
+-		if (!arch_validate_flags(newflags)) {
++		if (!arch_validate_flags(vma, newflags)) {
+ 			error = -EINVAL;
+ 			goto out;
+ 		}
+-- 
+2.34.1
+
diff --git a/0032-x86-cet-shstk-Introduce-map_shadow_stack-syscall.patch b/0032-x86-cet-shstk-Introduce-map_shadow_stack-syscall.patch
new file mode 100644
index 000000000..31f9bcd44
--- /dev/null
+++ b/0032-x86-cet-shstk-Introduce-map_shadow_stack-syscall.patch
@@ -0,0 +1,285 @@
+From 56b97817f55d803389aa4a7a941e43abd2e5f57d Mon Sep 17 00:00:00 2001
+From: Rick Edgecombe <rick.p.edgecombe@intel.com>
+Date: Thu, 7 Oct 2021 16:30:54 -0700
+Subject: [PATCH 32/39] x86/cet/shstk: Introduce map_shadow_stack syscall
+
+When operating with shadow stacks enabled, the kernel will automatically
+allocate shadow stacks for new threads, however in some cases userspace
+will need additional shadow stacks. The main example of this is the
+ucontext family of functions, which require userspace allocating and
+pivoting to userspace managed stacks.
+
+Unlike most other user memory permissions, shadow stacks need to be
+provisioned with special data (restore token) in order to be useful.
+They need to be setup with a restore token so that userspace can pivot
+to them via the RSTORSSP instruction. But, the security design of shadow
+stack's is that they should not be written to except in limited
+circumstances. This presents a problem for userspace, as how userspace
+can provision this special data, without allowing for the shadow stack
+to be generally writable.
+
+Previously, a new PROT_SHADOW_STACK was attempted, which could be
+mprotect()ed from RW permissions after the data was provisioned. This was
+found to not be secure enough, as other thread's could write to the
+shadow stack during the writable window.
+
+The kernel can use a special instruction, WRUSS, to write directly to
+userspace shadow stacks. So the solution can be that memory can be made
+as shadow stack permissions from the beginning (never generally writable
+in userspace), and the kernel itself can write the restore token.
+
+First, a new madvise() flag was explored, which could operate on the
+PROT_SHADOW_STACK memory. This had a couple downsides:
+1. Extra checks were needed in mprotect() to prevent writable memory from
+   ever becoming PROT_SHADOW_STACK.
+2. Extra checks/vma state were needed in the new madvise() to prevent
+   restore tokens being written into the middle of pre-used shadow stacks.
+   It is ideal to prevent restore tokens being added at arbitrary
+   locations, so the check was to make sure the shadow stack had never been
+   written to.
+3. It stood out from the rest of the madvise flags, as more of
+   direct action than a hint at future desired behavior.
+
+So rather than repurpose two existing syscalls that don't quite fit, just
+implement a new map_shadow_stack syscall to allow userspace to map and
+setup new shadow stacks in one step. While ucontext is the primary
+motivator, userspace may have other unforeseen reasons to manage it's
+own shadow stacks using the WRSS instruction. Towards this provide a
+flag so that stacks can be optionally setup securely for the common case
+of ucontext without enabling WRSS. Or potentially have the kernel set up
+the shadow stack in some new way.
+
+The following example demonstrates how to create a new shadow stack with
+map_shadow_stack:
+void *shadow_stack = map_shadow_stack(stack_size, SHADOW_STACK_SET_TOKEN);
+
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+===
+
+v1:
+- New patch (replaces PROT_SHADOW_STACK)
+---
+ arch/x86/entry/syscalls/syscall_32.tbl |  1 +
+ arch/x86/entry/syscalls/syscall_64.tbl |  1 +
+ arch/x86/include/uapi/asm/mman.h       |  2 +
+ arch/x86/kernel/shstk.c                | 91 +++++++++++++++++---------
+ include/linux/syscalls.h               |  1 +
+ include/uapi/asm-generic/unistd.h      |  2 +-
+ kernel/sys_ni.c                        |  1 +
+ 7 files changed, 68 insertions(+), 31 deletions(-)
+
+diff --git a/arch/x86/entry/syscalls/syscall_32.tbl b/arch/x86/entry/syscalls/syscall_32.tbl
+index 7e25543693de..e69c55f9a549 100644
+--- a/arch/x86/entry/syscalls/syscall_32.tbl
++++ b/arch/x86/entry/syscalls/syscall_32.tbl
+@@ -454,3 +454,4 @@
+ 447	i386	memfd_secret		sys_memfd_secret
+ 448	i386	process_mrelease	sys_process_mrelease
+ 449	i386	futex_waitv		sys_futex_waitv
++450	i386	map_shadow_stack	sys_map_shadow_stack
+diff --git a/arch/x86/entry/syscalls/syscall_64.tbl b/arch/x86/entry/syscalls/syscall_64.tbl
+index fe8f8dd157b4..e39f5232765f 100644
+--- a/arch/x86/entry/syscalls/syscall_64.tbl
++++ b/arch/x86/entry/syscalls/syscall_64.tbl
+@@ -371,6 +371,7 @@
+ 447	common	memfd_secret		sys_memfd_secret
+ 448	common	process_mrelease	sys_process_mrelease
+ 449	common	futex_waitv		sys_futex_waitv
++450	common	map_shadow_stack	sys_map_shadow_stack
+ 
+ #
+ # Due to a historical design error, certain syscalls are numbered differently
+diff --git a/arch/x86/include/uapi/asm/mman.h b/arch/x86/include/uapi/asm/mman.h
+index d4a8d0424bfb..42bc3741c6f1 100644
+--- a/arch/x86/include/uapi/asm/mman.h
++++ b/arch/x86/include/uapi/asm/mman.h
+@@ -26,6 +26,8 @@
+ 		((key) & 0x8 ? VM_PKEY_BIT3 : 0))
+ #endif
+ 
++#define SHADOW_STACK_SET_TOKEN	0x1	/* Set up a restore token in the newly allocatd shadow stack */
++
+ #include <asm-generic/mman.h>
+ 
+ #endif /* _ASM_X86_MMAN_H */
+diff --git a/arch/x86/kernel/shstk.c b/arch/x86/kernel/shstk.c
+index d21e9d279ffe..63e6dff88641 100644
+--- a/arch/x86/kernel/shstk.c
++++ b/arch/x86/kernel/shstk.c
+@@ -15,6 +15,7 @@
+ #include <linux/compat.h>
+ #include <linux/sizes.h>
+ #include <linux/user.h>
++#include <linux/syscalls.h>
+ #include <asm/msr.h>
+ #include <asm/fpu/internal.h>
+ #include <asm/fpu/xstate.h>
+@@ -23,7 +24,36 @@
+ #include <asm/special_insns.h>
+ #include <asm/fpu/api.h>
+ 
+-static unsigned long alloc_shstk(unsigned long size)
++/*
++ * Create a restore token on the shadow stack.  A token is always 8-byte
++ * and aligned to 8.
++ */
++static int create_rstor_token(bool proc32, unsigned long ssp,
++			      unsigned long *token_addr)
++{
++	unsigned long addr;
++
++	/* Aligned to 8 is aligned to 4, so test 8 first */
++	if ((!proc32 && !IS_ALIGNED(ssp, 8)) || !IS_ALIGNED(ssp, 4))
++		return -EINVAL;
++
++	addr = ALIGN_DOWN(ssp, 8) - 8;
++
++	/* Is the token for 64-bit? */
++	if (!proc32)
++		ssp |= BIT(0);
++
++	if (write_user_shstk_64((u64 __user *)addr, (u64)ssp))
++		return -EFAULT;
++
++	if (token_addr)
++		*token_addr = addr;
++
++	return 0;
++}
++
++static unsigned long alloc_shstk(unsigned long size, unsigned long token_offset,
++				 bool set_res_tok)
+ {
+ 	int flags = MAP_ANONYMOUS | MAP_PRIVATE;
+ 	struct mm_struct *mm = current->mm;
+@@ -34,6 +64,15 @@ static unsigned long alloc_shstk(unsigned long size)
+ 		       &unused, NULL);
+ 	mmap_write_unlock(mm);
+ 
++	if (!set_res_tok || IS_ERR_VALUE(addr))
++		goto out;
++
++	if (create_rstor_token(in_ia32_syscall(), addr + token_offset, NULL)) {
++		vm_munmap(addr, size);
++		return -EINVAL;
++	}
++
++out:
+ 	return addr;
+ }
+ 
+@@ -76,7 +115,7 @@ void shstk_setup(void)
+ 		return;
+ 
+ 	size = PAGE_ALIGN(min_t(unsigned long long, rlimit(RLIMIT_STACK), SZ_4G));
+-	addr = alloc_shstk(size);
++	addr = alloc_shstk(size, size, false);
+ 	if (IS_ERR_VALUE(addr))
+ 		return;
+ 
+@@ -147,7 +186,7 @@ int shstk_alloc_thread_stack(struct task_struct *tsk, unsigned long clone_flags,
+ 		return -EINVAL;
+ 
+ 	stack_size = PAGE_ALIGN(stack_size);
+-	addr = alloc_shstk(stack_size);
++	addr = alloc_shstk(stack_size, stack_size, false);
+ 	if (IS_ERR_VALUE(addr)) {
+ 		shstk->base = 0;
+ 		shstk->size = 0;
+@@ -216,33 +255,6 @@ static unsigned long get_user_shstk_addr(void)
+ 	return ssp;
+ }
+ 
+-/*
+- * Create a restore token on the shadow stack.  A token is always 8-byte
+- * and aligned to 8.
+- */
+-static int create_rstor_token(bool proc32, unsigned long ssp,
+-			       unsigned long *token_addr)
+-{
+-	unsigned long addr;
+-
+-	/* Aligned to 8 is aligned to 4, so test 8 first */
+-	if ((!proc32 && !IS_ALIGNED(ssp, 8)) || !IS_ALIGNED(ssp, 4))
+-		return -EINVAL;
+-
+-	addr = ALIGN_DOWN(ssp, 8) - 8;
+-
+-	/* Is the token for 64-bit? */
+-	if (!proc32)
+-		ssp |= BIT(0);
+-
+-	if (write_user_shstk_64((u64 __user *)addr, (u64)ssp))
+-		return -EFAULT;
+-
+-	*token_addr = addr;
+-
+-	return 0;
+-}
+-
+ /*
+  * Create a restore token on shadow stack, and then push the user-mode
+  * function return address.
+@@ -367,3 +379,22 @@ int restore_signal_shadow_stack(void)
+ 
+ 	return err;
+ }
++
++SYSCALL_DEFINE2(map_shadow_stack, unsigned long, size, unsigned int, flags)
++{
++	unsigned long aligned_size;
++
++	if (!cpu_feature_enabled(X86_FEATURE_SHSTK))
++		return -ENOSYS;
++
++	/*
++	 * An overflow would result in attempting to write the restore token
++	 * to the wrong location. Not catastrophic, but just return the right
++	 * error code and block it.
++	 */
++	aligned_size = PAGE_ALIGN(size);
++	if (aligned_size < size)
++		return -EOVERFLOW;
++
++	return alloc_shstk(aligned_size, size, flags & SHADOW_STACK_SET_TOKEN);
++}
+diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
+index 528a478dbda8..0ff86a848dc4 100644
+--- a/include/linux/syscalls.h
++++ b/include/linux/syscalls.h
+@@ -1057,6 +1057,7 @@ asmlinkage long sys_landlock_add_rule(int ruleset_fd, enum landlock_rule_type ru
+ 		const void __user *rule_attr, __u32 flags);
+ asmlinkage long sys_landlock_restrict_self(int ruleset_fd, __u32 flags);
+ asmlinkage long sys_memfd_secret(unsigned int flags);
++asmlinkage long sys_map_shadow_stack(unsigned long size, unsigned int flags);
+ 
+ /*
+  * Architecture-specific system calls
+diff --git a/include/uapi/asm-generic/unistd.h b/include/uapi/asm-generic/unistd.h
+index 4557a8b6086f..c8f3646de6cf 100644
+--- a/include/uapi/asm-generic/unistd.h
++++ b/include/uapi/asm-generic/unistd.h
+@@ -884,7 +884,7 @@ __SYSCALL(__NR_process_mrelease, sys_process_mrelease)
+ __SYSCALL(__NR_futex_waitv, sys_futex_waitv)
+ 
+ #undef __NR_syscalls
+-#define __NR_syscalls 450
++#define __NR_syscalls 451
+ 
+ /*
+  * 32 bit systems traditionally used different
+diff --git a/kernel/sys_ni.c b/kernel/sys_ni.c
+index d1944258cfc0..8011bedcdc38 100644
+--- a/kernel/sys_ni.c
++++ b/kernel/sys_ni.c
+@@ -379,6 +379,7 @@ COND_SYSCALL(vm86old);
+ COND_SYSCALL(modify_ldt);
+ COND_SYSCALL(vm86);
+ COND_SYSCALL(kexec_file_load);
++COND_SYSCALL(map_shadow_stack);
+ 
+ /* s390 */
+ COND_SYSCALL(s390_pci_mmio_read);
+-- 
+2.34.1
+
diff --git a/0033-selftests-x86-Add-map_shadow_stack-syscall-test.patch b/0033-selftests-x86-Add-map_shadow_stack-syscall-test.patch
new file mode 100644
index 000000000..9116ebbb5
--- /dev/null
+++ b/0033-selftests-x86-Add-map_shadow_stack-syscall-test.patch
@@ -0,0 +1,135 @@
+From c321de43ee1324b32005e4ec4c5c6202ec310eb6 Mon Sep 17 00:00:00 2001
+From: Rick Edgecombe <rick.p.edgecombe@intel.com>
+Date: Thu, 4 Nov 2021 17:49:36 -0700
+Subject: [PATCH 33/39] selftests/x86: Add map_shadow_stack syscall test
+
+Add a simple selftest for exercising the new map_shadow_stack syscall.
+
+Co-developed-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+
+===
+v1:
+- New patch
+---
+ tools/testing/selftests/x86/Makefile          |  9 ++-
+ .../selftests/x86/test_map_shadow_stack.c     | 77 +++++++++++++++++++
+ 2 files changed, 85 insertions(+), 1 deletion(-)
+ create mode 100644 tools/testing/selftests/x86/test_map_shadow_stack.c
+
+diff --git a/tools/testing/selftests/x86/Makefile b/tools/testing/selftests/x86/Makefile
+index 8a1f62ab3c8e..9114943336f9 100644
+--- a/tools/testing/selftests/x86/Makefile
++++ b/tools/testing/selftests/x86/Makefile
+@@ -9,11 +9,13 @@ UNAME_M := $(shell uname -m)
+ CAN_BUILD_I386 := $(shell ./check_cc.sh $(CC) trivial_32bit_program.c -m32)
+ CAN_BUILD_X86_64 := $(shell ./check_cc.sh $(CC) trivial_64bit_program.c)
+ CAN_BUILD_WITH_NOPIE := $(shell ./check_cc.sh $(CC) trivial_program.c -no-pie)
++CAN_BUILD_WITH_SHSTK := $(shell ./check_cc.sh $(CC) trivial_program.c -mshstk -fcf-protection)
+ 
+ TARGETS_C_BOTHBITS := single_step_syscall sysret_ss_attrs syscall_nt test_mremap_vdso \
+ 			check_initial_reg_state sigreturn iopl ioperm \
+ 			test_vsyscall mov_ss_trap \
+-			syscall_arg_fault fsgsbase_restore sigaltstack
++			syscall_arg_fault fsgsbase_restore sigaltstack \
++			test_map_shadow_stack
+ TARGETS_C_32BIT_ONLY := entry_from_vm86 test_syscall_vdso unwind_vdso \
+ 			test_FCMOV test_FCOMI test_FISTTP \
+ 			vdso_restorer
+@@ -105,3 +107,8 @@ $(OUTPUT)/test_syscall_vdso_32: thunks_32.S
+ # state.
+ $(OUTPUT)/check_initial_reg_state_32: CFLAGS += -Wl,-ereal_start -static
+ $(OUTPUT)/check_initial_reg_state_64: CFLAGS += -Wl,-ereal_start -static
++
++ifeq ($(CAN_BUILD_WITH_SHSTK),1)
++$(OUTPUT)/test_map_shadow_stack_64: CFLAGS += -mshstk -fcf-protection
++$(OUTPUT)/test_map_shadow_stack_32: CFLAGS += -mshstk -fcf-protection
++endif
+\ No newline at end of file
+diff --git a/tools/testing/selftests/x86/test_map_shadow_stack.c b/tools/testing/selftests/x86/test_map_shadow_stack.c
+new file mode 100644
+index 000000000000..654c5af363a7
+--- /dev/null
++++ b/tools/testing/selftests/x86/test_map_shadow_stack.c
+@@ -0,0 +1,77 @@
++// SPDX-License-Identifier: GPL-2.0
++
++#define _GNU_SOURCE
++
++#include <sys/mman.h>
++#include <sys/stat.h>
++#include <stdio.h>
++#include <stdlib.h>
++#include <fcntl.h>
++#include <unistd.h>
++#include <string.h>
++#include <errno.h>
++#include <stdbool.h>
++#include <x86intrin.h>
++
++#define SHADOW_STACK_SET_TOKEN	0x1
++#define __NR_map_shadow_stack 450
++
++size_t shstk_size = 0x200000;
++
++void *create_shstk(void)
++{
++	return (void *)syscall(__NR_map_shadow_stack, shstk_size, SHADOW_STACK_SET_TOKEN);
++}
++
++#if (__GNUC__ < 8) || (__GNUC__ == 8 &&__GNUC_MINOR__ < 5)
++int main(int argc, char *argv[])
++{
++	printf("SKIP: compiler does not support CET.");
++	return 0;
++}
++#else
++void try_shstk(unsigned long new_ssp)
++{
++	unsigned long ssp0, ssp1;
++
++	printf("pid=%d\n", getpid());
++	printf("new_ssp = %lx, *new_ssp = %lx\n",
++		new_ssp, *((unsigned long *)new_ssp));
++
++	ssp0 = _get_ssp();
++	printf("changing ssp from %lx to %lx\n", ssp0, new_ssp);
++
++	/* Make sure is aligned to 8 bytes */
++	if ((ssp0 & 0xf) != 0)
++		ssp0 &= -8;
++
++	asm volatile("rstorssp (%0)\n":: "r" (new_ssp));
++	asm volatile("saveprevssp");
++	ssp1 = _get_ssp();
++	printf("ssp is now %lx\n", ssp1);
++
++	ssp0 -= 8;
++	asm volatile("rstorssp (%0)\n":: "r" (ssp0));
++	asm volatile("saveprevssp");
++}
++
++int main(int argc, char *argv[])
++{
++	void *shstk;
++
++	if (!_get_ssp()) {
++		printf("SKIP: shadow stack disabled.");
++		return 0;
++	}
++
++	shstk = create_shstk();
++	if (shstk == MAP_FAILED) {
++		printf("FAIL: Error creaing shadow stack: %d\n", errno);
++		return 1;
++	}
++	try_shstk((unsigned long)shstk + shstk_size - 8);
++
++	printf("PASS.\n");
++	return 0;
++}
++#endif
+-- 
+2.34.1
+
diff --git a/0034-x86-cet-shstk-Support-wrss-for-userspace.patch b/0034-x86-cet-shstk-Support-wrss-for-userspace.patch
new file mode 100644
index 000000000..b39f3e188
--- /dev/null
+++ b/0034-x86-cet-shstk-Support-wrss-for-userspace.patch
@@ -0,0 +1,177 @@
+From 14e58a45608af920596db8c4846bb3c256a58ede Mon Sep 17 00:00:00 2001
+From: Rick Edgecombe <rick.p.edgecombe@intel.com>
+Date: Thu, 7 Oct 2021 17:14:16 -0700
+Subject: [PATCH 34/39] x86/cet/shstk: Support wrss for userspace
+
+For the current shadow stack implementation, shadow stacks contents cannot
+be arbitrarily provisioned with data. This property helps apps protect
+themselves better, but also restricts any potential apps that may want to
+do exotic things at the expense of a little security.
+
+The x86 shadow stack feature indroduces a new instruction, wrss, which
+can be enabled to write directly to shadow stack permissioned memory from
+userspace. Allow the instruction to be enabled via elf header interface.
+Since wrss is a capability, and not a restriction that could cause
+incompatibilities, also let it get enabled late via the prctl interface.
+
+Only enable the userspace wrss instruction, which allows writes to
+userspace shadow stacks from userspace. Let it be enabled independently
+of shadow stack for the application.
+
+From a fault handler perspective, WRSS will behave very similar to WRUSS, which
+is treated like a user access from a PF err code perspective.
+
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+===
+
+v1:
+- New patch
+---
+ arch/x86/include/asm/cet.h          |  3 +++
+ arch/x86/include/asm/mman.h         | 24 ++++++++++++++++++++++++
+ arch/x86/include/uapi/asm/mman.h    |  6 +++---
+ arch/x86/kernel/elf_feature_prctl.c |  2 ++
+ arch/x86/kernel/shstk.c             | 20 ++++++++++++++++++++
+ include/uapi/linux/elf.h            |  4 +++-
+ 6 files changed, 55 insertions(+), 4 deletions(-)
+ create mode 100644 arch/x86/include/asm/mman.h
+
+diff --git a/arch/x86/include/asm/cet.h b/arch/x86/include/asm/cet.h
+index caedbcf4f619..44a0e4abdbfb 100644
+--- a/arch/x86/include/asm/cet.h
++++ b/arch/x86/include/asm/cet.h
+@@ -10,6 +10,7 @@ struct task_struct;
+ struct thread_shstk {
+ 	u64	base;
+ 	u64	size;
++	bool	wrss;
+ };
+ 
+ #ifdef CONFIG_X86_SHADOW_STACK
+@@ -18,6 +19,7 @@ int shstk_alloc_thread_stack(struct task_struct *p, unsigned long clone_flags,
+ 			     unsigned long stack_size);
+ void shstk_free(struct task_struct *p);
+ void shstk_disable(void);
++void wrss_control(bool enable);
+ int shstk_setup_rstor_token(bool proc32, unsigned long restorer,
+ 			    unsigned long *new_ssp);
+ int shstk_check_rstor_token(bool proc32, unsigned long *new_ssp);
+@@ -30,6 +32,7 @@ static inline int shstk_alloc_thread_stack(struct task_struct *p,
+ 					   unsigned long stack_size) { return 0; }
+ static inline void shstk_free(struct task_struct *p) {}
+ static inline void shstk_disable(void) {}
++static inline void wrss_control(bool enable) {}
+ static inline int shstk_setup_rstor_token(bool proc32, unsigned long restorer,
+ 					  unsigned long *new_ssp) { return 0; }
+ static inline int shstk_check_rstor_token(bool proc32,
+diff --git a/arch/x86/include/asm/mman.h b/arch/x86/include/asm/mman.h
+new file mode 100644
+index 000000000000..274f5f1e1b70
+--- /dev/null
++++ b/arch/x86/include/asm/mman.h
+@@ -0,0 +1,24 @@
++/* SPDX-License-Identifier: GPL-2.0 */
++#ifndef _ASM_X86_MMAN_H
++#define _ASM_X86_MMAN_H
++
++#include <linux/mm.h>
++#include <uapi/asm/mman.h>
++
++#ifdef CONFIG_X86_SHADOW_STACK
++static inline bool arch_validate_flags(struct vm_area_struct *vma, unsigned long vm_flags)
++{
++	/*
++	 * Shadow stack must not be executable, to help with W^X due to wrss.
++	 */
++	if ((vm_flags & VM_SHADOW_STACK) && (vm_flags & VM_EXEC))
++		return false;
++
++	return true;
++}
++
++#define arch_validate_flags(vma, vm_flags) arch_validate_flags(vma, vm_flags)
++
++#endif /* CONFIG_X86_SHADOW_STACK */
++
++#endif /* _ASM_X86_MMAN_H */
+diff --git a/arch/x86/include/uapi/asm/mman.h b/arch/x86/include/uapi/asm/mman.h
+index 42bc3741c6f1..4f1a4a84e5fb 100644
+--- a/arch/x86/include/uapi/asm/mman.h
++++ b/arch/x86/include/uapi/asm/mman.h
+@@ -1,6 +1,6 @@
+ /* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+-#ifndef _ASM_X86_MMAN_H
+-#define _ASM_X86_MMAN_H
++#ifndef _UAPI_ASM_X86_MMAN_H
++#define _UAPI_ASM_X86_MMAN_H
+ 
+ #define MAP_32BIT	0x40		/* only give out 32bit addresses */
+ 
+@@ -30,4 +30,4 @@
+ 
+ #include <asm-generic/mman.h>
+ 
+-#endif /* _ASM_X86_MMAN_H */
++#endif /* _UAPI_ASM_X86_MMAN_H */
+diff --git a/arch/x86/kernel/elf_feature_prctl.c b/arch/x86/kernel/elf_feature_prctl.c
+index 97074aaff7ba..b1c6d748ccb2 100644
+--- a/arch/x86/kernel/elf_feature_prctl.c
++++ b/arch/x86/kernel/elf_feature_prctl.c
+@@ -21,6 +21,8 @@ static int elf_feat_copy_status_to_user(struct thread_shstk *shstk, u64 __user *
+ 		buf[1] = shstk->base;
+ 		buf[2] = shstk->size;
+ 	}
++	if (shstk->wrss)
++		buf[0] |= GNU_PROPERTY_X86_FEATURE_1_WRSS;
+ 
+ 	return copy_to_user(ubuf, buf, sizeof(buf));
+ }
+diff --git a/arch/x86/kernel/shstk.c b/arch/x86/kernel/shstk.c
+index 63e6dff88641..38d1dbb593ca 100644
+--- a/arch/x86/kernel/shstk.c
++++ b/arch/x86/kernel/shstk.c
+@@ -223,6 +223,26 @@ void shstk_free(struct task_struct *tsk)
+ 	shstk->size = 0;
+ }
+ 
++void wrss_control(bool enable)
++{
++	struct thread_shstk *shstk = &current->thread.shstk;
++	void *xstate;
++	int err;
++
++	if (!cpu_feature_enabled(X86_FEATURE_SHSTK) || shstk->wrss == enable)
++		return;
++
++	xstate = start_update_xsave_msrs(XFEATURE_CET_USER);
++	if (enable)
++		err = xsave_set_clear_bits_msrl(xstate, MSR_IA32_U_CET, CET_WRSS_EN, 0);
++	else
++		err = xsave_set_clear_bits_msrl(xstate, MSR_IA32_U_CET, 0, CET_WRSS_EN);
++	end_update_xsave_msrs();
++
++	WARN_ON_ONCE(err);
++	shstk->wrss = enable;
++}
++
+ void shstk_disable(void)
+ {
+ 	struct thread_shstk *shstk = &current->thread.shstk;
+diff --git a/include/uapi/linux/elf.h b/include/uapi/linux/elf.h
+index d3f1bc99b33a..c9fb616e6ee0 100644
+--- a/include/uapi/linux/elf.h
++++ b/include/uapi/linux/elf.h
+@@ -459,7 +459,9 @@ typedef struct elf64_note {
+ /* x86 GNU property bits */
+ #define GNU_PROPERTY_X86_FEATURE_1_IBT		0x00000001
+ #define GNU_PROPERTY_X86_FEATURE_1_SHSTK	0x00000002
++#define GNU_PROPERTY_X86_FEATURE_1_WRSS		0x00000010
+ #define GNU_PROPERTY_X86_FEATURE_1_VALID (GNU_PROPERTY_X86_FEATURE_1_IBT | \
+-					  GNU_PROPERTY_X86_FEATURE_1_SHSTK)
++					  GNU_PROPERTY_X86_FEATURE_1_SHSTK | \
++					  GNU_PROPERTY_X86_FEATURE_1_WRSS)
+ 
+ #endif /* _UAPI_LINUX_ELF_H */
+-- 
+2.34.1
+
diff --git a/0035-x86-cpufeatures-Limit-shadow-stack-to-Intel-CPUs.patch b/0035-x86-cpufeatures-Limit-shadow-stack-to-Intel-CPUs.patch
new file mode 100644
index 000000000..cd704d3a2
--- /dev/null
+++ b/0035-x86-cpufeatures-Limit-shadow-stack-to-Intel-CPUs.patch
@@ -0,0 +1,41 @@
+From 3cdca740b3d66a3ef8b2cec6cbe520dd8229e269 Mon Sep 17 00:00:00 2001
+From: Rick Edgecombe <rick.p.edgecombe@intel.com>
+Date: Wed, 24 Nov 2021 14:54:13 -0800
+Subject: [PATCH 35/39] x86/cpufeatures: Limit shadow stack to Intel CPUs
+
+Shadow stack is supported on newer AMD processors, but the kernel
+implementation has not been tested on them. Prevent basic issues from
+showing up for normal users by disabling shadow stack on all
+CPUs except Intel until it has been tested. At which point the
+limitation should be removed.
+
+Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
+===
+v1:
+- New patch
+---
+ arch/x86/kernel/cpu/common.c | 8 ++++++++
+ 1 file changed, 8 insertions(+)
+
+diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
+index e92a27509306..46a5ff5f0510 100644
+--- a/arch/x86/kernel/cpu/common.c
++++ b/arch/x86/kernel/cpu/common.c
+@@ -517,6 +517,14 @@ __setup("nopku", setup_disable_pku);
+ 
+ static __always_inline void setup_cet(struct cpuinfo_x86 *c)
+ {
++	/*
++	 * Shadow stack is supported on AMD processors, but has not been
++	 * tested. Only support it on Intel processors until this is done.
++	 * At which point, this vendor check should be removed.
++	 */
++	if (c->x86_vendor != X86_VENDOR_INTEL)
++		setup_clear_cpu_cap(X86_FEATURE_SHSTK);
++
+ 	if (!cpu_feature_enabled(X86_FEATURE_SHSTK))
+ 		return;
+ 
+-- 
+2.34.1
+
diff --git a/0036-x86-cet-Clear-thread.shstk-and-feat_prctl_locked.patch b/0036-x86-cet-Clear-thread.shstk-and-feat_prctl_locked.patch
new file mode 100644
index 000000000..d49eff549
--- /dev/null
+++ b/0036-x86-cet-Clear-thread.shstk-and-feat_prctl_locked.patch
@@ -0,0 +1,31 @@
+From 7cfdfbae1e24b2145e2b9ebc6de77b0d92d92472 Mon Sep 17 00:00:00 2001
+From: "H.J. Lu" <hjl.tools@gmail.com>
+Date: Sat, 15 Jan 2022 07:53:35 -0800
+Subject: [PATCH 36/39] x86/cet: Clear thread.shstk and feat_prctl_locked
+
+Clear thread.shstk and feat_prctl_locked in start_thread_common.
+
+Signed-off-by: H.J. Lu <hjl.tools@gmail.com>
+---
+ arch/x86/kernel/process_64.c | 5 +++++
+ 1 file changed, 5 insertions(+)
+
+diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
+index 3402edec236c..5a7aee8774c7 100644
+--- a/arch/x86/kernel/process_64.c
++++ b/arch/x86/kernel/process_64.c
+@@ -514,6 +514,11 @@ start_thread_common(struct pt_regs *regs, unsigned long new_ip,
+ 		load_gs_index(__USER_DS);
+ 	}
+ 
++#ifdef CONFIG_X86_SHADOW_STACK
++	memset(&current->thread.shstk, 0, sizeof(struct thread_shstk));
++	current->thread.feat_prctl_locked = 0;
++#endif
++
+ 	loadsegment(fs, 0);
+ 	loadsegment(es, _ds);
+ 	loadsegment(ds, _ds);
+-- 
+2.34.1
+
diff --git a/0037-x86-cet-Add-PTRACE-interface-for-CET.patch b/0037-x86-cet-Add-PTRACE-interface-for-CET.patch
new file mode 100644
index 000000000..ee727bd37
--- /dev/null
+++ b/0037-x86-cet-Add-PTRACE-interface-for-CET.patch
@@ -0,0 +1,166 @@
+From 5611cb325127233bae8cc2dc6b3419eb9da9cdf7 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 23 Apr 2018 12:55:13 -0700
+Subject: [PATCH 37/39] x86/cet: Add PTRACE interface for CET
+
+Add REGSET_CET64/REGSET_CET32 to get/set CET MSRs:
+
+    IA32_U_CET (user-mode CET settings) and
+    IA32_PL3_SSP (user-mode Shadow Stack)
+
+v13:
+- fpu->state.xsave -> &fpu->fpstate->regs.xsave
+v12:
+- Return -ENODEV when CET registers are in INIT state.
+- Check reserved/non-support bits from user input.
+---
+ arch/x86/include/asm/fpu/regset.h |  7 ++--
+ arch/x86/kernel/fpu/regset.c      | 56 +++++++++++++++++++++++++++++++
+ arch/x86/kernel/ptrace.c          | 16 +++++++++
+ include/uapi/linux/elf.h          |  1 +
+ 4 files changed, 77 insertions(+), 3 deletions(-)
+
+diff --git a/arch/x86/include/asm/fpu/regset.h b/arch/x86/include/asm/fpu/regset.h
+index 4f928d6a367b..8622184d87f5 100644
+--- a/arch/x86/include/asm/fpu/regset.h
++++ b/arch/x86/include/asm/fpu/regset.h
+@@ -7,11 +7,12 @@
+ 
+ #include <linux/regset.h>
+ 
+-extern user_regset_active_fn regset_fpregs_active, regset_xregset_fpregs_active;
++extern user_regset_active_fn regset_fpregs_active, regset_xregset_fpregs_active,
++				cetregs_active;
+ extern user_regset_get2_fn fpregs_get, xfpregs_get, fpregs_soft_get,
+-				 xstateregs_get;
++				 xstateregs_get, cetregs_get;
+ extern user_regset_set_fn fpregs_set, xfpregs_set, fpregs_soft_set,
+-				 xstateregs_set;
++				 xstateregs_set, cetregs_set;
+ 
+ /*
+  * xstateregs_active == regset_fpregs_active. Please refer to the comment
+diff --git a/arch/x86/kernel/fpu/regset.c b/arch/x86/kernel/fpu/regset.c
+index 437d7c930c0b..b13b09d03998 100644
+--- a/arch/x86/kernel/fpu/regset.c
++++ b/arch/x86/kernel/fpu/regset.c
+@@ -175,6 +175,62 @@ int xstateregs_set(struct task_struct *target, const struct user_regset *regset,
+ 	return ret;
+ }
+ 
++int cetregs_active(struct task_struct *target, const struct user_regset *regset)
++{
++#ifdef CONFIG_X86_SHADOW_STACK
++	if (target->thread.shstk.size)
++		return regset->n;
++#endif
++	return 0;
++}
++
++int cetregs_get(struct task_struct *target, const struct user_regset *regset,
++		struct membuf to)
++{
++	struct fpu *fpu = &target->thread.fpu;
++	struct cet_user_state *cetregs;
++
++	if (!boot_cpu_has(X86_FEATURE_SHSTK))
++		return -ENODEV;
++
++	sync_fpstate(fpu);
++	cetregs = get_xsave_addr(&fpu->fpstate->regs.xsave,
++				 XFEATURE_CET_USER);
++	if (!cetregs)
++		return -ENODEV;
++
++	return membuf_write(&to, cetregs, sizeof(struct cet_user_state));
++}
++
++int cetregs_set(struct task_struct *target, const struct user_regset *regset,
++		  unsigned int pos, unsigned int count,
++		  const void *kbuf, const void __user *ubuf)
++{
++	struct fpu *fpu = &target->thread.fpu;
++	struct cet_user_state *cetregs, tmp;
++	int r;
++
++	if (!boot_cpu_has(X86_FEATURE_SHSTK))
++		return -ENODEV;
++
++	fpu_force_restore(fpu);
++	cetregs = get_xsave_addr(&fpu->fpstate->regs.xsave,
++				 XFEATURE_CET_USER);
++	if (!cetregs)
++		return -ENODEV;
++
++	r = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &tmp, 0, -1);
++	if (r)
++		return r;
++
++	if ((tmp.user_ssp >= TASK_SIZE_MAX) || (tmp.user_cet & CET_RESERVED) ||
++	    ((tmp.user_cet & (CET_SUPPRESS | CET_WAIT_ENDBR))
++	     == (CET_SUPPRESS | CET_WAIT_ENDBR)))
++		return -EINVAL;
++	memmove(cetregs, &tmp, sizeof(tmp));
++	return 0;
++}
++
+ #if defined CONFIG_X86_32 || defined CONFIG_IA32_EMULATION
+ 
+ /*
+diff --git a/arch/x86/kernel/ptrace.c b/arch/x86/kernel/ptrace.c
+index 6d2244c94799..bf44b3a1dc08 100644
+--- a/arch/x86/kernel/ptrace.c
++++ b/arch/x86/kernel/ptrace.c
+@@ -52,7 +52,9 @@ enum x86_regset {
+ 	REGSET_IOPERM64 = REGSET_XFP,
+ 	REGSET_XSTATE,
+ 	REGSET_TLS,
++	REGSET_CET64 = REGSET_TLS,
+ 	REGSET_IOPERM32,
++	REGSET_CET32,
+ };
+ 
+ struct pt_regs_offset {
+@@ -1240,6 +1242,13 @@ static struct user_regset x86_64_regsets[] __ro_after_init = {
+ 		.size = sizeof(long), .align = sizeof(long),
+ 		.active = ioperm_active, .regset_get = ioperm_get
+ 	},
++	[REGSET_CET64] = {
++		.core_note_type = NT_X86_CET,
++		.n = sizeof(struct cet_user_state) / sizeof(u64),
++		.size = sizeof(u64), .align = sizeof(u64),
++		.active = cetregs_active, .regset_get = cetregs_get,
++		.set = cetregs_set
++	},
+ };
+ 
+ static const struct user_regset_view user_x86_64_view = {
+@@ -1295,6 +1304,13 @@ static struct user_regset x86_32_regsets[] __ro_after_init = {
+ 		.size = sizeof(u32), .align = sizeof(u32),
+ 		.active = ioperm_active, .regset_get = ioperm_get
+ 	},
++	[REGSET_CET32] = {
++		.core_note_type = NT_X86_CET,
++		.n = sizeof(struct cet_user_state) / sizeof(u64),
++		.size = sizeof(u64), .align = sizeof(u64),
++		.active = cetregs_active, .regset_get = cetregs_get,
++		.set = cetregs_set
++	},
+ };
+ 
+ static const struct user_regset_view user_x86_32_view = {
+diff --git a/include/uapi/linux/elf.h b/include/uapi/linux/elf.h
+index c9fb616e6ee0..4f2ac6e3b70d 100644
+--- a/include/uapi/linux/elf.h
++++ b/include/uapi/linux/elf.h
+@@ -402,6 +402,7 @@ typedef struct elf64_shdr {
+ #define NT_386_TLS	0x200		/* i386 TLS slots (struct user_desc) */
+ #define NT_386_IOPERM	0x201		/* x86 io permission bitmap (1=deny) */
+ #define NT_X86_XSTATE	0x202		/* x86 extended state using xsave */
++#define NT_X86_CET	0x203		/* x86 cet state */
+ #define NT_S390_HIGH_GPRS	0x300	/* s390 upper register halves */
+ #define NT_S390_TIMER	0x301		/* s390 timer register */
+ #define NT_S390_TODCMP	0x302		/* s390 TOD clock comparator register */
+-- 
+2.34.1
+
diff --git a/0038-powerpc-Keep-.rela-sections-when-CONFIG_RELOCATABLE-.patch b/0038-powerpc-Keep-.rela-sections-when-CONFIG_RELOCATABLE-.patch
new file mode 100644
index 000000000..2344bb867
--- /dev/null
+++ b/0038-powerpc-Keep-.rela-sections-when-CONFIG_RELOCATABLE-.patch
@@ -0,0 +1,56 @@
+From da7b601a10c6096a41e8a789fef5d54eed39ebc1 Mon Sep 17 00:00:00 2001
+From: "H.J. Lu" <hjl.tools@gmail.com>
+Date: Mon, 27 Apr 2020 14:04:48 -0700
+Subject: [PATCH 38/39] powerpc: Keep .rela* sections when CONFIG_RELOCATABLE
+ is defined
+
+arch/powerpc/kernel/vmlinux.lds.S has
+
+ #ifdef CONFIG_RELOCATABLE
+ ...
+        .rela.dyn : AT(ADDR(.rela.dyn) - LOAD_OFFSET)
+        {
+                __rela_dyn_start = .;
+                *(.rela*)
+        }
+ #endif
+ ...
+        DISCARDS
+        /DISCARD/ : {
+                *(*.EMB.apuinfo)
+                *(.glink .iplt .plt .rela* .comment)
+                *(.gnu.version*)
+                *(.gnu.attributes)
+                *(.eh_frame)
+        }
+
+Since .rela* sections are needed when CONFIG_RELOCATABLE is defined,
+don't discard .rela* sections if CONFIG_RELOCATABLE is defined.
+
+Signed-off-by: H.J. Lu <hjl.tools@gmail.com>
+Acked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
+---
+ arch/powerpc/kernel/vmlinux.lds.S | 5 ++++-
+ 1 file changed, 4 insertions(+), 1 deletion(-)
+
+diff --git a/arch/powerpc/kernel/vmlinux.lds.S b/arch/powerpc/kernel/vmlinux.lds.S
+index 18e42c74abdd..b2763bf3c851 100644
+--- a/arch/powerpc/kernel/vmlinux.lds.S
++++ b/arch/powerpc/kernel/vmlinux.lds.S
+@@ -401,9 +401,12 @@ SECTIONS
+ 	DISCARDS
+ 	/DISCARD/ : {
+ 		*(*.EMB.apuinfo)
+-		*(.glink .iplt .plt .rela* .comment)
++		*(.glink .iplt .plt .comment)
+ 		*(.gnu.version*)
+ 		*(.gnu.attributes)
+ 		*(.eh_frame)
++#ifndef CONFIG_RELOCATABLE
++		*(.rela*)
++#endif
+ 	}
+ }
+-- 
+2.34.1
+
diff --git a/0039-Discard-.note.gnu.property-sections-in-generic-NOTES.patch b/0039-Discard-.note.gnu.property-sections-in-generic-NOTES.patch
new file mode 100644
index 000000000..d815a99f6
--- /dev/null
+++ b/0039-Discard-.note.gnu.property-sections-in-generic-NOTES.patch
@@ -0,0 +1,81 @@
+From 3850fe5606346f33cabb5ecd781ed7abf424c2c8 Mon Sep 17 00:00:00 2001
+From: "H.J. Lu" <hjl.tools@gmail.com>
+Date: Thu, 30 Jan 2020 12:39:09 -0800
+Subject: [PATCH 39/39] Discard .note.gnu.property sections in generic NOTES
+
+With the command-line option, -mx86-used-note=yes, the x86 assembler
+in binutils 2.32 and above generates a program property note in a note
+section, .note.gnu.property, to encode used x86 ISAs and features.  But
+kernel linker script only contains a single NOTE segment:
+
+PHDRS {
+ text PT_LOAD FLAGS(5);
+ data PT_LOAD FLAGS(6);
+ percpu PT_LOAD FLAGS(6);
+ init PT_LOAD FLAGS(7);
+ note PT_NOTE FLAGS(0);
+}
+SECTIONS
+{
+...
+ .notes : AT(ADDR(.notes) - 0xffffffff80000000) { __start_notes = .; KEEP(*(.not
+e.*)) __stop_notes = .; } :text :note
+...
+}
+
+The NOTE segment generated by kernel linker script is aligned to 4 bytes.
+But .note.gnu.property section must be aligned to 8 bytes on x86-64 and
+we get
+
+[hjl@gnu-skx-1 linux]$ readelf -n vmlinux
+
+Displaying notes found in: .notes
+  Owner                Data size Description
+  Xen                  0x00000006 Unknown note type: (0x00000006)
+   description data: 6c 69 6e 75 78 00
+  Xen                  0x00000004 Unknown note type: (0x00000007)
+   description data: 32 2e 36 00
+  xen-3.0              0x00000005 Unknown note type: (0x006e6558)
+   description data: 08 00 00 00 03
+readelf: Warning: note with invalid namesz and/or descsz found at offset 0x50
+readelf: Warning:  type: 0xffffffff, namesize: 0x006e6558, descsize:
+0x80000000, alignment: 8
+[hjl@gnu-skx-1 linux]$
+
+Since note.gnu.property section in kernel image is never used, this patch
+discards .note.gnu.property sections in kernel linker script by adding
+
+/DISCARD/ : {
+  *(.note.gnu.property)
+}
+
+before kernel NOTE segment in generic NOTES.
+
+Signed-off-by: H.J. Lu <hjl.tools@gmail.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+---
+ include/asm-generic/vmlinux.lds.h | 7 +++++++
+ 1 file changed, 7 insertions(+)
+
+diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
+index 42f3866bca69..ce88ad03df16 100644
+--- a/include/asm-generic/vmlinux.lds.h
++++ b/include/asm-generic/vmlinux.lds.h
+@@ -916,7 +916,14 @@
+ #define PRINTK_INDEX
+ #endif
+ 
++/*
++ * Discard .note.gnu.property sections which are unused and have
++ * different alignment requirement from kernel note sections.
++ */
+ #define NOTES								\
++	/DISCARD/ : {							\
++		*(.note.gnu.property)					\
++	}								\
+ 	.notes : AT(ADDR(.notes) - LOAD_OFFSET) {			\
+ 		__start_notes = .;					\
+ 		KEEP(*(.note.*))					\
+-- 
+2.34.1
+
diff --git a/kernel.spec b/kernel.spec
index f60fb38ce..eed715fdc 100755
--- a/kernel.spec
+++ b/kernel.spec
@@ -1,4 +1,43 @@
-# All Global changes to build and install go here.
+Patch200001: 0001-Documentation-x86-Add-CET-description.patch
+Patch200002: 0002-x86-cet-shstk-Add-Kconfig-option-for-Shadow-Stack.patch
+Patch200003: 0003-x86-cpufeatures-Add-CET-CPU-feature-flags-for-Contro.patch
+Patch200004: 0004-x86-cpufeatures-Introduce-CPU-setup-and-option-parsi.patch
+Patch200005: 0005-x86-fpu-xstate-Introduce-CET-MSR-and-XSAVES-supervis.patch
+Patch200006: 0006-x86-cet-Add-control-protection-fault-handler.patch
+Patch200007: 0007-x86-mm-Remove-_PAGE_DIRTY-from-kernel-RO-pages.patch
+Patch200008: 0008-x86-mm-Move-pmd_write-pud_write-up-in-the-file.patch
+Patch200009: 0009-x86-mm-Introduce-_PAGE_COW.patch
+Patch200010: 0010-drm-i915-gvt-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_BITS.patch
+Patch200011: 0011-x86-mm-Update-pte_modify-for-_PAGE_COW.patch
+Patch200012: 0012-x86-mm-Update-ptep_set_wrprotect-and-pmdp_set_wrprot.patch
+Patch200013: 0013-mm-Move-VM_UFFD_MINOR_BIT-from-37-to-38.patch
+Patch200014: 0014-mm-Introduce-VM_SHADOW_STACK-for-shadow-stack-memory.patch
+Patch200015: 0015-x86-mm-Check-Shadow-Stack-page-fault-errors.patch
+Patch200016: 0016-x86-mm-Update-maybe_mkwrite-for-shadow-stack.patch
+Patch200017: 0017-mm-Fixup-places-that-call-pte_mkwrite-directly.patch
+Patch200018: 0018-mm-Add-guard-pages-around-a-shadow-stack.patch
+Patch200019: 0019-mm-mmap-Add-shadow-stack-pages-to-memory-accounting.patch
+Patch200020: 0020-mm-Update-can_follow_write_pte-for-shadow-stack.patch
+Patch200021: 0021-mm-mprotect-Exclude-shadow-stack-from-preserve_write.patch
+Patch200022: 0022-mm-Re-introduce-vm_flags-to-do_mmap.patch
+Patch200023: 0023-x86-fpu-Add-helpers-for-modifying-supervisor-xstate.patch
+Patch200024: 0024-x86-cet-shstk-Add-user-mode-shadow-stack-support.patch
+Patch200025: 0025-x86-process-Change-copy_thread-argument-arg-to-stack.patch
+Patch200026: 0026-x86-fpu-Add-unsafe-xsave-buffer-helpers.patch
+Patch200027: 0027-x86-cet-shstk-Handle-thread-shadow-stack.patch
+Patch200028: 0028-x86-cet-shstk-Introduce-shadow-stack-token-setup-ver.patch
+Patch200029: 0029-x86-cet-shstk-Handle-signals-for-shadow-stack.patch
+Patch200030: 0030-x86-cet-shstk-Add-arch_prctl-elf-feature-functions.patch
+Patch200031: 0031-mm-Update-arch_validate_flags-to-test-vma-anonymous.patch
+Patch200032: 0032-x86-cet-shstk-Introduce-map_shadow_stack-syscall.patch
+Patch200033: 0033-selftests-x86-Add-map_shadow_stack-syscall-test.patch
+Patch200034: 0034-x86-cet-shstk-Support-wrss-for-userspace.patch
+Patch200035: 0035-x86-cpufeatures-Limit-shadow-stack-to-Intel-CPUs.patch
+Patch200036: 0036-x86-cet-Clear-thread.shstk-and-feat_prctl_locked.patch
+Patch200037: 0037-x86-cet-Add-PTRACE-interface-for-CET.patch
+Patch200038: 0038-powerpc-Keep-.rela-sections-when-CONFIG_RELOCATABLE-.patch
+Patch200039: 0039-Discard-.note.gnu.property-sections-in-generic-NOTES.patch
+
 # Per the below section about __spec_install_pre, any rpm
 # environment changes that affect %%install need to go
 # here before the %%install macro is pre-built.
@@ -1396,6 +1435,47 @@ ApplyOptionalPatch patch-%{patchversion}-redhat.patch
 
 ApplyOptionalPatch linux-kernel-test.patch
 
+ApplyOptionalPatch 0001-Documentation-x86-Add-CET-description.patch
+ApplyOptionalPatch 0002-x86-cet-shstk-Add-Kconfig-option-for-Shadow-Stack.patch
+ApplyOptionalPatch 0003-x86-cpufeatures-Add-CET-CPU-feature-flags-for-Contro.patch
+ApplyOptionalPatch 0004-x86-cpufeatures-Introduce-CPU-setup-and-option-parsi.patch
+ApplyOptionalPatch 0005-x86-fpu-xstate-Introduce-CET-MSR-and-XSAVES-supervis.patch
+ApplyOptionalPatch 0006-x86-cet-Add-control-protection-fault-handler.patch
+ApplyOptionalPatch 0007-x86-mm-Remove-_PAGE_DIRTY-from-kernel-RO-pages.patch
+ApplyOptionalPatch 0008-x86-mm-Move-pmd_write-pud_write-up-in-the-file.patch
+ApplyOptionalPatch 0009-x86-mm-Introduce-_PAGE_COW.patch
+ApplyOptionalPatch 0010-drm-i915-gvt-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_BITS.patch
+ApplyOptionalPatch 0011-x86-mm-Update-pte_modify-for-_PAGE_COW.patch
+ApplyOptionalPatch 0012-x86-mm-Update-ptep_set_wrprotect-and-pmdp_set_wrprot.patch
+ApplyOptionalPatch 0013-mm-Move-VM_UFFD_MINOR_BIT-from-37-to-38.patch
+ApplyOptionalPatch 0014-mm-Introduce-VM_SHADOW_STACK-for-shadow-stack-memory.patch
+ApplyOptionalPatch 0015-x86-mm-Check-Shadow-Stack-page-fault-errors.patch
+ApplyOptionalPatch 0016-x86-mm-Update-maybe_mkwrite-for-shadow-stack.patch
+ApplyOptionalPatch 0017-mm-Fixup-places-that-call-pte_mkwrite-directly.patch
+ApplyOptionalPatch 0018-mm-Add-guard-pages-around-a-shadow-stack.patch
+ApplyOptionalPatch 0019-mm-mmap-Add-shadow-stack-pages-to-memory-accounting.patch
+ApplyOptionalPatch 0020-mm-Update-can_follow_write_pte-for-shadow-stack.patch
+ApplyOptionalPatch 0021-mm-mprotect-Exclude-shadow-stack-from-preserve_write.patch
+ApplyOptionalPatch 0022-mm-Re-introduce-vm_flags-to-do_mmap.patch
+ApplyOptionalPatch 0023-x86-fpu-Add-helpers-for-modifying-supervisor-xstate.patch
+ApplyOptionalPatch 0024-x86-cet-shstk-Add-user-mode-shadow-stack-support.patch
+ApplyOptionalPatch 0025-x86-process-Change-copy_thread-argument-arg-to-stack.patch
+ApplyOptionalPatch 0026-x86-fpu-Add-unsafe-xsave-buffer-helpers.patch
+ApplyOptionalPatch 0027-x86-cet-shstk-Handle-thread-shadow-stack.patch
+ApplyOptionalPatch 0028-x86-cet-shstk-Introduce-shadow-stack-token-setup-ver.patch
+ApplyOptionalPatch 0029-x86-cet-shstk-Handle-signals-for-shadow-stack.patch
+ApplyOptionalPatch 0030-x86-cet-shstk-Add-arch_prctl-elf-feature-functions.patch
+ApplyOptionalPatch 0031-mm-Update-arch_validate_flags-to-test-vma-anonymous.patch
+ApplyOptionalPatch 0032-x86-cet-shstk-Introduce-map_shadow_stack-syscall.patch
+ApplyOptionalPatch 0033-selftests-x86-Add-map_shadow_stack-syscall-test.patch
+ApplyOptionalPatch 0034-x86-cet-shstk-Support-wrss-for-userspace.patch
+ApplyOptionalPatch 0035-x86-cpufeatures-Limit-shadow-stack-to-Intel-CPUs.patch
+ApplyOptionalPatch 0036-x86-cet-Clear-thread.shstk-and-feat_prctl_locked.patch
+ApplyOptionalPatch 0037-x86-cet-Add-PTRACE-interface-for-CET.patch
+ApplyOptionalPatch 0038-powerpc-Keep-.rela-sections-when-CONFIG_RELOCATABLE-.patch
+ApplyOptionalPatch 0039-Discard-.note.gnu.property-sections-in-generic-NOTES.patch
+# All Global changes to build and install go here.
+
 # END OF PATCH APPLICATIONS
 
 # Any further pre-build tree manipulations happen here.
-- 
2.34.1

