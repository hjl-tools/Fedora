From d9dbba460011366b2de78a609321e1498a5a0798 Mon Sep 17 00:00:00 2001
From: "H.J. Lu" <hjl.tools@gmail.com>
Date: Tue, 14 Apr 2020 10:43:52 -0700
Subject: [PATCH 4/5] Apply Intel CET patches for Linux kernel 5.7

---
 ...ename-validate_xstate_header-to-vali.patch |  98 +++
 ...efine-new-macros-for-supervisor-and-.patch | 184 ++++++
 ...eparate-user-and-supervisor-xfeature.patch | 353 +++++++++++
 ...e-Introduce-XSAVES-supervisor-states.patch |  85 +++
 ...efine-new-functions-for-clearing-fpr.patch | 184 ++++++
 ...pdate-sanitize_restored_xstate-for-s.patch | 138 +++++
 ...pdate-copy_kernel_to_xregs_err-for-X.patch |  46 ++
 ...-Introduce-copy_supervisor_to_kernel.patch | 154 +++++
 ...reserve-supervisor-states-for-slow-p.patch | 130 ++++
 ...estore-supervisor-states-for-signal-.patch | 129 ++++
 ...ocumentation-x86-Add-CET-description.patch | 194 ++++++
 ...Add-CET-CPU-feature-flags-for-Contro.patch |  55 ++
 ...ntroduce-CET-MSR-XSAVES-supervisor-s.patch | 203 +++++++
 ...Add-control-protection-fault-handler.patch | 190 ++++++
 ...d-Kconfig-option-for-user-mode-Shado.patch |  79 +++
 ...Change-_PAGE_DIRTY-to-_PAGE_DIRTY_HW.patch | 197 ++++++
 ...-_PAGE_DIRTY_HW-from-kernel-RO-pages.patch |  56 ++
 0018-x86-mm-Introduce-_PAGE_COW.patch         | 372 ++++++++++++
 ...ange-_PAGE_DIRTY-to-_PAGE_DIRTY_BITS.patch |  37 ++
 ...6-mm-Update-pte_modify-for-_PAGE_COW.patch |  84 +++
 ...ep_set_wrprotect-and-pmdp_set_wrprot.patch | 117 ++++
 ...uce-VM_SHSTK-for-shadow-stack-memory.patch |  83 +++
 ...adow-Stack-page-fault-error-checking.patch |  96 +++
 ...pdate-maybe_mkwrite-for-shadow-stack.patch | 132 ++++
 ...laces-that-call-pte_mkwrite-directly.patch |  80 +++
 ...dd-guard-pages-around-a-shadow-stack.patch |  98 +++
 ...dow-stack-pages-to-memory-accounting.patch |  83 +++
 ...an_follow_write_pte-for-shadow-stack.patch |  82 +++
 ...shstk-User-mode-shadow-stack-support.patch | 371 ++++++++++++
 ...hstk-Handle-signals-for-shadow-stack.patch | 569 ++++++++++++++++++
 ...nfig-additions-for-ELF-program-prope.patch |  87 +++
 ...ELF-program-property-parsing-support.patch | 310 ++++++++++
 ...LF-Introduce-arch_setup_elf_property.patch |  54 ++
 ...-ELF-header-parsing-for-shadow-stack.patch |  98 +++
 ...cet-shstk-Handle-thread-shadow-stack.patch | 149 +++++
 ...d-arch_prctl-functions-for-shadow-st.patch | 282 +++++++++
 ...Kconfig-option-for-user-mode-Indirec.patch |  52 ++
 ...-mode-Indirect-Branch-Tracking-suppo.patch | 178 ++++++
 ...le-signals-for-Indirect-Branch-Track.patch | 105 ++++
 ...header-parsing-for-Indirect-Branch-T.patch |  52 ++
 ...arch_prctl-functions-for-Indirect-Br.patch |  51 ++
 ...x86-cet-Add-PTRACE-interface-for-CET.patch | 151 +++++
 ...ENDBR32-to-__kernel_vsyscall-entry-p.patch |  32 +
 ...-vdso-Insert-endbr32-endbr64-to-vDSO.patch |  35 ++
 ...yscall-emulation-when-CET-is-enabled.patch |  59 ++
 ...la-sections-when-CONFIG_RELOCATABLE-.patch |  56 ++
 ...u.property-sections-in-generic-NOTES.patch |  81 +++
 kernel.spec                                   |  48 ++
 48 files changed, 6559 insertions(+)
 create mode 100644 0001-x86-fpu-xstate-Rename-validate_xstate_header-to-vali.patch
 create mode 100644 0002-x86-fpu-xstate-Define-new-macros-for-supervisor-and-.patch
 create mode 100644 0003-x86-fpu-xstate-Separate-user-and-supervisor-xfeature.patch
 create mode 100644 0004-x86-fpu-xstate-Introduce-XSAVES-supervisor-states.patch
 create mode 100644 0005-x86-fpu-xstate-Define-new-functions-for-clearing-fpr.patch
 create mode 100644 0006-x86-fpu-xstate-Update-sanitize_restored_xstate-for-s.patch
 create mode 100644 0007-x86-fpu-xstate-Update-copy_kernel_to_xregs_err-for-X.patch
 create mode 100644 0008-x86-fpu-Introduce-copy_supervisor_to_kernel.patch
 create mode 100644 0009-x86-fpu-xstate-Preserve-supervisor-states-for-slow-p.patch
 create mode 100644 0010-x86-fpu-xstate-Restore-supervisor-states-for-signal-.patch
 create mode 100644 0011-Documentation-x86-Add-CET-description.patch
 create mode 100644 0012-x86-cpufeatures-Add-CET-CPU-feature-flags-for-Contro.patch
 create mode 100644 0013-x86-fpu-xstate-Introduce-CET-MSR-XSAVES-supervisor-s.patch
 create mode 100644 0014-x86-cet-Add-control-protection-fault-handler.patch
 create mode 100644 0015-x86-cet-shstk-Add-Kconfig-option-for-user-mode-Shado.patch
 create mode 100644 0016-x86-mm-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_HW.patch
 create mode 100644 0017-x86-mm-Remove-_PAGE_DIRTY_HW-from-kernel-RO-pages.patch
 create mode 100644 0018-x86-mm-Introduce-_PAGE_COW.patch
 create mode 100644 0019-drm-i915-gvt-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_BITS.patch
 create mode 100644 0020-x86-mm-Update-pte_modify-for-_PAGE_COW.patch
 create mode 100644 0021-x86-mm-Update-ptep_set_wrprotect-and-pmdp_set_wrprot.patch
 create mode 100644 0022-mm-Introduce-VM_SHSTK-for-shadow-stack-memory.patch
 create mode 100644 0023-x86-mm-Shadow-Stack-page-fault-error-checking.patch
 create mode 100644 0024-x86-mm-Update-maybe_mkwrite-for-shadow-stack.patch
 create mode 100644 0025-mm-Fixup-places-that-call-pte_mkwrite-directly.patch
 create mode 100644 0026-mm-Add-guard-pages-around-a-shadow-stack.patch
 create mode 100644 0027-mm-mmap-Add-shadow-stack-pages-to-memory-accounting.patch
 create mode 100644 0028-mm-Update-can_follow_write_pte-for-shadow-stack.patch
 create mode 100644 0029-x86-cet-shstk-User-mode-shadow-stack-support.patch
 create mode 100644 0030-x86-cet-shstk-Handle-signals-for-shadow-stack.patch
 create mode 100644 0031-ELF-UAPI-and-Kconfig-additions-for-ELF-program-prope.patch
 create mode 100644 0032-ELF-Add-ELF-program-property-parsing-support.patch
 create mode 100644 0033-ELF-Introduce-arch_setup_elf_property.patch
 create mode 100644 0034-x86-cet-shstk-ELF-header-parsing-for-shadow-stack.patch
 create mode 100644 0035-x86-cet-shstk-Handle-thread-shadow-stack.patch
 create mode 100644 0036-x86-cet-shstk-Add-arch_prctl-functions-for-shadow-st.patch
 create mode 100644 0037-x86-cet-ibt-Add-Kconfig-option-for-user-mode-Indirec.patch
 create mode 100644 0038-x86-cet-ibt-User-mode-Indirect-Branch-Tracking-suppo.patch
 create mode 100644 0039-x86-cet-ibt-Handle-signals-for-Indirect-Branch-Track.patch
 create mode 100644 0040-x86-cet-ibt-ELF-header-parsing-for-Indirect-Branch-T.patch
 create mode 100644 0041-x86-cet-ibt-Add-arch_prctl-functions-for-Indirect-Br.patch
 create mode 100644 0042-x86-cet-Add-PTRACE-interface-for-CET.patch
 create mode 100644 0043-x86-vdso-32-Add-ENDBR32-to-__kernel_vsyscall-entry-p.patch
 create mode 100644 0044-x86-vdso-Insert-endbr32-endbr64-to-vDSO.patch
 create mode 100644 0045-x86-Disallow-vsyscall-emulation-when-CET-is-enabled.patch
 create mode 100644 0046-powerpc-Keep-.rela-sections-when-CONFIG_RELOCATABLE-.patch
 create mode 100644 0047-Discard-.note.gnu.property-sections-in-generic-NOTES.patch

diff --git a/0001-x86-fpu-xstate-Rename-validate_xstate_header-to-vali.patch b/0001-x86-fpu-xstate-Rename-validate_xstate_header-to-vali.patch
new file mode 100644
index 000000000..1a6c49751
--- /dev/null
+++ b/0001-x86-fpu-xstate-Rename-validate_xstate_header-to-vali.patch
@@ -0,0 +1,98 @@
+From a795020f5251c22dceaa2480f0cf4d9569bcf65c Mon Sep 17 00:00:00 2001
+From: Fenghua Yu <fenghua.yu@intel.com>
+Date: Wed, 13 Dec 2017 16:08:28 -0800
+Subject: [PATCH 01/47] x86/fpu/xstate: Rename validate_xstate_header() to
+ validate_user_xstate_header()
+
+The function validate_xstate_header() validates an xstate header coming
+from userspace (PTRACE or sigreturn).  To make it clear, rename it to
+validate_user_xstate_header().
+
+v3:
+- Change validate_xstate_header_from_user() to validate_user_xstate_header().
+
+Suggested-by: Dave Hansen <dave.hansen@intel.com>
+Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
+Reviewed-by: Tony Luck <tony.luck@intel.com>
+Reviewed-by: Borislav Petkov <bp@suse.de>
+---
+ arch/x86/include/asm/fpu/xstate.h | 2 +-
+ arch/x86/kernel/fpu/regset.c      | 2 +-
+ arch/x86/kernel/fpu/signal.c      | 2 +-
+ arch/x86/kernel/fpu/xstate.c      | 6 +++---
+ 4 files changed, 6 insertions(+), 6 deletions(-)
+
+diff --git a/arch/x86/include/asm/fpu/xstate.h b/arch/x86/include/asm/fpu/xstate.h
+index c6136d79f8c0..fc4db51f3b53 100644
+--- a/arch/x86/include/asm/fpu/xstate.h
++++ b/arch/x86/include/asm/fpu/xstate.h
+@@ -56,6 +56,6 @@ int copy_kernel_to_xstate(struct xregs_state *xsave, const void *kbuf);
+ int copy_user_to_xstate(struct xregs_state *xsave, const void __user *ubuf);
+ 
+ /* Validate an xstate header supplied by userspace (ptrace or sigreturn) */
+-extern int validate_xstate_header(const struct xstate_header *hdr);
++int validate_user_xstate_header(const struct xstate_header *hdr);
+ 
+ #endif
+diff --git a/arch/x86/kernel/fpu/regset.c b/arch/x86/kernel/fpu/regset.c
+index d652b939ccfb..bd1d0649f8ce 100644
+--- a/arch/x86/kernel/fpu/regset.c
++++ b/arch/x86/kernel/fpu/regset.c
+@@ -139,7 +139,7 @@ int xstateregs_set(struct task_struct *target, const struct user_regset *regset,
+ 	} else {
+ 		ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, xsave, 0, -1);
+ 		if (!ret)
+-			ret = validate_xstate_header(&xsave->header);
++			ret = validate_user_xstate_header(&xsave->header);
+ 	}
+ 
+ 	/*
+diff --git a/arch/x86/kernel/fpu/signal.c b/arch/x86/kernel/fpu/signal.c
+index 400a05e1c1c5..585e3651b98f 100644
+--- a/arch/x86/kernel/fpu/signal.c
++++ b/arch/x86/kernel/fpu/signal.c
+@@ -366,7 +366,7 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 			ret = __copy_from_user(&fpu->state.xsave, buf_fx, state_size);
+ 
+ 			if (!ret && state_size > offsetof(struct xregs_state, header))
+-				ret = validate_xstate_header(&fpu->state.xsave.header);
++				ret = validate_user_xstate_header(&fpu->state.xsave.header);
+ 		}
+ 		if (ret)
+ 			goto err_out;
+diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c
+index 6a54e83d5589..56a4379a2001 100644
+--- a/arch/x86/kernel/fpu/xstate.c
++++ b/arch/x86/kernel/fpu/xstate.c
+@@ -472,7 +472,7 @@ int using_compacted_format(void)
+ }
+ 
+ /* Validate an xstate header supplied by userspace (ptrace or sigreturn) */
+-int validate_xstate_header(const struct xstate_header *hdr)
++int validate_user_xstate_header(const struct xstate_header *hdr)
+ {
+ 	/* No unknown or supervisor features may be set */
+ 	if (hdr->xfeatures & (~xfeatures_mask | XFEATURE_MASK_SUPERVISOR))
+@@ -1157,7 +1157,7 @@ int copy_kernel_to_xstate(struct xregs_state *xsave, const void *kbuf)
+ 
+ 	memcpy(&hdr, kbuf + offset, size);
+ 
+-	if (validate_xstate_header(&hdr))
++	if (validate_user_xstate_header(&hdr))
+ 		return -EINVAL;
+ 
+ 	for (i = 0; i < XFEATURE_MAX; i++) {
+@@ -1211,7 +1211,7 @@ int copy_user_to_xstate(struct xregs_state *xsave, const void __user *ubuf)
+ 	if (__copy_from_user(&hdr, ubuf + offset, size))
+ 		return -EFAULT;
+ 
+-	if (validate_xstate_header(&hdr))
++	if (validate_user_xstate_header(&hdr))
+ 		return -EINVAL;
+ 
+ 	for (i = 0; i < XFEATURE_MAX; i++) {
+-- 
+2.26.2
+
diff --git a/0002-x86-fpu-xstate-Define-new-macros-for-supervisor-and-.patch b/0002-x86-fpu-xstate-Define-new-macros-for-supervisor-and-.patch
new file mode 100644
index 000000000..27f492590
--- /dev/null
+++ b/0002-x86-fpu-xstate-Define-new-macros-for-supervisor-and-.patch
@@ -0,0 +1,184 @@
+From 6b03d53f4931a4fc6b58f570f7326a12868903cf Mon Sep 17 00:00:00 2001
+From: Fenghua Yu <fenghua.yu@intel.com>
+Date: Thu, 10 Nov 2016 10:13:56 -0800
+Subject: [PATCH 02/47] x86/fpu/xstate: Define new macros for supervisor and
+ user xstates
+
+XCNTXT_MASK is 'all supported xfeatures' before introducing supervisor
+xstates.  Rename it to XFEATURE_MASK_USER_SUPPORTED to make clear that
+these are user xstates.
+
+XFEATURE_MASK_SUPERVISOR is replaced with the following:
+- XFEATURE_MASK_SUPERVISOR_SUPPORTED: Currently nothing.  ENQCMD and
+  Control-flow Enforcement Technology (CET) will be introduced in separate
+  series.
+- XFEATURE_MASK_SUPERVISOR_UNSUPPORTED: Currently only Processor Trace.
+- XFEATURE_MASK_SUPERVISOR_ALL: the combination of above.
+
+v3:
+- Change SUPPORTED_XFEATURES_*, UNSUPPORTED_XFEATURES_*, ALL_XFEATURES_* to
+  XFEATURE_MASK_*.
+
+Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
+Co-developed-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
+Reviewed-by: Tony Luck <tony.luck@intel.com>
+---
+ arch/x86/include/asm/fpu/xstate.h | 36 ++++++++++++++++++++-----------
+ arch/x86/kernel/fpu/init.c        |  3 ++-
+ arch/x86/kernel/fpu/xstate.c      | 26 +++++++++++-----------
+ 3 files changed, 38 insertions(+), 27 deletions(-)
+
+diff --git a/arch/x86/include/asm/fpu/xstate.h b/arch/x86/include/asm/fpu/xstate.h
+index fc4db51f3b53..b08fa823425f 100644
+--- a/arch/x86/include/asm/fpu/xstate.h
++++ b/arch/x86/include/asm/fpu/xstate.h
+@@ -21,19 +21,29 @@
+ #define XSAVE_YMM_SIZE	    256
+ #define XSAVE_YMM_OFFSET    (XSAVE_HDR_SIZE + XSAVE_HDR_OFFSET)
+ 
+-/* Supervisor features */
+-#define XFEATURE_MASK_SUPERVISOR (XFEATURE_MASK_PT)
+-
+-/* All currently supported features */
+-#define XCNTXT_MASK		(XFEATURE_MASK_FP | \
+-				 XFEATURE_MASK_SSE | \
+-				 XFEATURE_MASK_YMM | \
+-				 XFEATURE_MASK_OPMASK | \
+-				 XFEATURE_MASK_ZMM_Hi256 | \
+-				 XFEATURE_MASK_Hi16_ZMM	 | \
+-				 XFEATURE_MASK_PKRU | \
+-				 XFEATURE_MASK_BNDREGS | \
+-				 XFEATURE_MASK_BNDCSR)
++/* All currently supported user features */
++#define XFEATURE_MASK_USER_SUPPORTED (XFEATURE_MASK_FP | \
++				      XFEATURE_MASK_SSE | \
++				      XFEATURE_MASK_YMM | \
++				      XFEATURE_MASK_OPMASK | \
++				      XFEATURE_MASK_ZMM_Hi256 | \
++				      XFEATURE_MASK_Hi16_ZMM	 | \
++				      XFEATURE_MASK_PKRU | \
++				      XFEATURE_MASK_BNDREGS | \
++				      XFEATURE_MASK_BNDCSR)
++
++/* All currently supported supervisor features */
++#define XFEATURE_MASK_SUPERVISOR_SUPPORTED (0)
++
++/*
++ * Unsupported supervisor features. When a supervisor feature in this mask is
++ * supported in the future, move it to the supported supervisor feature mask.
++ */
++#define XFEATURE_MASK_SUPERVISOR_UNSUPPORTED (XFEATURE_MASK_PT)
++
++/* All supervisor states including supported and unsupported states. */
++#define XFEATURE_MASK_SUPERVISOR_ALL (XFEATURE_MASK_SUPERVISOR_SUPPORTED | \
++				      XFEATURE_MASK_SUPERVISOR_UNSUPPORTED)
+ 
+ #ifdef CONFIG_X86_64
+ #define REX_PREFIX	"0x48, "
+diff --git a/arch/x86/kernel/fpu/init.c b/arch/x86/kernel/fpu/init.c
+index 6ce7e0a23268..61ddc3a5e5c2 100644
+--- a/arch/x86/kernel/fpu/init.c
++++ b/arch/x86/kernel/fpu/init.c
+@@ -224,7 +224,8 @@ static void __init fpu__init_system_xstate_size_legacy(void)
+  */
+ u64 __init fpu__get_supported_xfeatures_mask(void)
+ {
+-	return XCNTXT_MASK;
++	return XFEATURE_MASK_USER_SUPPORTED |
++	       XFEATURE_MASK_SUPERVISOR_SUPPORTED;
+ }
+ 
+ /* Legacy code to initialize eager fpu mode. */
+diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c
+index 56a4379a2001..f3093c63c584 100644
+--- a/arch/x86/kernel/fpu/xstate.c
++++ b/arch/x86/kernel/fpu/xstate.c
+@@ -208,14 +208,13 @@ void fpu__init_cpu_xstate(void)
+ 	if (!boot_cpu_has(X86_FEATURE_XSAVE) || !xfeatures_mask)
+ 		return;
+ 	/*
+-	 * Make it clear that XSAVES supervisor states are not yet
+-	 * implemented should anyone expect it to work by changing
+-	 * bits in XFEATURE_MASK_* macros and XCR0.
++	 * Unsupported supervisor xstates should not be found in
++	 * the xfeatures mask.
+ 	 */
+-	WARN_ONCE((xfeatures_mask & XFEATURE_MASK_SUPERVISOR),
+-		"x86/fpu: XSAVES supervisor states are not yet implemented.\n");
++	WARN_ONCE((xfeatures_mask & XFEATURE_MASK_SUPERVISOR_UNSUPPORTED),
++		  "x86/fpu: Found unsupported supervisor xstates.\n");
+ 
+-	xfeatures_mask &= ~XFEATURE_MASK_SUPERVISOR;
++	xfeatures_mask &= ~XFEATURE_MASK_SUPERVISOR_UNSUPPORTED;
+ 
+ 	cr4_set_bits(X86_CR4_OSXSAVE);
+ 	xsetbv(XCR_XFEATURE_ENABLED_MASK, xfeatures_mask);
+@@ -438,7 +437,7 @@ static int xfeature_uncompacted_offset(int xfeature_nr)
+ 	 * format. Checking a supervisor state's uncompacted offset is
+ 	 * an error.
+ 	 */
+-	if (XFEATURE_MASK_SUPERVISOR & BIT_ULL(xfeature_nr)) {
++	if (XFEATURE_MASK_SUPERVISOR_ALL & BIT_ULL(xfeature_nr)) {
+ 		WARN_ONCE(1, "No fixed offset for xstate %d\n", xfeature_nr);
+ 		return -1;
+ 	}
+@@ -475,7 +474,7 @@ int using_compacted_format(void)
+ int validate_user_xstate_header(const struct xstate_header *hdr)
+ {
+ 	/* No unknown or supervisor features may be set */
+-	if (hdr->xfeatures & (~xfeatures_mask | XFEATURE_MASK_SUPERVISOR))
++	if (hdr->xfeatures & ~(xfeatures_mask & XFEATURE_MASK_USER_SUPPORTED))
+ 		return -EINVAL;
+ 
+ 	/* Userspace must use the uncompacted format */
+@@ -768,7 +767,8 @@ void __init fpu__init_system_xstate(void)
+ 	 * Update info used for ptrace frames; use standard-format size and no
+ 	 * supervisor xstates:
+ 	 */
+-	update_regset_xstate_info(fpu_user_xstate_size,	xfeatures_mask & ~XFEATURE_MASK_SUPERVISOR);
++	update_regset_xstate_info(fpu_user_xstate_size,
++				  xfeatures_mask & XFEATURE_MASK_USER_SUPPORTED);
+ 
+ 	fpu__init_prepare_fx_sw_frame();
+ 	setup_init_fpu_buf();
+@@ -1010,7 +1010,7 @@ int copy_xstate_to_kernel(void *kbuf, struct xregs_state *xsave, unsigned int of
+ 	 */
+ 	memset(&header, 0, sizeof(header));
+ 	header.xfeatures = xsave->header.xfeatures;
+-	header.xfeatures &= ~XFEATURE_MASK_SUPERVISOR;
++	header.xfeatures &= XFEATURE_MASK_USER_SUPPORTED;
+ 
+ 	if (header.xfeatures & XFEATURE_MASK_FP)
+ 		copy_part(0, off_mxcsr,
+@@ -1090,7 +1090,7 @@ int copy_xstate_to_user(void __user *ubuf, struct xregs_state *xsave, unsigned i
+ 	 */
+ 	memset(&header, 0, sizeof(header));
+ 	header.xfeatures = xsave->header.xfeatures;
+-	header.xfeatures &= ~XFEATURE_MASK_SUPERVISOR;
++	header.xfeatures &= XFEATURE_MASK_USER_SUPPORTED;
+ 
+ 	/*
+ 	 * Copy xregs_state->header:
+@@ -1183,7 +1183,7 @@ int copy_kernel_to_xstate(struct xregs_state *xsave, const void *kbuf)
+ 	 * The state that came in from userspace was user-state only.
+ 	 * Mask all the user states out of 'xfeatures':
+ 	 */
+-	xsave->header.xfeatures &= XFEATURE_MASK_SUPERVISOR;
++	xsave->header.xfeatures &= XFEATURE_MASK_SUPERVISOR_ALL;
+ 
+ 	/*
+ 	 * Add back in the features that came in from userspace:
+@@ -1239,7 +1239,7 @@ int copy_user_to_xstate(struct xregs_state *xsave, const void __user *ubuf)
+ 	 * The state that came in from userspace was user-state only.
+ 	 * Mask all the user states out of 'xfeatures':
+ 	 */
+-	xsave->header.xfeatures &= XFEATURE_MASK_SUPERVISOR;
++	xsave->header.xfeatures &= XFEATURE_MASK_SUPERVISOR_ALL;
+ 
+ 	/*
+ 	 * Add back in the features that came in from userspace:
+-- 
+2.26.2
+
diff --git a/0003-x86-fpu-xstate-Separate-user-and-supervisor-xfeature.patch b/0003-x86-fpu-xstate-Separate-user-and-supervisor-xfeature.patch
new file mode 100644
index 000000000..d2ece6d04
--- /dev/null
+++ b/0003-x86-fpu-xstate-Separate-user-and-supervisor-xfeature.patch
@@ -0,0 +1,353 @@
+From 973a93ce4dc3ef0e3a6d341a2a0cbb2493808359 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Wed, 13 Dec 2017 16:08:28 -0800
+Subject: [PATCH 03/47] x86/fpu/xstate: Separate user and supervisor xfeatures
+ mask
+
+Before the introduction of XSAVES supervisor states, 'xfeatures_mask' is
+used at various places to determine XSAVE buffer components and XCR0 bits.
+It contains only user xstates.  To support supervisor xstates, it is
+necessary to separate user and supervisor xstates:
+
+- First, change 'xfeatures_mask' to 'xfeatures_mask_all', which represents
+  the full set of bits that should ever be set in a kernel XSAVE buffer.
+- Introduce xfeatures_mask_supervisor() and xfeatures_mask_user() to
+  extract relevant xfeatures from xfeatures_mask_all.
+
+v3:
+- Change xfeature_enabled() type from static int to static bool while at
+  it.
+
+v2:
+- Fix typo in commit log.
+- Move xfeatures_mask_supervisor() from xstate.c to xstate.h.
+- Remove printing of user xstates from fpu__init_system_xstate().
+
+Co-developed-by: Fenghua Yu <fenghua.yu@intel.com>
+Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
+Reviewed-by: Tony Luck <tony.luck@intel.com>
+---
+ arch/x86/include/asm/fpu/internal.h |  2 +-
+ arch/x86/include/asm/fpu/xstate.h   | 13 ++++-
+ arch/x86/kernel/fpu/signal.c        | 16 +++++--
+ arch/x86/kernel/fpu/xstate.c        | 73 +++++++++++++++++------------
+ 4 files changed, 67 insertions(+), 37 deletions(-)
+
+diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
+index 44c48e34d799..ccb1bb32ad7d 100644
+--- a/arch/x86/include/asm/fpu/internal.h
++++ b/arch/x86/include/asm/fpu/internal.h
+@@ -92,7 +92,7 @@ static inline void fpstate_init_xstate(struct xregs_state *xsave)
+ 	 * XRSTORS requires these bits set in xcomp_bv, or it will
+ 	 * trigger #GP:
+ 	 */
+-	xsave->header.xcomp_bv = XCOMP_BV_COMPACTED_FORMAT | xfeatures_mask;
++	xsave->header.xcomp_bv = XCOMP_BV_COMPACTED_FORMAT | xfeatures_mask_all;
+ }
+ 
+ static inline void fpstate_init_fxstate(struct fxregs_state *fx)
+diff --git a/arch/x86/include/asm/fpu/xstate.h b/arch/x86/include/asm/fpu/xstate.h
+index b08fa823425f..92104b298d77 100644
+--- a/arch/x86/include/asm/fpu/xstate.h
++++ b/arch/x86/include/asm/fpu/xstate.h
+@@ -51,7 +51,18 @@
+ #define REX_PREFIX
+ #endif
+ 
+-extern u64 xfeatures_mask;
++extern u64 xfeatures_mask_all;
++
++static inline u64 xfeatures_mask_supervisor(void)
++{
++	return xfeatures_mask_all & XFEATURE_MASK_SUPERVISOR_SUPPORTED;
++}
++
++static inline u64 xfeatures_mask_user(void)
++{
++	return xfeatures_mask_all & XFEATURE_MASK_USER_SUPPORTED;
++}
++
+ extern u64 xstate_fx_sw_bytes[USER_XSTATE_FX_SW_WORDS];
+ 
+ extern void __init update_regset_xstate_info(unsigned int size,
+diff --git a/arch/x86/kernel/fpu/signal.c b/arch/x86/kernel/fpu/signal.c
+index 585e3651b98f..3df0cfae535f 100644
+--- a/arch/x86/kernel/fpu/signal.c
++++ b/arch/x86/kernel/fpu/signal.c
+@@ -252,13 +252,17 @@ sanitize_restored_xstate(union fpregs_state *state,
+  */
+ static int copy_user_to_fpregs_zeroing(void __user *buf, u64 xbv, int fx_only)
+ {
++	u64 init_bv;
++
+ 	if (use_xsave()) {
+ 		if (fx_only) {
+-			u64 init_bv = xfeatures_mask & ~XFEATURE_MASK_FPSSE;
++			init_bv = xfeatures_mask_user() & ~XFEATURE_MASK_FPSSE;
++
+ 			copy_kernel_to_xregs(&init_fpstate.xsave, init_bv);
+ 			return copy_user_to_fxregs(buf);
+ 		} else {
+-			u64 init_bv = xfeatures_mask & ~xbv;
++			init_bv = xfeatures_mask_user() & ~xbv;
++
+ 			if (unlikely(init_bv))
+ 				copy_kernel_to_xregs(&init_fpstate.xsave, init_bv);
+ 			return copy_user_to_xregs(buf, xbv);
+@@ -358,7 +362,7 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 
+ 
+ 	if (use_xsave() && !fx_only) {
+-		u64 init_bv = xfeatures_mask & ~xfeatures;
++		u64 init_bv = xfeatures_mask_user() & ~xfeatures;
+ 
+ 		if (using_compacted_format()) {
+ 			ret = copy_user_to_xstate(&fpu->state.xsave, buf_fx);
+@@ -389,7 +393,9 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 
+ 		fpregs_lock();
+ 		if (use_xsave()) {
+-			u64 init_bv = xfeatures_mask & ~XFEATURE_MASK_FPSSE;
++			u64 init_bv;
++
++			init_bv = xfeatures_mask_user() & ~XFEATURE_MASK_FPSSE;
+ 			copy_kernel_to_xregs(&init_fpstate.xsave, init_bv);
+ 		}
+ 
+@@ -465,7 +471,7 @@ void fpu__init_prepare_fx_sw_frame(void)
+ 
+ 	fx_sw_reserved.magic1 = FP_XSTATE_MAGIC1;
+ 	fx_sw_reserved.extended_size = size;
+-	fx_sw_reserved.xfeatures = xfeatures_mask;
++	fx_sw_reserved.xfeatures = xfeatures_mask_user();
+ 	fx_sw_reserved.xstate_size = fpu_user_xstate_size;
+ 
+ 	if (IS_ENABLED(CONFIG_IA32_EMULATION) ||
+diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c
+index f3093c63c584..18e0e2a4c890 100644
+--- a/arch/x86/kernel/fpu/xstate.c
++++ b/arch/x86/kernel/fpu/xstate.c
+@@ -54,9 +54,10 @@ static short xsave_cpuid_features[] __initdata = {
+ };
+ 
+ /*
+- * Mask of xstate features supported by the CPU and the kernel:
++ * This represents the full set of bits that should ever be set in a kernel
++ * XSAVE buffer, both supervisor and user xstates.
+  */
+-u64 xfeatures_mask __read_mostly;
++u64 xfeatures_mask_all __read_mostly;
+ 
+ static unsigned int xstate_offsets[XFEATURE_MAX] = { [ 0 ... XFEATURE_MAX - 1] = -1};
+ static unsigned int xstate_sizes[XFEATURE_MAX]   = { [ 0 ... XFEATURE_MAX - 1] = -1};
+@@ -76,7 +77,7 @@ unsigned int fpu_user_xstate_size;
+  */
+ int cpu_has_xfeatures(u64 xfeatures_needed, const char **feature_name)
+ {
+-	u64 xfeatures_missing = xfeatures_needed & ~xfeatures_mask;
++	u64 xfeatures_missing = xfeatures_needed & ~xfeatures_mask_all;
+ 
+ 	if (unlikely(feature_name)) {
+ 		long xfeature_idx, max_idx;
+@@ -150,7 +151,7 @@ void fpstate_sanitize_xstate(struct fpu *fpu)
+ 	 * None of the feature bits are in init state. So nothing else
+ 	 * to do for us, as the memory layout is up to date.
+ 	 */
+-	if ((xfeatures & xfeatures_mask) == xfeatures_mask)
++	if ((xfeatures & xfeatures_mask_all) == xfeatures_mask_all)
+ 		return;
+ 
+ 	/*
+@@ -177,7 +178,7 @@ void fpstate_sanitize_xstate(struct fpu *fpu)
+ 	 * in a special way already:
+ 	 */
+ 	feature_bit = 0x2;
+-	xfeatures = (xfeatures_mask & ~xfeatures) >> 2;
++	xfeatures = (xfeatures_mask_user() & ~xfeatures) >> 2;
+ 
+ 	/*
+ 	 * Update all the remaining memory layouts according to their
+@@ -205,19 +206,28 @@ void fpstate_sanitize_xstate(struct fpu *fpu)
+  */
+ void fpu__init_cpu_xstate(void)
+ {
+-	if (!boot_cpu_has(X86_FEATURE_XSAVE) || !xfeatures_mask)
++	u64 unsup_bits;
++
++	if (!boot_cpu_has(X86_FEATURE_XSAVE) || !xfeatures_mask_all)
+ 		return;
+ 	/*
+ 	 * Unsupported supervisor xstates should not be found in
+ 	 * the xfeatures mask.
+ 	 */
+-	WARN_ONCE((xfeatures_mask & XFEATURE_MASK_SUPERVISOR_UNSUPPORTED),
+-		  "x86/fpu: Found unsupported supervisor xstates.\n");
++	unsup_bits = xfeatures_mask_all & XFEATURE_MASK_SUPERVISOR_UNSUPPORTED;
++	WARN_ONCE(unsup_bits, "x86/fpu: Found unsupported supervisor xstates: 0x%llx\n",
++		  unsup_bits);
+ 
+-	xfeatures_mask &= ~XFEATURE_MASK_SUPERVISOR_UNSUPPORTED;
++	xfeatures_mask_all &= ~XFEATURE_MASK_SUPERVISOR_UNSUPPORTED;
+ 
+ 	cr4_set_bits(X86_CR4_OSXSAVE);
+-	xsetbv(XCR_XFEATURE_ENABLED_MASK, xfeatures_mask);
++
++	/*
++	 * XCR_XFEATURE_ENABLED_MASK (aka. XCR0) sets user features
++	 * managed by XSAVE{C, OPT, S} and XRSTOR{S}.  Only XSAVE user
++	 * states can be set here.
++	 */
++	xsetbv(XCR_XFEATURE_ENABLED_MASK, xfeatures_mask_user());
+ }
+ 
+ /*
+@@ -225,9 +235,9 @@ void fpu__init_cpu_xstate(void)
+  * functions here: one for user xstates and the other for
+  * system xstates.  For now, they are the same.
+  */
+-static int xfeature_enabled(enum xfeature xfeature)
++static bool xfeature_enabled(enum xfeature xfeature)
+ {
+-	return !!(xfeatures_mask & (1UL << xfeature));
++	return xfeatures_mask_all & BIT_ULL(xfeature);
+ }
+ 
+ /*
+@@ -414,7 +424,7 @@ static void __init setup_init_fpu_buf(void)
+ 
+ 	if (boot_cpu_has(X86_FEATURE_XSAVES))
+ 		init_fpstate.xsave.header.xcomp_bv = XCOMP_BV_COMPACTED_FORMAT |
+-						     xfeatures_mask;
++						     xfeatures_mask_all;
+ 
+ 	/*
+ 	 * Init all the features state with header.xfeatures being 0x0
+@@ -474,7 +484,7 @@ int using_compacted_format(void)
+ int validate_user_xstate_header(const struct xstate_header *hdr)
+ {
+ 	/* No unknown or supervisor features may be set */
+-	if (hdr->xfeatures & ~(xfeatures_mask & XFEATURE_MASK_USER_SUPPORTED))
++	if (hdr->xfeatures & ~xfeatures_mask_user())
+ 		return -EINVAL;
+ 
+ 	/* Userspace must use the uncompacted format */
+@@ -609,7 +619,7 @@ static void do_extra_xstate_size_checks(void)
+ 
+ 
+ /*
+- * Get total size of enabled xstates in XCR0/xfeatures_mask.
++ * Get total size of enabled xstates in XCR0 | IA32_XSS.
+  *
+  * Note the SDM's wording here.  "sub-function 0" only enumerates
+  * the size of the *user* states.  If we use it to size a buffer
+@@ -699,7 +709,7 @@ static int __init init_xstate_size(void)
+  */
+ static void fpu__init_disable_system_xstate(void)
+ {
+-	xfeatures_mask = 0;
++	xfeatures_mask_all = 0;
+ 	cr4_clear_bits(X86_CR4_OSXSAVE);
+ 	setup_clear_cpu_cap(X86_FEATURE_XSAVE);
+ }
+@@ -734,16 +744,21 @@ void __init fpu__init_system_xstate(void)
+ 		return;
+ 	}
+ 
++	/*
++	 * Find user xstates supported by the processor.
++	 */
+ 	cpuid_count(XSTATE_CPUID, 0, &eax, &ebx, &ecx, &edx);
+-	xfeatures_mask = eax + ((u64)edx << 32);
++	xfeatures_mask_all = eax + ((u64)edx << 32);
+ 
+-	if ((xfeatures_mask & XFEATURE_MASK_FPSSE) != XFEATURE_MASK_FPSSE) {
++	/* Place supervisor features in xfeatures_mask_all here */
++	if ((xfeatures_mask_user() & XFEATURE_MASK_FPSSE) != XFEATURE_MASK_FPSSE) {
+ 		/*
+ 		 * This indicates that something really unexpected happened
+ 		 * with the enumeration.  Disable XSAVE and try to continue
+ 		 * booting without it.  This is too early to BUG().
+ 		 */
+-		pr_err("x86/fpu: FP/SSE not present amongst the CPU's xstate features: 0x%llx.\n", xfeatures_mask);
++		pr_err("x86/fpu: FP/SSE not present amongst the CPU's xstate features: 0x%llx.\n",
++		       xfeatures_mask_all);
+ 		goto out_disable;
+ 	}
+ 
+@@ -752,10 +767,10 @@ void __init fpu__init_system_xstate(void)
+ 	 */
+ 	for (i = 0; i < ARRAY_SIZE(xsave_cpuid_features); i++) {
+ 		if (!boot_cpu_has(xsave_cpuid_features[i]))
+-			xfeatures_mask &= ~BIT(i);
++			xfeatures_mask_all &= ~BIT_ULL(i);
+ 	}
+ 
+-	xfeatures_mask &= fpu__get_supported_xfeatures_mask();
++	xfeatures_mask_all &= fpu__get_supported_xfeatures_mask();
+ 
+ 	/* Enable xstate instructions to be able to continue with initialization: */
+ 	fpu__init_cpu_xstate();
+@@ -767,8 +782,7 @@ void __init fpu__init_system_xstate(void)
+ 	 * Update info used for ptrace frames; use standard-format size and no
+ 	 * supervisor xstates:
+ 	 */
+-	update_regset_xstate_info(fpu_user_xstate_size,
+-				  xfeatures_mask & XFEATURE_MASK_USER_SUPPORTED);
++	update_regset_xstate_info(fpu_user_xstate_size, xfeatures_mask_user());
+ 
+ 	fpu__init_prepare_fx_sw_frame();
+ 	setup_init_fpu_buf();
+@@ -776,7 +790,7 @@ void __init fpu__init_system_xstate(void)
+ 	print_xstate_offset_size();
+ 
+ 	pr_info("x86/fpu: Enabled xstate features 0x%llx, context size is %d bytes, using '%s' format.\n",
+-		xfeatures_mask,
++		xfeatures_mask_all,
+ 		fpu_kernel_xstate_size,
+ 		boot_cpu_has(X86_FEATURE_XSAVES) ? "compacted" : "standard");
+ 	return;
+@@ -795,7 +809,7 @@ void fpu__resume_cpu(void)
+ 	 * Restore XCR0 on xsave capable CPUs:
+ 	 */
+ 	if (boot_cpu_has(X86_FEATURE_XSAVE))
+-		xsetbv(XCR_XFEATURE_ENABLED_MASK, xfeatures_mask);
++		xsetbv(XCR_XFEATURE_ENABLED_MASK, xfeatures_mask_user());
+ }
+ 
+ /*
+@@ -840,10 +854,9 @@ void *get_xsave_addr(struct xregs_state *xsave, int xfeature_nr)
+ 
+ 	/*
+ 	 * We should not ever be requesting features that we
+-	 * have not enabled.  Remember that xfeatures_mask is
+-	 * what we write to the XCR0 register.
++	 * have not enabled.
+ 	 */
+-	WARN_ONCE(!(xfeatures_mask & BIT_ULL(xfeature_nr)),
++	WARN_ONCE(!(xfeatures_mask_all & BIT_ULL(xfeature_nr)),
+ 		  "get of unsupported state");
+ 	/*
+ 	 * This assumes the last 'xsave*' instruction to
+@@ -1010,7 +1023,7 @@ int copy_xstate_to_kernel(void *kbuf, struct xregs_state *xsave, unsigned int of
+ 	 */
+ 	memset(&header, 0, sizeof(header));
+ 	header.xfeatures = xsave->header.xfeatures;
+-	header.xfeatures &= XFEATURE_MASK_USER_SUPPORTED;
++	header.xfeatures &= xfeatures_mask_user();
+ 
+ 	if (header.xfeatures & XFEATURE_MASK_FP)
+ 		copy_part(0, off_mxcsr,
+@@ -1090,7 +1103,7 @@ int copy_xstate_to_user(void __user *ubuf, struct xregs_state *xsave, unsigned i
+ 	 */
+ 	memset(&header, 0, sizeof(header));
+ 	header.xfeatures = xsave->header.xfeatures;
+-	header.xfeatures &= XFEATURE_MASK_USER_SUPPORTED;
++	header.xfeatures &= xfeatures_mask_user();
+ 
+ 	/*
+ 	 * Copy xregs_state->header:
+-- 
+2.26.2
+
diff --git a/0004-x86-fpu-xstate-Introduce-XSAVES-supervisor-states.patch b/0004-x86-fpu-xstate-Introduce-XSAVES-supervisor-states.patch
new file mode 100644
index 000000000..dc0c79e3a
--- /dev/null
+++ b/0004-x86-fpu-xstate-Introduce-XSAVES-supervisor-states.patch
@@ -0,0 +1,85 @@
+From 18ab1746423145306069f2acb972197f723dc6b0 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Thu, 10 Nov 2016 10:13:56 -0800
+Subject: [PATCH 04/47] x86/fpu/xstate: Introduce XSAVES supervisor states
+
+Enable XSAVES supervisor states by setting MSR_IA32_XSS bits according to
+CPUID enumeration results.  Also revise comments at various places.
+
+v2:
+- Remove printing of supervisor xstates from fpu__init_system_xstate().
+
+Co-developed-by: Fenghua Yu <fenghua.yu@intel.com>
+Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
+Reviewed-by: Tony Luck <tony.luck@intel.com>
+---
+ arch/x86/kernel/fpu/xstate.c | 28 +++++++++++++++++++---------
+ 1 file changed, 19 insertions(+), 9 deletions(-)
+
+diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c
+index 18e0e2a4c890..95f09c2e8cd6 100644
+--- a/arch/x86/kernel/fpu/xstate.c
++++ b/arch/x86/kernel/fpu/xstate.c
+@@ -228,13 +228,14 @@ void fpu__init_cpu_xstate(void)
+ 	 * states can be set here.
+ 	 */
+ 	xsetbv(XCR_XFEATURE_ENABLED_MASK, xfeatures_mask_user());
++
++	/*
++	 * MSR_IA32_XSS sets supervisor states managed by XSAVES.
++	 */
++	if (boot_cpu_has(X86_FEATURE_XSAVES))
++		wrmsrl(MSR_IA32_XSS, xfeatures_mask_supervisor());
+ }
+ 
+-/*
+- * Note that in the future we will likely need a pair of
+- * functions here: one for user xstates and the other for
+- * system xstates.  For now, they are the same.
+- */
+ static bool xfeature_enabled(enum xfeature xfeature)
+ {
+ 	return xfeatures_mask_all & BIT_ULL(xfeature);
+@@ -625,9 +626,6 @@ static void do_extra_xstate_size_checks(void)
+  * the size of the *user* states.  If we use it to size a buffer
+  * that we use 'XSAVES' on, we could potentially overflow the
+  * buffer because 'XSAVES' saves system states too.
+- *
+- * Note that we do not currently set any bits on IA32_XSS so
+- * 'XCR0 | IA32_XSS == XCR0' for now.
+  */
+ static unsigned int __init get_xsaves_size(void)
+ {
+@@ -750,7 +748,12 @@ void __init fpu__init_system_xstate(void)
+ 	cpuid_count(XSTATE_CPUID, 0, &eax, &ebx, &ecx, &edx);
+ 	xfeatures_mask_all = eax + ((u64)edx << 32);
+ 
+-	/* Place supervisor features in xfeatures_mask_all here */
++	/*
++	 * Find supervisor xstates supported by the processor.
++	 */
++	cpuid_count(XSTATE_CPUID, 1, &eax, &ebx, &ecx, &edx);
++	xfeatures_mask_all |= ecx + ((u64)edx << 32);
++
+ 	if ((xfeatures_mask_user() & XFEATURE_MASK_FPSSE) != XFEATURE_MASK_FPSSE) {
+ 		/*
+ 		 * This indicates that something really unexpected happened
+@@ -810,6 +813,13 @@ void fpu__resume_cpu(void)
+ 	 */
+ 	if (boot_cpu_has(X86_FEATURE_XSAVE))
+ 		xsetbv(XCR_XFEATURE_ENABLED_MASK, xfeatures_mask_user());
++
++	/*
++	 * Restore IA32_XSS. The same CPUID bit enumerates support
++	 * of XSAVES and MSR_IA32_XSS.
++	 */
++	if (boot_cpu_has(X86_FEATURE_XSAVES))
++		wrmsrl(MSR_IA32_XSS, xfeatures_mask_supervisor());
+ }
+ 
+ /*
+-- 
+2.26.2
+
diff --git a/0005-x86-fpu-xstate-Define-new-functions-for-clearing-fpr.patch b/0005-x86-fpu-xstate-Define-new-functions-for-clearing-fpr.patch
new file mode 100644
index 000000000..bb491e023
--- /dev/null
+++ b/0005-x86-fpu-xstate-Define-new-functions-for-clearing-fpr.patch
@@ -0,0 +1,184 @@
+From ecde7e8b453458e3424431e3d4f8a3800d74e259 Mon Sep 17 00:00:00 2001
+From: Fenghua Yu <fenghua.yu@intel.com>
+Date: Thu, 10 Nov 2016 10:13:56 -0800
+Subject: [PATCH 05/47] x86/fpu/xstate: Define new functions for clearing
+ fpregs and xstates
+
+Currently, fpu__clear() clears all fpregs and xstates.  Once XSAVES
+supervisor states are introduced, supervisor settings (e.g. CET xstates)
+must remain active for signals; It is necessary to have separate functions:
+
+- Create fpu__clear_user_states(): clear only user settings for signals;
+- Create fpu__clear_all(): clear both user and supervisor settings in
+   flush_thread().
+
+Also modify copy_init_fpstate_to_fpregs() to take a mask from above two
+functions.
+
+Remove obvious side-comment in fpu__clear(), while at it.
+
+Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
+Co-developed-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
+Reviewed-by: Tony Luck <tony.luck@intel.com>
+
+v3:
+- Put common code into a static function fpu__clear(), with a parameter
+  user_only.
+
+v2:
+- Fixed an issue where fpu__clear_user_states() drops supervisor xstates.
+- Revise commit log.
+---
+ arch/x86/include/asm/fpu/internal.h |  3 +-
+ arch/x86/kernel/fpu/core.c          | 49 +++++++++++++++++++----------
+ arch/x86/kernel/fpu/signal.c        |  4 +--
+ arch/x86/kernel/process.c           |  2 +-
+ arch/x86/kernel/signal.c            |  2 +-
+ 5 files changed, 39 insertions(+), 21 deletions(-)
+
+diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
+index ccb1bb32ad7d..a42fcb4b690d 100644
+--- a/arch/x86/include/asm/fpu/internal.h
++++ b/arch/x86/include/asm/fpu/internal.h
+@@ -31,7 +31,8 @@ extern void fpu__save(struct fpu *fpu);
+ extern int  fpu__restore_sig(void __user *buf, int ia32_frame);
+ extern void fpu__drop(struct fpu *fpu);
+ extern int  fpu__copy(struct task_struct *dst, struct task_struct *src);
+-extern void fpu__clear(struct fpu *fpu);
++extern void fpu__clear_user_states(struct fpu *fpu);
++extern void fpu__clear_all(struct fpu *fpu);
+ extern int  fpu__exception_code(struct fpu *fpu, int trap_nr);
+ extern int  dump_fpu(struct pt_regs *ptregs, struct user_i387_struct *fpstate);
+ 
+diff --git a/arch/x86/kernel/fpu/core.c b/arch/x86/kernel/fpu/core.c
+index 12c70840980e..7fddd5d60443 100644
+--- a/arch/x86/kernel/fpu/core.c
++++ b/arch/x86/kernel/fpu/core.c
+@@ -294,12 +294,10 @@ void fpu__drop(struct fpu *fpu)
+  * Clear FPU registers by setting them up from
+  * the init fpstate:
+  */
+-static inline void copy_init_fpstate_to_fpregs(void)
++static inline void copy_init_fpstate_to_fpregs(u64 features_mask)
+ {
+-	fpregs_lock();
+-
+ 	if (use_xsave())
+-		copy_kernel_to_xregs(&init_fpstate.xsave, -1);
++		copy_kernel_to_xregs(&init_fpstate.xsave, features_mask);
+ 	else if (static_cpu_has(X86_FEATURE_FXSR))
+ 		copy_kernel_to_fxregs(&init_fpstate.fxsave);
+ 	else
+@@ -307,9 +305,6 @@ static inline void copy_init_fpstate_to_fpregs(void)
+ 
+ 	if (boot_cpu_has(X86_FEATURE_OSPKE))
+ 		copy_init_pkru_to_fpregs();
+-
+-	fpregs_mark_activate();
+-	fpregs_unlock();
+ }
+ 
+ /*
+@@ -318,18 +313,40 @@ static inline void copy_init_fpstate_to_fpregs(void)
+  * Called by sys_execve(), by the signal handler code and by various
+  * error paths.
+  */
+-void fpu__clear(struct fpu *fpu)
++static void fpu__clear(struct fpu *fpu, int user_only)
+ {
+-	WARN_ON_FPU(fpu != &current->thread.fpu); /* Almost certainly an anomaly */
++	WARN_ON_FPU(fpu != &current->thread.fpu);
+ 
+-	fpu__drop(fpu);
++	if (!static_cpu_has(X86_FEATURE_FPU)) {
++		fpu__drop(fpu);
++		fpu__initialize(fpu);
++		return;
++	}
+ 
+-	/*
+-	 * Make sure fpstate is cleared and initialized.
+-	 */
+-	fpu__initialize(fpu);
+-	if (static_cpu_has(X86_FEATURE_FPU))
+-		copy_init_fpstate_to_fpregs();
++	fpregs_lock();
++
++	if (user_only) {
++		if (!fpregs_state_valid(fpu, smp_processor_id()) &&
++		    xfeatures_mask_supervisor())
++			copy_kernel_to_xregs(&fpu->state.xsave,
++					     xfeatures_mask_supervisor());
++		copy_init_fpstate_to_fpregs(xfeatures_mask_user());
++	} else {
++		copy_init_fpstate_to_fpregs(xfeatures_mask_all);
++	}
++
++	fpregs_mark_activate();
++	fpregs_unlock();
++}
++
++void fpu__clear_user_states(struct fpu *fpu)
++{
++	fpu__clear(fpu, 1);
++}
++
++void fpu__clear_all(struct fpu *fpu)
++{
++	fpu__clear(fpu, 0);
+ }
+ 
+ /*
+diff --git a/arch/x86/kernel/fpu/signal.c b/arch/x86/kernel/fpu/signal.c
+index 3df0cfae535f..cd6eafba12da 100644
+--- a/arch/x86/kernel/fpu/signal.c
++++ b/arch/x86/kernel/fpu/signal.c
+@@ -289,7 +289,7 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 			 IS_ENABLED(CONFIG_IA32_EMULATION));
+ 
+ 	if (!buf) {
+-		fpu__clear(fpu);
++		fpu__clear_user_states(fpu);
+ 		return 0;
+ 	}
+ 
+@@ -416,7 +416,7 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 
+ err_out:
+ 	if (ret)
+-		fpu__clear(fpu);
++		fpu__clear_user_states(fpu);
+ 	return ret;
+ }
+ 
+diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
+index 35638f1c5791..ce6cd220f722 100644
+--- a/arch/x86/kernel/process.c
++++ b/arch/x86/kernel/process.c
+@@ -191,7 +191,7 @@ void flush_thread(void)
+ 	flush_ptrace_hw_breakpoint(tsk);
+ 	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));
+ 
+-	fpu__clear(&tsk->thread.fpu);
++	fpu__clear_all(&tsk->thread.fpu);
+ }
+ 
+ void disable_TSC(void)
+diff --git a/arch/x86/kernel/signal.c b/arch/x86/kernel/signal.c
+index 83b74fb38c8f..0052bbe5dfd4 100644
+--- a/arch/x86/kernel/signal.c
++++ b/arch/x86/kernel/signal.c
+@@ -732,7 +732,7 @@ handle_signal(struct ksignal *ksig, struct pt_regs *regs)
+ 		/*
+ 		 * Ensure the signal handler starts with the new fpu state.
+ 		 */
+-		fpu__clear(fpu);
++		fpu__clear_user_states(fpu);
+ 	}
+ 	signal_setup_done(failed, ksig, stepping);
+ }
+-- 
+2.26.2
+
diff --git a/0006-x86-fpu-xstate-Update-sanitize_restored_xstate-for-s.patch b/0006-x86-fpu-xstate-Update-sanitize_restored_xstate-for-s.patch
new file mode 100644
index 000000000..bae40cca0
--- /dev/null
+++ b/0006-x86-fpu-xstate-Update-sanitize_restored_xstate-for-s.patch
@@ -0,0 +1,138 @@
+From bda901519751a4765e8546939ff198b8535dee12 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 12 Nov 2019 09:09:10 -0800
+Subject: [PATCH 06/47] x86/fpu/xstate: Update sanitize_restored_xstate() for
+ supervisor xstates
+
+The function sanitize_restored_xstate() sanitizes user xstates of an XSAVE
+buffer by clearing bits not in the input 'xfeatures' from the buffer's
+header->xfeatures, effectively resetting those features back to the init
+state.
+
+When supervisor xstates are introduced, it is necessary to make sure only
+user xstates are sanitized.  Ensure supervisor bits in header->xfeatures
+stay set and supervisor states are not modified.
+
+To make names clear, also:
+
+- Rename the function to sanitize_restored_user_xstate().
+- Rename input parameter 'xfeatures' to 'user_xfeatures'.
+- In __fpu__restore_sig(), rename 'xfeatures' to 'user_xfeatures'.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
+
+v3:
+- Change xfeatures_user to user_xfeatures.
+---
+ arch/x86/kernel/fpu/signal.c | 37 +++++++++++++++++++++++-------------
+ 1 file changed, 24 insertions(+), 13 deletions(-)
+
+diff --git a/arch/x86/kernel/fpu/signal.c b/arch/x86/kernel/fpu/signal.c
+index cd6eafba12da..40583487883e 100644
+--- a/arch/x86/kernel/fpu/signal.c
++++ b/arch/x86/kernel/fpu/signal.c
+@@ -211,9 +211,9 @@ int copy_fpstate_to_sigframe(void __user *buf, void __user *buf_fx, int size)
+ }
+ 
+ static inline void
+-sanitize_restored_xstate(union fpregs_state *state,
+-			 struct user_i387_ia32_struct *ia32_env,
+-			 u64 xfeatures, int fx_only)
++sanitize_restored_user_xstate(union fpregs_state *state,
++			      struct user_i387_ia32_struct *ia32_env,
++			      u64 user_xfeatures, int fx_only)
+ {
+ 	struct xregs_state *xsave = &state->xsave;
+ 	struct xstate_header *header = &xsave->header;
+@@ -226,13 +226,22 @@ sanitize_restored_xstate(union fpregs_state *state,
+ 		 */
+ 
+ 		/*
+-		 * Init the state that is not present in the memory
+-		 * layout and not enabled by the OS.
++		 * 'user_xfeatures' might have bits clear which are
++		 * set in header->xfeatures. This represents features that
++		 * were in init state prior to a signal delivery, and need
++		 * to be reset back to the init state.  Clear any user
++		 * feature bits which are set in the kernel buffer to get
++		 * them back to the init state.
++		 *
++		 * Supervisor state is unchanged by input from userspace.
++		 * Ensure supervisor state bits stay set and supervisor
++		 * state is not modified.
+ 		 */
+ 		if (fx_only)
+ 			header->xfeatures = XFEATURE_MASK_FPSSE;
+ 		else
+-			header->xfeatures &= xfeatures;
++			header->xfeatures &= user_xfeatures |
++					     xfeatures_mask_supervisor();
+ 	}
+ 
+ 	if (use_fxsr()) {
+@@ -281,7 +290,7 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 	struct task_struct *tsk = current;
+ 	struct fpu *fpu = &tsk->thread.fpu;
+ 	struct user_i387_ia32_struct env;
+-	u64 xfeatures = 0;
++	u64 user_xfeatures = 0;
+ 	int fx_only = 0;
+ 	int ret = 0;
+ 
+@@ -314,7 +323,7 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 			trace_x86_fpu_xstate_check_failed(fpu);
+ 		} else {
+ 			state_size = fx_sw_user.xstate_size;
+-			xfeatures = fx_sw_user.xfeatures;
++			user_xfeatures = fx_sw_user.xfeatures;
+ 		}
+ 	}
+ 
+@@ -349,7 +358,7 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 		 */
+ 		fpregs_lock();
+ 		pagefault_disable();
+-		ret = copy_user_to_fpregs_zeroing(buf_fx, xfeatures, fx_only);
++		ret = copy_user_to_fpregs_zeroing(buf_fx, user_xfeatures, fx_only);
+ 		pagefault_enable();
+ 		if (!ret) {
+ 			fpregs_mark_activate();
+@@ -362,7 +371,7 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 
+ 
+ 	if (use_xsave() && !fx_only) {
+-		u64 init_bv = xfeatures_mask_user() & ~xfeatures;
++		u64 init_bv = xfeatures_mask_user() & ~user_xfeatures;
+ 
+ 		if (using_compacted_format()) {
+ 			ret = copy_user_to_xstate(&fpu->state.xsave, buf_fx);
+@@ -375,12 +384,13 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 		if (ret)
+ 			goto err_out;
+ 
+-		sanitize_restored_xstate(&fpu->state, envp, xfeatures, fx_only);
++		sanitize_restored_user_xstate(&fpu->state, envp, user_xfeatures,
++					      fx_only);
+ 
+ 		fpregs_lock();
+ 		if (unlikely(init_bv))
+ 			copy_kernel_to_xregs(&init_fpstate.xsave, init_bv);
+-		ret = copy_kernel_to_xregs_err(&fpu->state.xsave, xfeatures);
++		ret = copy_kernel_to_xregs_err(&fpu->state.xsave, user_xfeatures);
+ 
+ 	} else if (use_fxsr()) {
+ 		ret = __copy_from_user(&fpu->state.fxsave, buf_fx, state_size);
+@@ -389,7 +399,8 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 			goto err_out;
+ 		}
+ 
+-		sanitize_restored_xstate(&fpu->state, envp, xfeatures, fx_only);
++		sanitize_restored_user_xstate(&fpu->state, envp,
++					      user_xfeatures, fx_only);
+ 
+ 		fpregs_lock();
+ 		if (use_xsave()) {
+-- 
+2.26.2
+
diff --git a/0007-x86-fpu-xstate-Update-copy_kernel_to_xregs_err-for-X.patch b/0007-x86-fpu-xstate-Update-copy_kernel_to_xregs_err-for-X.patch
new file mode 100644
index 000000000..fe1140c15
--- /dev/null
+++ b/0007-x86-fpu-xstate-Update-copy_kernel_to_xregs_err-for-X.patch
@@ -0,0 +1,46 @@
+From a6d2ac13fc0efe552faf2f761abff8b1e21d9b31 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 29 Oct 2019 12:42:06 -0700
+Subject: [PATCH 07/47] x86/fpu/xstate: Update copy_kernel_to_xregs_err() for
+ XSAVES supervisor states
+
+The function copy_kernel_to_xregs_err() uses XRSTOR, which can work with
+standard or compacted format without supervisor xstates.  However, when
+supervisor xstates are present, XRSTORS must be used.  Fix it by using
+XRSTORS when XSAVES is enabled.
+
+I also considered if there were additional cases where XRSTOR might be
+mistakenly called instead of XRSTORS.  There are only three XRSTOR sites
+in kernel:
+
+1. copy_kernel_to_xregs_booting(), already switches between XRSTOR and
+   XRSTORS based on X86_FEATURE_XSAVES.
+2. copy_user_to_xregs(), which *needs* XRSTOR because it is copying from
+   userspace and must never copy supervisor state with XRSTORS.
+3. copy_kernel_to_xregs_err() mistakenly used XRSTOR only.  Fixed it.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
+---
+ arch/x86/include/asm/fpu/internal.h | 5 ++++-
+ 1 file changed, 4 insertions(+), 1 deletion(-)
+
+diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
+index a42fcb4b690d..42159f45bf9c 100644
+--- a/arch/x86/include/asm/fpu/internal.h
++++ b/arch/x86/include/asm/fpu/internal.h
+@@ -400,7 +400,10 @@ static inline int copy_kernel_to_xregs_err(struct xregs_state *xstate, u64 mask)
+ 	u32 hmask = mask >> 32;
+ 	int err;
+ 
+-	XSTATE_OP(XRSTOR, xstate, lmask, hmask, err);
++	if (static_cpu_has(X86_FEATURE_XSAVES))
++		XSTATE_OP(XRSTORS, xstate, lmask, hmask, err);
++	else
++		XSTATE_OP(XRSTOR, xstate, lmask, hmask, err);
+ 
+ 	return err;
+ }
+-- 
+2.26.2
+
diff --git a/0008-x86-fpu-Introduce-copy_supervisor_to_kernel.patch b/0008-x86-fpu-Introduce-copy_supervisor_to_kernel.patch
new file mode 100644
index 000000000..99adf031e
--- /dev/null
+++ b/0008-x86-fpu-Introduce-copy_supervisor_to_kernel.patch
@@ -0,0 +1,154 @@
+From cb17dcf151aab3c650d7cb81715d5c5e99041fb7 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 16 Mar 2020 10:29:12 -0700
+Subject: [PATCH 08/47] x86/fpu: Introduce copy_supervisor_to_kernel()
+
+The XSAVES instruction takes a mask and saves only the features specified
+in that mask.  The kernel normally specifies that all features be saved.
+
+XSAVES also unconditionally uses the "compacted format" which means that
+all specified features are saved next to each other in memory.  If a
+feature is removed from the mask, all the features after it will "move
+up" into earlier locations in the buffer.
+
+Introduce copy_supervisor_to_kernel(), which saves only supervisor states
+and then moves those states into the standard location where they are
+normally found.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/include/asm/fpu/xstate.h |  1 +
+ arch/x86/kernel/fpu/xstate.c      | 84 +++++++++++++++++++++++++++++++
+ 2 files changed, 85 insertions(+)
+
+diff --git a/arch/x86/include/asm/fpu/xstate.h b/arch/x86/include/asm/fpu/xstate.h
+index 92104b298d77..422d8369012a 100644
+--- a/arch/x86/include/asm/fpu/xstate.h
++++ b/arch/x86/include/asm/fpu/xstate.h
+@@ -75,6 +75,7 @@ int copy_xstate_to_kernel(void *kbuf, struct xregs_state *xsave, unsigned int of
+ int copy_xstate_to_user(void __user *ubuf, struct xregs_state *xsave, unsigned int offset, unsigned int size);
+ int copy_kernel_to_xstate(struct xregs_state *xsave, const void *kbuf);
+ int copy_user_to_xstate(struct xregs_state *xsave, const void __user *ubuf);
++void copy_supervisor_to_kernel(struct xregs_state *xsave);
+ 
+ /* Validate an xstate header supplied by userspace (ptrace or sigreturn) */
+ int validate_user_xstate_header(const struct xstate_header *hdr);
+diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c
+index 95f09c2e8cd6..bda2e5eaca0e 100644
+--- a/arch/x86/kernel/fpu/xstate.c
++++ b/arch/x86/kernel/fpu/xstate.c
+@@ -62,6 +62,7 @@ u64 xfeatures_mask_all __read_mostly;
+ static unsigned int xstate_offsets[XFEATURE_MAX] = { [ 0 ... XFEATURE_MAX - 1] = -1};
+ static unsigned int xstate_sizes[XFEATURE_MAX]   = { [ 0 ... XFEATURE_MAX - 1] = -1};
+ static unsigned int xstate_comp_offsets[XFEATURE_MAX] = { [ 0 ... XFEATURE_MAX - 1] = -1};
++static unsigned int xstate_supervisor_only_offsets[XFEATURE_MAX] = { [ 0 ... XFEATURE_MAX - 1] = -1};
+ 
+ /*
+  * The XSAVE area of kernel can be in standard or compacted format;
+@@ -392,6 +393,33 @@ static void __init setup_xstate_comp_offsets(void)
+ 	}
+ }
+ 
++/*
++ * Setup offsets of a supervisor-state-only XSAVES buffer:
++ *
++ * The offsets stored in xstate_comp_offsets[] only work for one specific
++ * value of the Requested Feature BitMap (RFBM).  In cases where a different
++ * RFBM value is used, a different set of offsets is required.  This set of
++ * offsets is for when RFBM=xfeatures_mask_supervisor().
++ */
++static void __init setup_supervisor_only_offsets(void)
++{
++	unsigned int next_offset;
++	int i;
++
++	next_offset = FXSAVE_SIZE + XSAVE_HDR_SIZE;
++
++	for (i = FIRST_EXTENDED_XFEATURE; i < XFEATURE_MAX; i++) {
++		if (!xfeature_enabled(i) || !xfeature_is_supervisor(i))
++			continue;
++
++		if (xfeature_is_aligned(i))
++			next_offset = ALIGN(next_offset, 64);
++
++		xstate_supervisor_only_offsets[i] = next_offset;
++		next_offset += xstate_sizes[i];
++	}
++}
++
+ /*
+  * Print out xstate component offsets and sizes
+  */
+@@ -790,6 +818,7 @@ void __init fpu__init_system_xstate(void)
+ 	fpu__init_prepare_fx_sw_frame();
+ 	setup_init_fpu_buf();
+ 	setup_xstate_comp_offsets();
++	setup_supervisor_only_offsets();
+ 	print_xstate_offset_size();
+ 
+ 	pr_info("x86/fpu: Enabled xstate features 0x%llx, context size is %d bytes, using '%s' format.\n",
+@@ -1272,6 +1301,61 @@ int copy_user_to_xstate(struct xregs_state *xsave, const void __user *ubuf)
+ 	return 0;
+ }
+ 
++/*
++ * Save only supervisor states to the kernel buffer.  This blows away all
++ * old states, and is intended to be used only in __fpu__restore_sig(), where
++ * user states are restored from the user buffer.
++ */
++void copy_supervisor_to_kernel(struct xregs_state *xstate)
++{
++	struct xstate_header *header;
++	u64 max_bit, min_bit;
++	u32 lmask, hmask;
++	int err, i;
++
++	if (WARN_ON(!boot_cpu_has(X86_FEATURE_XSAVES)))
++		return;
++
++	if (!xfeatures_mask_supervisor())
++		return;
++
++	max_bit = __fls(xfeatures_mask_supervisor());
++	min_bit = __ffs(xfeatures_mask_supervisor());
++
++	lmask = xfeatures_mask_supervisor();
++	hmask = xfeatures_mask_supervisor() >> 32;
++	XSTATE_OP(XSAVES, xstate, lmask, hmask, err);
++
++	/* We should never fault when copying to a kernel buffer: */
++	if (WARN_ON_FPU(err))
++		return;
++
++	/*
++	 * At this point, the buffer has only supervisor states and must be
++	 * converted back to normal kernel format.
++	 */
++	header = &xstate->header;
++	header->xcomp_bv |= xfeatures_mask_all;
++
++	/*
++	 * This only moves states up in the buffer.  Start with
++	 * the last state and move backwards so that states are
++	 * not overwritten until after they are moved.  Note:
++	 * memmove() allows overlapping src/dst buffers.
++	 */
++	for (i = max_bit; i >= min_bit; i--) {
++		u8 *xbuf = (u8 *)xstate;
++
++		if (!((header->xfeatures >> i) & 1))
++			continue;
++
++		/* Move xfeature 'i' into its normal location */
++		memmove(xbuf + xstate_comp_offsets[i],
++			xbuf + xstate_supervisor_only_offsets[i],
++			xstate_sizes[i]);
++	}
++}
++
+ #ifdef CONFIG_PROC_PID_ARCH_STATUS
+ /*
+  * Report the amount of time elapsed in millisecond since last AVX512
+-- 
+2.26.2
+
diff --git a/0009-x86-fpu-xstate-Preserve-supervisor-states-for-slow-p.patch b/0009-x86-fpu-xstate-Preserve-supervisor-states-for-slow-p.patch
new file mode 100644
index 000000000..c3b2b0702
--- /dev/null
+++ b/0009-x86-fpu-xstate-Preserve-supervisor-states-for-slow-p.patch
@@ -0,0 +1,130 @@
+From 2b149ef4cd5acfe5112c9d10b2144c0b99fcfd5d Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 29 Oct 2019 13:03:10 -0700
+Subject: [PATCH 09/47] x86/fpu/xstate: Preserve supervisor states for slow
+ path of __fpu__restore_sig()
+
+The signal return code is responsible for taking an XSAVE buffer present
+in user memory and loading it into the hardware registers.  This
+operation only affects user XSAVE state and never affects supervisor state.
+
+The fast path through this code simply points XRSTOR directly at the
+user buffer.  However, since user memory is not guaranteed to be always
+mapped, this XRSTOR can fail.  If it fails, the signal return code falls
+back to a slow path which can tolerate page faults.
+
+That slow path copies the xfeatures one by one out of the user buffer
+into the task's fpu state area.  However, by being in a context where it
+can handle page faults, the code can also schedule.  The lazy-fpu-load code
+would think it has an up-to-date fpstate and would fail to save the
+supervisor state when scheduling the task out.  When scheduling back in, it
+would likely restore stale supervisor state.
+
+To fix that, preserve supervisor state before the slow path.  Modify
+copy_user_to_fpregs_zeroing() so that if it fails, fpregs are not zeroed,
+and there is no need for fpregs_deactivate() and supervisor states are
+preserved.
+
+Move set_thread_flag(TIF_NEED_FPU_LOAD) to the slow path.  Without doing
+this, the fast path also needs supervisor states to be saved first.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/kernel/fpu/signal.c | 53 +++++++++++++++++++-----------------
+ 1 file changed, 28 insertions(+), 25 deletions(-)
+
+diff --git a/arch/x86/kernel/fpu/signal.c b/arch/x86/kernel/fpu/signal.c
+index 40583487883e..c0e07b548076 100644
+--- a/arch/x86/kernel/fpu/signal.c
++++ b/arch/x86/kernel/fpu/signal.c
+@@ -262,19 +262,23 @@ sanitize_restored_user_xstate(union fpregs_state *state,
+ static int copy_user_to_fpregs_zeroing(void __user *buf, u64 xbv, int fx_only)
+ {
+ 	u64 init_bv;
++	int r;
+ 
+ 	if (use_xsave()) {
+ 		if (fx_only) {
+ 			init_bv = xfeatures_mask_user() & ~XFEATURE_MASK_FPSSE;
+ 
+-			copy_kernel_to_xregs(&init_fpstate.xsave, init_bv);
+-			return copy_user_to_fxregs(buf);
++			r = copy_user_to_fxregs(buf);
++			if (!r)
++				copy_kernel_to_xregs(&init_fpstate.xsave, init_bv);
++			return r;
+ 		} else {
+ 			init_bv = xfeatures_mask_user() & ~xbv;
+ 
+-			if (unlikely(init_bv))
++			r = copy_user_to_xregs(buf, xbv);
++			if (!r && unlikely(init_bv))
+ 				copy_kernel_to_xregs(&init_fpstate.xsave, init_bv);
+-			return copy_user_to_xregs(buf, xbv);
++			return r;
+ 		}
+ 	} else if (use_fxsr()) {
+ 		return copy_user_to_fxregs(buf);
+@@ -327,28 +331,10 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 		}
+ 	}
+ 
+-	/*
+-	 * The current state of the FPU registers does not matter. By setting
+-	 * TIF_NEED_FPU_LOAD unconditionally it is ensured that the our xstate
+-	 * is not modified on context switch and that the xstate is considered
+-	 * to be loaded again on return to userland (overriding last_cpu avoids
+-	 * the optimisation).
+-	 */
+-	set_thread_flag(TIF_NEED_FPU_LOAD);
+-	__fpu_invalidate_fpregs_state(fpu);
+-
+ 	if ((unsigned long)buf_fx % 64)
+ 		fx_only = 1;
+-	/*
+-	 * For 32-bit frames with fxstate, copy the fxstate so it can be
+-	 * reconstructed later.
+-	 */
+-	if (ia32_fxstate) {
+-		ret = __copy_from_user(&env, buf, sizeof(env));
+-		if (ret)
+-			goto err_out;
+-		envp = &env;
+-	} else {
++
++	if (!ia32_fxstate) {
+ 		/*
+ 		 * Attempt to restore the FPU registers directly from user
+ 		 * memory. For that to succeed, the user access cannot cause
+@@ -365,10 +351,27 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 			fpregs_unlock();
+ 			return 0;
+ 		}
+-		fpregs_deactivate(fpu);
+ 		fpregs_unlock();
++	} else {
++		/*
++		 * For 32-bit frames with fxstate, copy the fxstate so it can
++		 * be reconstructed later.
++		 */
++		ret = __copy_from_user(&env, buf, sizeof(env));
++		if (ret)
++			goto err_out;
++		envp = &env;
+ 	}
+ 
++	/*
++	 * The current state of the FPU registers does not matter. By setting
++	 * TIF_NEED_FPU_LOAD unconditionally it is ensured that the our xstate
++	 * is not modified on context switch and that the xstate is considered
++	 * to be loaded again on return to userland (overriding last_cpu avoids
++	 * the optimisation).
++	 */
++	set_thread_flag(TIF_NEED_FPU_LOAD);
++	__fpu_invalidate_fpregs_state(fpu);
+ 
+ 	if (use_xsave() && !fx_only) {
+ 		u64 init_bv = xfeatures_mask_user() & ~user_xfeatures;
+-- 
+2.26.2
+
diff --git a/0010-x86-fpu-xstate-Restore-supervisor-states-for-signal-.patch b/0010-x86-fpu-xstate-Restore-supervisor-states-for-signal-.patch
new file mode 100644
index 000000000..4e437bc0f
--- /dev/null
+++ b/0010-x86-fpu-xstate-Restore-supervisor-states-for-signal-.patch
@@ -0,0 +1,129 @@
+From 299a4d2e1c53addbf2169a9cb88d28069d0aa58e Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Fri, 27 Mar 2020 09:42:02 -0700
+Subject: [PATCH 10/47] x86/fpu/xstate: Restore supervisor states for signal
+ return
+
+As described in the previous patch, the signal return fast path directly
+restores user states from the user buffer.  Once that succeeds, restore
+supervisor states (but only when they are not yet restored).
+
+For the slow path, save supervisor states to preserve them across context
+switches, and restore after the user states are restored.
+
+The previous version has the overhead of an XSAVES in both the fast and the
+slow paths.  It is addressed as the following:
+
+- In the fast path, only do an XRSTORS.
+- In the slow path, do a supervisor-state-only XSAVES, and relocate the
+  buffer contents.
+
+Some thoughts in the implementation:
+
+- In the slow path, can any supervisor state become stale between
+  save/restore?
+
+  Answer: set_thread_flag(TIF_NEED_FPU_LOAD) protects the xstate buffer.
+
+- In the slow path, can any code reference a stale supervisor state
+  register between save/restore?
+
+  Answer: In the current lazy-restore scheme, any reference to xstate
+  registers needs fpregs_lock()/fpregs_unlock() and __fpregs_load_activate().
+
+- Are there other options?
+
+  One other option is eagerly restoring all supervisor states.
+
+  Currently, CET user-mode states and ENQCMD's PASID do not need to be
+  eagerly restored.  The upcoming CET kernel-mode states (24 bytes) need
+  to be eagerly restored.  To me, eagerly restoring all supervisor states
+  adds more overhead then benefit at this point.
+
+v3:
+- Change copy_xregs_to_kernel() to copy_supervisor_to_kernel(), which is
+  introduced in a previous patch.
+- Update commit log.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
+---
+ arch/x86/kernel/fpu/signal.c | 44 ++++++++++++++++++++++++++++++++----
+ 1 file changed, 39 insertions(+), 5 deletions(-)
+
+diff --git a/arch/x86/kernel/fpu/signal.c b/arch/x86/kernel/fpu/signal.c
+index c0e07b548076..003735eec674 100644
+--- a/arch/x86/kernel/fpu/signal.c
++++ b/arch/x86/kernel/fpu/signal.c
+@@ -347,6 +347,23 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 		ret = copy_user_to_fpregs_zeroing(buf_fx, user_xfeatures, fx_only);
+ 		pagefault_enable();
+ 		if (!ret) {
++
++			/*
++			 * Restore supervisor states: previous context switch
++			 * etc has done XSAVES and saved the supervisor states
++			 * in the kernel buffer from which they can be restored
++			 * now.
++			 *
++			 * We cannot do a single XRSTORS here - which would
++			 * be nice - because the rest of the FPU registers are
++			 * being restored from a user buffer directly. The
++			 * single XRSTORS happens below, when the user buffer
++			 * has been copied to the kernel one.
++			 */
++			if (test_thread_flag(TIF_NEED_FPU_LOAD) &&
++			    xfeatures_mask_supervisor())
++				copy_kernel_to_xregs(&fpu->state.xsave,
++						     xfeatures_mask_supervisor());
+ 			fpregs_mark_activate();
+ 			fpregs_unlock();
+ 			return 0;
+@@ -364,14 +381,25 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 	}
+ 
+ 	/*
+-	 * The current state of the FPU registers does not matter. By setting
+-	 * TIF_NEED_FPU_LOAD unconditionally it is ensured that the our xstate
+-	 * is not modified on context switch and that the xstate is considered
++	 * By setting TIF_NEED_FPU_LOAD it is ensured that our xstate is
++	 * not modified on context switch and that the xstate is considered
+ 	 * to be loaded again on return to userland (overriding last_cpu avoids
+ 	 * the optimisation).
+ 	 */
+-	set_thread_flag(TIF_NEED_FPU_LOAD);
++	fpregs_lock();
++
++	if (!test_thread_flag(TIF_NEED_FPU_LOAD)) {
++
++		/*
++		 * Supervisor states are not modified by user space input.  Save
++		 * current supervisor states first and invalidate the FPU regs.
++		 */
++		if (xfeatures_mask_supervisor())
++			copy_supervisor_to_kernel(&fpu->state.xsave);
++		set_thread_flag(TIF_NEED_FPU_LOAD);
++	}
+ 	__fpu_invalidate_fpregs_state(fpu);
++	fpregs_unlock();
+ 
+ 	if (use_xsave() && !fx_only) {
+ 		u64 init_bv = xfeatures_mask_user() & ~user_xfeatures;
+@@ -393,7 +421,13 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 		fpregs_lock();
+ 		if (unlikely(init_bv))
+ 			copy_kernel_to_xregs(&init_fpstate.xsave, init_bv);
+-		ret = copy_kernel_to_xregs_err(&fpu->state.xsave, user_xfeatures);
++
++		/*
++		 * Restore previously saved supervisor xstates along with
++		 * copied-in user xstates.
++		 */
++		ret = copy_kernel_to_xregs_err(&fpu->state.xsave,
++					       user_xfeatures | xfeatures_mask_supervisor());
+ 
+ 	} else if (use_fxsr()) {
+ 		ret = __copy_from_user(&fpu->state.fxsave, buf_fx, state_size);
+-- 
+2.26.2
+
diff --git a/0011-Documentation-x86-Add-CET-description.patch b/0011-Documentation-x86-Add-CET-description.patch
new file mode 100644
index 000000000..20481249e
--- /dev/null
+++ b/0011-Documentation-x86-Add-CET-description.patch
@@ -0,0 +1,194 @@
+From 2f2184cda3322b9a1a23d88a5bcc99f26410041f Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Sun, 17 Dec 2017 09:09:23 -0800
+Subject: [PATCH 11/47] Documentation/x86: Add CET description
+
+Explain no_user_shstk/no_user_ibt kernel parameters, and introduce a new
+document on Control-flow Enforcement Technology (CET).
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+
+v10:
+- Change no_cet_shstk and no_cet_ibt to no_user_shstk and no_user_ibt.
+- Remove the opcode section, as it is already in the Intel SDM.
+- Remove sections related to GLIBC implementation.
+- Remove shadow stack memory management section, as it is already in the
+  code comments.
+- Remove legacy bitmap related information, as it is not supported now.
+- Fix arch_ioctl() related text.
+- Change SHSTK, IBT to plain English.
+---
+ .../admin-guide/kernel-parameters.txt         |   6 +
+ Documentation/x86/index.rst                   |   1 +
+ Documentation/x86/intel_cet.rst               | 129 ++++++++++++++++++
+ 3 files changed, 136 insertions(+)
+ create mode 100644 Documentation/x86/intel_cet.rst
+
+diff --git a/Documentation/admin-guide/kernel-parameters.txt b/Documentation/admin-guide/kernel-parameters.txt
+index 7bc83f3d9bdf..be715675df6d 100644
+--- a/Documentation/admin-guide/kernel-parameters.txt
++++ b/Documentation/admin-guide/kernel-parameters.txt
+@@ -3093,6 +3093,12 @@
+ 			noexec=on: enable non-executable mappings (default)
+ 			noexec=off: disable non-executable mappings
+ 
++	no_user_shstk	[X86-64] Disable Shadow Stack for user-mode
++			applications
++
++	no_user_ibt	[X86-64] Disable Indirect Branch Tracking for user-mode
++			applications
++
+ 	nosmap		[X86,PPC]
+ 			Disable SMAP (Supervisor Mode Access Prevention)
+ 			even if it is supported by processor.
+diff --git a/Documentation/x86/index.rst b/Documentation/x86/index.rst
+index 265d9e9a093b..2aef972a868d 100644
+--- a/Documentation/x86/index.rst
++++ b/Documentation/x86/index.rst
+@@ -19,6 +19,7 @@ x86-specific Documentation
+    tlb
+    mtrr
+    pat
++   intel_cet
+    intel-iommu
+    intel_txt
+    amd-memory-encryption
+diff --git a/Documentation/x86/intel_cet.rst b/Documentation/x86/intel_cet.rst
+new file mode 100644
+index 000000000000..746eda8c82f3
+--- /dev/null
++++ b/Documentation/x86/intel_cet.rst
+@@ -0,0 +1,129 @@
++.. SPDX-License-Identifier: GPL-2.0
++
++=========================================
++Control-flow Enforcement Technology (CET)
++=========================================
++
++[1] Overview
++============
++
++Control-flow Enforcement Technology (CET) is an Intel processor feature
++that provides protection against return/jump-oriented programming (ROP)
++attacks.  It can be set up to protect both applications and the kernel.
++Only user-mode protection is implemented in the 64-bit kernel, including
++support for running legacy 32-bit applications.
++
++CET introduces Shadow Stack and Indirect Branch Tracking.  Shadow stack is
++a secondary stack allocated from memory and cannot be directly modified by
++applications.  When executing a CALL, the processor pushes the return
++address to both the normal stack and the shadow stack.  Upon function
++return, the processor pops the shadow stack copy and compares it to the
++normal stack copy.  If the two differ, the processor raises a control-
++protection fault.  Indirect branch tracking verifies indirect CALL/JMP
++targets are intended as marked by the compiler with 'ENDBR' opcodes.
++
++There are two kernel configuration options:
++
++    X86_INTEL_SHADOW_STACK_USER, and
++    X86_INTEL_BRANCH_TRACKING_USER.
++
++These need to be enabled to build a CET-enabled kernel, and Binutils v2.31
++and GCC v8.1 or later are required to build a CET kernel.  To build a CET-
++enabled application, GLIBC v2.28 or later is also required.
++
++There are two command-line options for disabling CET features::
++
++    no_user_shstk - disables user shadow stack, and
++    no_user_ibt   - disables user indirect branch tracking.
++
++At run time, /proc/cpuinfo shows CET features if the processor supports
++CET.
++
++[2] Application Enabling
++========================
++
++An application's CET capability is marked in its ELF header and can be
++verified from the following command output, in the NT_GNU_PROPERTY_TYPE_0
++field:
++
++    readelf -n <application>
++
++If an application supports CET and is statically linked, it will run with
++CET protection.  If the application needs any shared libraries, the loader
++checks all dependencies and enables CET when all requirements are met.
++
++[3] CET arch_prctl()'s
++======================
++
++Several arch_prctl()'s have been added for CET:
++
++arch_prctl(ARCH_X86_CET_STATUS, u64 *addr)
++    Return CET feature status.
++
++    The parameter 'addr' is a pointer to a user buffer.
++    On returning to the caller, the kernel fills the following
++    information::
++
++        *addr       = shadow stack/indirect branch tracking status
++        *(addr + 1) = shadow stack base address
++        *(addr + 2) = shadow stack size
++
++arch_prctl(ARCH_X86_CET_DISABLE, u64 features)
++    Disable shadow stack and/or indirect branch tracking as specified in
++    'features'.  Return -EPERM if CET is locked.
++
++arch_prctl(ARCH_X86_CET_LOCK)
++    Lock in all CET features.  They cannot be turned off afterwards.
++
++arch_prctl(ARCH_X86_CET_ALLOC_SHSTK, u64 *addr)
++    Allocate a new shadow stack and put a restore token at top.
++
++    The parameter 'addr' is a pointer to a user buffer and indicates the
++    shadow stack size to allocate.  On returning to the caller, the kernel
++    fills '*addr' with the base address of the new shadow stack.
++
++    User-level threads that need a new stack are expected to allocate a
++    new shadow stack.
++
++Note:
++  There is no CET-enabling arch_prctl function.  By design, CET is enabled
++  automatically if the binary and the system can support it.
++
++[4] The implementation of the Shadow Stack
++==========================================
++
++Shadow Stack size
++-----------------
++
++A task's shadow stack is allocated from memory to a fixed size of
++MIN(RLIMIT_STACK, 4 GB).  In other words, the shadow stack is allocated to
++the maximum size of the normal stack, but capped to 4 GB.  However,
++a compat-mode application's address space is smaller, each of its thread's
++shadow stack size is MIN(1/4 RLIMIT_STACK, 4 GB).
++
++Signal
++------
++
++The main program and its signal handlers use the same shadow stack.
++Because the shadow stack stores only return addresses, a large shadow
++stack covers the condition that both the program stack and the signal
++alternate stack run out.
++
++The kernel creates a restore token for the shadow stack restoring address
++and verifies that token when restoring from the signal handler.
++
++Fork
++----
++
++The shadow stack's vma has VM_SHSTK flag set; its PTEs are required to be
++read-only and dirty.  When a shadow stack PTE is not RO and dirty, a
++shadow access triggers a page fault with the shadow stack access bit set
++in the page fault error code.
++
++When a task forks a child, its shadow stack PTEs are copied and both the
++parent's and the child's shadow stack PTEs are cleared of the dirty bit.
++Upon the next shadow stack access, the resulting shadow stack page fault
++is handled by page copy/re-use.
++
++When a pthread child is created, the kernel allocates a new shadow stack
++for the new thread.
+-- 
+2.26.2
+
diff --git a/0012-x86-cpufeatures-Add-CET-CPU-feature-flags-for-Contro.patch b/0012-x86-cpufeatures-Add-CET-CPU-feature-flags-for-Contro.patch
new file mode 100644
index 000000000..a6c9285a1
--- /dev/null
+++ b/0012-x86-cpufeatures-Add-CET-CPU-feature-flags-for-Contro.patch
@@ -0,0 +1,55 @@
+From eb3b52bc6441b6a8b437d9b356d00b5b767cb619 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Wed, 9 Nov 2016 16:26:37 -0800
+Subject: [PATCH 12/47] x86/cpufeatures: Add CET CPU feature flags for
+ Control-flow Enforcement Technology (CET)
+
+Add CPU feature flags for Control-flow Enforcement Technology (CET).
+
+CPUID.(EAX=7,ECX=0):ECX[bit 7] Shadow stack
+CPUID.(EAX=7,ECX=0):EDX[bit 20] Indirect Branch Tracking
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Borislav Petkov <bp@suse.de>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+---
+ arch/x86/include/asm/cpufeatures.h | 2 ++
+ arch/x86/kernel/cpu/cpuid-deps.c   | 2 ++
+ 2 files changed, 4 insertions(+)
+
+diff --git a/arch/x86/include/asm/cpufeatures.h b/arch/x86/include/asm/cpufeatures.h
+index db189945e9b0..2c95a3efc2d9 100644
+--- a/arch/x86/include/asm/cpufeatures.h
++++ b/arch/x86/include/asm/cpufeatures.h
+@@ -339,6 +339,7 @@
+ #define X86_FEATURE_OSPKE		(16*32+ 4) /* OS Protection Keys Enable */
+ #define X86_FEATURE_WAITPKG		(16*32+ 5) /* UMONITOR/UMWAIT/TPAUSE Instructions */
+ #define X86_FEATURE_AVX512_VBMI2	(16*32+ 6) /* Additional AVX512 Vector Bit Manipulation Instructions */
++#define X86_FEATURE_SHSTK		(16*32+ 7) /* Shadow Stack */
+ #define X86_FEATURE_GFNI		(16*32+ 8) /* Galois Field New Instructions */
+ #define X86_FEATURE_VAES		(16*32+ 9) /* Vector AES */
+ #define X86_FEATURE_VPCLMULQDQ		(16*32+10) /* Carry-Less Multiplication Double Quadword */
+@@ -365,6 +366,7 @@
+ #define X86_FEATURE_MD_CLEAR		(18*32+10) /* VERW clears CPU buffers */
+ #define X86_FEATURE_TSX_FORCE_ABORT	(18*32+13) /* "" TSX_FORCE_ABORT */
+ #define X86_FEATURE_PCONFIG		(18*32+18) /* Intel PCONFIG */
++#define X86_FEATURE_IBT			(18*32+20) /* Indirect Branch Tracking */
+ #define X86_FEATURE_SPEC_CTRL		(18*32+26) /* "" Speculation Control (IBRS + IBPB) */
+ #define X86_FEATURE_INTEL_STIBP		(18*32+27) /* "" Single Thread Indirect Branch Predictors */
+ #define X86_FEATURE_FLUSH_L1D		(18*32+28) /* Flush L1D cache */
+diff --git a/arch/x86/kernel/cpu/cpuid-deps.c b/arch/x86/kernel/cpu/cpuid-deps.c
+index 3cbe24ca80ab..fec83cc74b9e 100644
+--- a/arch/x86/kernel/cpu/cpuid-deps.c
++++ b/arch/x86/kernel/cpu/cpuid-deps.c
+@@ -69,6 +69,8 @@ static const struct cpuid_dep cpuid_deps[] = {
+ 	{ X86_FEATURE_CQM_MBM_TOTAL,		X86_FEATURE_CQM_LLC   },
+ 	{ X86_FEATURE_CQM_MBM_LOCAL,		X86_FEATURE_CQM_LLC   },
+ 	{ X86_FEATURE_AVX512_BF16,		X86_FEATURE_AVX512VL  },
++	{ X86_FEATURE_SHSTK,			X86_FEATURE_XSAVES    },
++	{ X86_FEATURE_IBT,			X86_FEATURE_XSAVES    },
+ 	{}
+ };
+ 
+-- 
+2.26.2
+
diff --git a/0013-x86-fpu-xstate-Introduce-CET-MSR-XSAVES-supervisor-s.patch b/0013-x86-fpu-xstate-Introduce-CET-MSR-XSAVES-supervisor-s.patch
new file mode 100644
index 000000000..4708cbbca
--- /dev/null
+++ b/0013-x86-fpu-xstate-Introduce-CET-MSR-XSAVES-supervisor-s.patch
@@ -0,0 +1,203 @@
+From 86722a77bc955478c0399d653937d522893b8991 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Thu, 10 Nov 2016 10:13:56 -0800
+Subject: [PATCH 13/47] x86/fpu/xstate: Introduce CET MSR XSAVES supervisor
+ states
+
+Control-flow Enforcement Technology (CET) adds five MSRs.  Introduce them
+and their XSAVES supervisor states:
+
+    MSR_IA32_U_CET (user-mode CET settings),
+    MSR_IA32_PL3_SSP (user-mode Shadow Stack pointer),
+    MSR_IA32_PL0_SSP (kernel-mode Shadow Stack pointer),
+    MSR_IA32_PL1_SSP (Privilege Level 1 Shadow Stack pointer),
+    MSR_IA32_PL2_SSP (Privilege Level 2 Shadow Stack pointer).
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+
+v6:
+- Remove __packed from struct cet_user_state, struct cet_kernel_state.
+---
+ arch/x86/include/asm/fpu/types.h            | 22 ++++++++++++++++++
+ arch/x86/include/asm/fpu/xstate.h           |  5 +++--
+ arch/x86/include/asm/msr-index.h            | 18 +++++++++++++++
+ arch/x86/include/uapi/asm/processor-flags.h |  2 ++
+ arch/x86/kernel/fpu/xstate.c                | 25 +++++++++++++++++++--
+ 5 files changed, 68 insertions(+), 4 deletions(-)
+
+diff --git a/arch/x86/include/asm/fpu/types.h b/arch/x86/include/asm/fpu/types.h
+index f098f6cab94b..d7ef4d9c7ad5 100644
+--- a/arch/x86/include/asm/fpu/types.h
++++ b/arch/x86/include/asm/fpu/types.h
+@@ -114,6 +114,9 @@ enum xfeature {
+ 	XFEATURE_Hi16_ZMM,
+ 	XFEATURE_PT_UNIMPLEMENTED_SO_FAR,
+ 	XFEATURE_PKRU,
++	XFEATURE_RESERVED,
++	XFEATURE_CET_USER,
++	XFEATURE_CET_KERNEL,
+ 
+ 	XFEATURE_MAX,
+ };
+@@ -128,6 +131,8 @@ enum xfeature {
+ #define XFEATURE_MASK_Hi16_ZMM		(1 << XFEATURE_Hi16_ZMM)
+ #define XFEATURE_MASK_PT		(1 << XFEATURE_PT_UNIMPLEMENTED_SO_FAR)
+ #define XFEATURE_MASK_PKRU		(1 << XFEATURE_PKRU)
++#define XFEATURE_MASK_CET_USER		(1 << XFEATURE_CET_USER)
++#define XFEATURE_MASK_CET_KERNEL	(1 << XFEATURE_CET_KERNEL)
+ 
+ #define XFEATURE_MASK_FPSSE		(XFEATURE_MASK_FP | XFEATURE_MASK_SSE)
+ #define XFEATURE_MASK_AVX512		(XFEATURE_MASK_OPMASK \
+@@ -229,6 +234,23 @@ struct pkru_state {
+ 	u32				pad;
+ } __packed;
+ 
++/*
++ * State component 11 is Control-flow Enforcement user states
++ */
++struct cet_user_state {
++	u64 user_cet;			/* user control-flow settings */
++	u64 user_ssp;			/* user shadow stack pointer */
++};
++
++/*
++ * State component 12 is Control-flow Enforcement kernel states
++ */
++struct cet_kernel_state {
++	u64 kernel_ssp;			/* kernel shadow stack */
++	u64 pl1_ssp;			/* privilege level 1 shadow stack */
++	u64 pl2_ssp;			/* privilege level 2 shadow stack */
++};
++
+ struct xstate_header {
+ 	u64				xfeatures;
+ 	u64				xcomp_bv;
+diff --git a/arch/x86/include/asm/fpu/xstate.h b/arch/x86/include/asm/fpu/xstate.h
+index 422d8369012a..db89d796b22e 100644
+--- a/arch/x86/include/asm/fpu/xstate.h
++++ b/arch/x86/include/asm/fpu/xstate.h
+@@ -33,13 +33,14 @@
+ 				      XFEATURE_MASK_BNDCSR)
+ 
+ /* All currently supported supervisor features */
+-#define XFEATURE_MASK_SUPERVISOR_SUPPORTED (0)
++#define XFEATURE_MASK_SUPERVISOR_SUPPORTED (XFEATURE_MASK_CET_USER)
+ 
+ /*
+  * Unsupported supervisor features. When a supervisor feature in this mask is
+  * supported in the future, move it to the supported supervisor feature mask.
+  */
+-#define XFEATURE_MASK_SUPERVISOR_UNSUPPORTED (XFEATURE_MASK_PT)
++#define XFEATURE_MASK_SUPERVISOR_UNSUPPORTED (XFEATURE_MASK_PT | \
++					      XFEATURE_MASK_CET_KERNEL)
+ 
+ /* All supervisor states including supported and unsupported states. */
+ #define XFEATURE_MASK_SUPERVISOR_ALL (XFEATURE_MASK_SUPERVISOR_SUPPORTED | \
+diff --git a/arch/x86/include/asm/msr-index.h b/arch/x86/include/asm/msr-index.h
+index 12c9684d59ba..47f603729543 100644
+--- a/arch/x86/include/asm/msr-index.h
++++ b/arch/x86/include/asm/msr-index.h
+@@ -885,4 +885,22 @@
+ #define MSR_VM_IGNNE                    0xc0010115
+ #define MSR_VM_HSAVE_PA                 0xc0010117
+ 
++/* Control-flow Enforcement Technology MSRs */
++#define MSR_IA32_U_CET		0x6a0 /* user mode cet setting */
++#define MSR_IA32_S_CET		0x6a2 /* kernel mode cet setting */
++#define MSR_IA32_PL0_SSP	0x6a4 /* kernel shstk pointer */
++#define MSR_IA32_PL1_SSP	0x6a5 /* ring-1 shstk pointer */
++#define MSR_IA32_PL2_SSP	0x6a6 /* ring-2 shstk pointer */
++#define MSR_IA32_PL3_SSP	0x6a7 /* user shstk pointer */
++#define MSR_IA32_INT_SSP_TAB	0x6a8 /* exception shstk table */
++
++/* MSR_IA32_U_CET and MSR_IA32_S_CET bits */
++#define MSR_IA32_CET_SHSTK_EN		0x0000000000000001ULL
++#define MSR_IA32_CET_WRSS_EN		0x0000000000000002ULL
++#define MSR_IA32_CET_ENDBR_EN		0x0000000000000004ULL
++#define MSR_IA32_CET_LEG_IW_EN		0x0000000000000008ULL
++#define MSR_IA32_CET_NO_TRACK_EN	0x0000000000000010ULL
++#define MSR_IA32_CET_WAIT_ENDBR	0x00000000000000800UL
++#define MSR_IA32_CET_BITMAP_MASK	0xfffffffffffff000ULL
++
+ #endif /* _ASM_X86_MSR_INDEX_H */
+diff --git a/arch/x86/include/uapi/asm/processor-flags.h b/arch/x86/include/uapi/asm/processor-flags.h
+index bcba3c643e63..a8df907e8017 100644
+--- a/arch/x86/include/uapi/asm/processor-flags.h
++++ b/arch/x86/include/uapi/asm/processor-flags.h
+@@ -130,6 +130,8 @@
+ #define X86_CR4_SMAP		_BITUL(X86_CR4_SMAP_BIT)
+ #define X86_CR4_PKE_BIT		22 /* enable Protection Keys support */
+ #define X86_CR4_PKE		_BITUL(X86_CR4_PKE_BIT)
++#define X86_CR4_CET_BIT		23 /* enable Control-flow Enforcement */
++#define X86_CR4_CET		_BITUL(X86_CR4_CET_BIT)
+ 
+ /*
+  * x86-64 Task Priority Register, CR8
+diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c
+index bda2e5eaca0e..85b7f295c399 100644
+--- a/arch/x86/kernel/fpu/xstate.c
++++ b/arch/x86/kernel/fpu/xstate.c
+@@ -38,6 +38,9 @@ static const char *xfeature_names[] =
+ 	"Processor Trace (unused)"	,
+ 	"Protection Keys User registers",
+ 	"unknown xstate feature"	,
++	"Control-flow User registers"	,
++	"Control-flow Kernel registers"	,
++	"unknown xstate feature"	,
+ };
+ 
+ static short xsave_cpuid_features[] __initdata = {
+@@ -51,6 +54,9 @@ static short xsave_cpuid_features[] __initdata = {
+ 	X86_FEATURE_AVX512F,
+ 	X86_FEATURE_INTEL_PT,
+ 	X86_FEATURE_PKU,
++	-1,		   /* Unused */
++	X86_FEATURE_SHSTK, /* XFEATURE_CET_USER */
++	X86_FEATURE_SHSTK, /* XFEATURE_CET_KERNEL */
+ };
+ 
+ /*
+@@ -316,6 +322,8 @@ static void __init print_xstate_features(void)
+ 	print_xstate_feature(XFEATURE_MASK_ZMM_Hi256);
+ 	print_xstate_feature(XFEATURE_MASK_Hi16_ZMM);
+ 	print_xstate_feature(XFEATURE_MASK_PKRU);
++	print_xstate_feature(XFEATURE_MASK_CET_USER);
++	print_xstate_feature(XFEATURE_MASK_CET_KERNEL);
+ }
+ 
+ /*
+@@ -590,6 +598,8 @@ static void check_xstate_against_struct(int nr)
+ 	XCHECK_SZ(sz, nr, XFEATURE_ZMM_Hi256, struct avx_512_zmm_uppers_state);
+ 	XCHECK_SZ(sz, nr, XFEATURE_Hi16_ZMM,  struct avx_512_hi16_state);
+ 	XCHECK_SZ(sz, nr, XFEATURE_PKRU,      struct pkru_state);
++	XCHECK_SZ(sz, nr, XFEATURE_CET_USER,   struct cet_user_state);
++	XCHECK_SZ(sz, nr, XFEATURE_CET_KERNEL, struct cet_kernel_state);
+ 
+ 	/*
+ 	 * Make *SURE* to add any feature numbers in below if
+@@ -797,8 +807,19 @@ void __init fpu__init_system_xstate(void)
+ 	 * Clear XSAVE features that are disabled in the normal CPUID.
+ 	 */
+ 	for (i = 0; i < ARRAY_SIZE(xsave_cpuid_features); i++) {
+-		if (!boot_cpu_has(xsave_cpuid_features[i]))
+-			xfeatures_mask_all &= ~BIT_ULL(i);
++		if (xsave_cpuid_features[i] == X86_FEATURE_SHSTK) {
++			/*
++			 * X86_FEATURE_SHSTK and X86_FEATURE_IBT share
++			 * same states, but can be enabled separately.
++			 */
++			if (!boot_cpu_has(X86_FEATURE_SHSTK) &&
++			    !boot_cpu_has(X86_FEATURE_IBT))
++				xfeatures_mask_all &= ~BIT_ULL(i);
++		} else {
++			if ((xsave_cpuid_features[i] == -1) ||
++			    !boot_cpu_has(xsave_cpuid_features[i]))
++				xfeatures_mask_all &= ~BIT_ULL(i);
++		}
+ 	}
+ 
+ 	xfeatures_mask_all &= fpu__get_supported_xfeatures_mask();
+-- 
+2.26.2
+
diff --git a/0014-x86-cet-Add-control-protection-fault-handler.patch b/0014-x86-cet-Add-control-protection-fault-handler.patch
new file mode 100644
index 000000000..c20b3d1b7
--- /dev/null
+++ b/0014-x86-cet-Add-control-protection-fault-handler.patch
@@ -0,0 +1,190 @@
+From 5482399434ea919130c6825ef4235ecd9fcd470d Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Thu, 10 Nov 2016 13:17:46 -0800
+Subject: [PATCH 14/47] x86/cet: Add control-protection fault handler
+
+A control-protection fault is triggered when a control-flow transfer
+attempt violates Shadow Stack or Indirect Branch Tracking constraints.
+For example, the return address for a RET instruction differs from the copy
+on the Shadow Stack; or an indirect JMP instruction, without the NOTRACK
+prefix, arrives at a non-ENDBR opcode.
+
+The control-protection fault handler works in a similar way as the general
+protection fault handler.  It provides the si_code SEGV_CPERR to the signal
+handler.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+
+v10:
+- Change CONFIG_X86_64 to CONFIG_X86_INTEL_CET.
+
+v9:
+- Add Shadow Stack pointer to the fault printout.
+---
+ arch/x86/entry/entry_64.S          |  2 +-
+ arch/x86/include/asm/traps.h       |  5 +++
+ arch/x86/kernel/idt.c              |  4 ++
+ arch/x86/kernel/signal_compat.c    |  2 +-
+ arch/x86/kernel/traps.c            | 59 ++++++++++++++++++++++++++++++
+ include/uapi/asm-generic/siginfo.h |  3 +-
+ 6 files changed, 72 insertions(+), 3 deletions(-)
+
+diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S
+index 3063aa9090f9..b260e3eaecda 100644
+--- a/arch/x86/entry/entry_64.S
++++ b/arch/x86/entry/entry_64.S
+@@ -1034,7 +1034,7 @@ idtentry spurious_interrupt_bug		do_spurious_interrupt_bug	has_error_code=0
+ idtentry coprocessor_error		do_coprocessor_error		has_error_code=0
+ idtentry alignment_check		do_alignment_check		has_error_code=1
+ idtentry simd_coprocessor_error		do_simd_coprocessor_error	has_error_code=0
+-
++idtentry control_protection		do_control_protection		has_error_code=1
+ 
+ 	/*
+ 	 * Reload gs selector with exception handling
+diff --git a/arch/x86/include/asm/traps.h b/arch/x86/include/asm/traps.h
+index c26a7e1d8a2c..9bf804709ee6 100644
+--- a/arch/x86/include/asm/traps.h
++++ b/arch/x86/include/asm/traps.h
+@@ -35,6 +35,9 @@ asmlinkage void alignment_check(void);
+ asmlinkage void machine_check(void);
+ #endif /* CONFIG_X86_MCE */
+ asmlinkage void simd_coprocessor_error(void);
++#ifdef CONFIG_X86_INTEL_CET
++asmlinkage void control_protection(void);
++#endif
+ 
+ #if defined(CONFIG_X86_64) && defined(CONFIG_XEN_PV)
+ asmlinkage void xen_divide_error(void);
+@@ -86,6 +89,7 @@ dotraplinkage void do_simd_coprocessor_error(struct pt_regs *regs, long error_co
+ dotraplinkage void do_iret_error(struct pt_regs *regs, long error_code);
+ #endif
+ dotraplinkage void do_mce(struct pt_regs *regs, long error_code);
++dotraplinkage void do_control_protection(struct pt_regs *regs, long error_code);
+ 
+ #ifdef CONFIG_X86_64
+ asmlinkage __visible notrace struct pt_regs *sync_regs(struct pt_regs *eregs);
+@@ -151,6 +155,7 @@ enum {
+ 	X86_TRAP_AC,		/* 17, Alignment Check */
+ 	X86_TRAP_MC,		/* 18, Machine Check */
+ 	X86_TRAP_XF,		/* 19, SIMD Floating-Point Exception */
++	X86_TRAP_CP = 21,	/* 21 Control Protection Fault */
+ 	X86_TRAP_IRET = 32,	/* 32, IRET Exception */
+ };
+ 
+diff --git a/arch/x86/kernel/idt.c b/arch/x86/kernel/idt.c
+index 87ef69a72c52..19160c8d734f 100644
+--- a/arch/x86/kernel/idt.c
++++ b/arch/x86/kernel/idt.c
+@@ -102,6 +102,10 @@ static const __initconst struct idt_data def_idts[] = {
+ #elif defined(CONFIG_X86_32)
+ 	SYSG(IA32_SYSCALL_VECTOR,	entry_INT80_32),
+ #endif
++
++#ifdef CONFIG_X86_INTEL_CET
++	INTG(X86_TRAP_CP,		control_protection),
++#endif
+ };
+ 
+ /*
+diff --git a/arch/x86/kernel/signal_compat.c b/arch/x86/kernel/signal_compat.c
+index 9ccbf0576cd0..c572a3de1037 100644
+--- a/arch/x86/kernel/signal_compat.c
++++ b/arch/x86/kernel/signal_compat.c
+@@ -27,7 +27,7 @@ static inline void signal_compat_build_tests(void)
+ 	 */
+ 	BUILD_BUG_ON(NSIGILL  != 11);
+ 	BUILD_BUG_ON(NSIGFPE  != 15);
+-	BUILD_BUG_ON(NSIGSEGV != 7);
++	BUILD_BUG_ON(NSIGSEGV != 8);
+ 	BUILD_BUG_ON(NSIGBUS  != 5);
+ 	BUILD_BUG_ON(NSIGTRAP != 5);
+ 	BUILD_BUG_ON(NSIGCHLD != 6);
+diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
+index d54cffdc7cac..d2515dfbc178 100644
+--- a/arch/x86/kernel/traps.c
++++ b/arch/x86/kernel/traps.c
+@@ -586,6 +586,65 @@ dotraplinkage void do_general_protection(struct pt_regs *regs, long error_code)
+ }
+ NOKPROBE_SYMBOL(do_general_protection);
+ 
++static const char * const control_protection_err[] = {
++	"unknown",
++	"near-ret",
++	"far-ret/iret",
++	"endbranch",
++	"rstorssp",
++	"setssbsy",
++};
++
++/*
++ * When a control protection exception occurs, send a signal
++ * to the responsible application.  Currently, control
++ * protection is only enabled for the user mode.  This
++ * exception should not come from the kernel mode.
++ */
++dotraplinkage void
++do_control_protection(struct pt_regs *regs, long error_code)
++{
++	struct task_struct *tsk;
++
++	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
++	if (notify_die(DIE_TRAP, "control protection fault", regs,
++		       error_code, X86_TRAP_CP, SIGSEGV) == NOTIFY_STOP)
++		return;
++	cond_local_irq_enable(regs);
++
++	if (!user_mode(regs))
++		die("kernel control protection fault", regs, error_code);
++
++	if (!static_cpu_has(X86_FEATURE_SHSTK) &&
++	    !static_cpu_has(X86_FEATURE_IBT))
++		WARN_ONCE(1, "CET is disabled but got control protection fault\n");
++
++	tsk = current;
++	tsk->thread.error_code = error_code;
++	tsk->thread.trap_nr = X86_TRAP_CP;
++
++	if (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&
++	    printk_ratelimit()) {
++		unsigned int max_err;
++		unsigned long ssp;
++
++		max_err = ARRAY_SIZE(control_protection_err) - 1;
++		if ((error_code < 0) || (error_code > max_err))
++			error_code = 0;
++		rdmsrl(MSR_IA32_PL3_SSP, ssp);
++		pr_info("%s[%d] control protection ip:%lx sp:%lx ssp:%lx error:%lx(%s)",
++			tsk->comm, task_pid_nr(tsk),
++			regs->ip, regs->sp, ssp, error_code,
++			control_protection_err[error_code]);
++		print_vma_addr(KERN_CONT " in ", regs->ip);
++		pr_cont("\n");
++	}
++
++	force_sig_fault(SIGSEGV, SEGV_CPERR,
++			(void __user *)uprobe_get_trap_addr(regs));
++}
++NOKPROBE_SYMBOL(do_control_protection);
++
+ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
+ {
+ 	if (poke_int3_handler(regs))
+diff --git a/include/uapi/asm-generic/siginfo.h b/include/uapi/asm-generic/siginfo.h
+index cb3d6c267181..693071dbe641 100644
+--- a/include/uapi/asm-generic/siginfo.h
++++ b/include/uapi/asm-generic/siginfo.h
+@@ -229,7 +229,8 @@ typedef struct siginfo {
+ #define SEGV_ACCADI	5	/* ADI not enabled for mapped object */
+ #define SEGV_ADIDERR	6	/* Disrupting MCD error */
+ #define SEGV_ADIPERR	7	/* Precise MCD exception */
+-#define NSIGSEGV	7
++#define SEGV_CPERR	8
++#define NSIGSEGV	8
+ 
+ /*
+  * SIGBUS si_codes
+-- 
+2.26.2
+
diff --git a/0015-x86-cet-shstk-Add-Kconfig-option-for-user-mode-Shado.patch b/0015-x86-cet-shstk-Add-Kconfig-option-for-user-mode-Shado.patch
new file mode 100644
index 000000000..f07882a67
--- /dev/null
+++ b/0015-x86-cet-shstk-Add-Kconfig-option-for-user-mode-Shado.patch
@@ -0,0 +1,79 @@
+From ba4ba07816db79eb7a3b77283fe2d50074f0d42d Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 3 Oct 2017 12:55:03 -0700
+Subject: [PATCH 15/47] x86/cet/shstk: Add Kconfig option for user-mode Shadow
+ Stack
+
+Shadow Stack provides protection against function return address
+corruption.  It is active when the processor supports it, the kernel has
+CONFIG_X86_INTEL_SHADOW_STACK_USER, and the application is built for the
+feature.  This is only implemented for the 64-bit kernel.  When it is
+enabled, legacy non-shadow stack applications continue to work, but without
+protection.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+
+v10:
+- Change SHSTK to shadow stack in the help text.
+- Change build-time check to config-time check.
+- Change ARCH_HAS_SHSTK to ARCH_HAS_SHADOW_STACK.
+---
+ arch/x86/Kconfig                      | 30 +++++++++++++++++++++++++++
+ scripts/as-x86_64-has-shadow-stack.sh |  4 ++++
+ 2 files changed, 34 insertions(+)
+ create mode 100755 scripts/as-x86_64-has-shadow-stack.sh
+
+diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
+index 2d3f963fd6f1..315647350fa2 100644
+--- a/arch/x86/Kconfig
++++ b/arch/x86/Kconfig
+@@ -1948,6 +1948,36 @@ config X86_INTEL_TSX_MODE_AUTO
+ 	  side channel attacks- equals the tsx=auto command line parameter.
+ endchoice
+ 
++config AS_HAS_SHADOW_STACK
++	def_bool $(success,$(srctree)/scripts/as-x86_64-has-shadow-stack.sh $(CC))
++	help
++	  Test the assembler for shadow stack instructions.
++
++config X86_INTEL_CET
++	def_bool n
++
++config ARCH_HAS_SHADOW_STACK
++	def_bool n
++
++config X86_INTEL_SHADOW_STACK_USER
++	prompt "Intel Shadow Stacks for user-mode"
++	def_bool n
++	depends on CPU_SUP_INTEL && X86_64
++	depends on AS_HAS_SHADOW_STACK
++	select ARCH_USES_HIGH_VMA_FLAGS
++	select X86_INTEL_CET
++	select ARCH_HAS_SHADOW_STACK
++	help
++	  Shadow Stacks provides protection against program stack
++	  corruption.  It's a hardware feature.  This only matters
++	  if you have the right hardware.  It's a security hardening
++	  feature and apps must be enabled to use it.  You get no
++	  protection "for free" on old userspace.  The hardware can
++	  support user and kernel, but this option is for user space
++	  only.
++
++	  If unsure, say y.
++
+ config EFI
+ 	bool "EFI runtime service support"
+ 	depends on ACPI
+diff --git a/scripts/as-x86_64-has-shadow-stack.sh b/scripts/as-x86_64-has-shadow-stack.sh
+new file mode 100755
+index 000000000000..fac1d363a1b8
+--- /dev/null
++++ b/scripts/as-x86_64-has-shadow-stack.sh
+@@ -0,0 +1,4 @@
++#!/bin/sh
++# SPDX-License-Identifier: GPL-2.0
++
++echo "wrussq %rax, (%rbx)" | $* -x assembler -c -
+-- 
+2.26.2
+
diff --git a/0016-x86-mm-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_HW.patch b/0016-x86-mm-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_HW.patch
new file mode 100644
index 000000000..e121baacb
--- /dev/null
+++ b/0016-x86-mm-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_HW.patch
@@ -0,0 +1,197 @@
+From ce2166e6e1ea24b455444c68679ed5acde6675fd Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Thu, 12 Apr 2018 09:32:59 -0700
+Subject: [PATCH 16/47] x86/mm: Change _PAGE_DIRTY to _PAGE_DIRTY_HW
+
+Before introducing _PAGE_COW for non-hardware memory management purposes in
+the next patch, rename _PAGE_DIRTY to _PAGE_DIRTY_HW and _PAGE_BIT_DIRTY to
+_PAGE_BIT_DIRTY_HW to make meanings more clear.  There are no functional
+changes from this patch.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+Reviewed-by: Dave Hansen <dave.hansen@intel.com>
+
+v9:
+- At some places _PAGE_DIRTY were not changed to _PAGE_DIRTY_HW, because
+  they will be changed again in the next patch to _PAGE_DIRTY_BITS.
+  However, this causes compile issues if the next patch is not yet applied.
+  Fix it by changing all _PAGE_DIRTY to _PAGE_DRITY_HW.
+---
+ arch/x86/include/asm/pgtable.h       | 18 +++++++++---------
+ arch/x86/include/asm/pgtable_types.h | 11 +++++------
+ arch/x86/kernel/relocate_kernel_64.S |  2 +-
+ arch/x86/kvm/vmx/vmx.c               |  2 +-
+ 4 files changed, 16 insertions(+), 17 deletions(-)
+
+diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
+index 4d02e64af1b3..90f9a73881ad 100644
+--- a/arch/x86/include/asm/pgtable.h
++++ b/arch/x86/include/asm/pgtable.h
+@@ -124,7 +124,7 @@ extern pmdval_t early_pmd_flags;
+  */
+ static inline int pte_dirty(pte_t pte)
+ {
+-	return pte_flags(pte) & _PAGE_DIRTY;
++	return pte_flags(pte) & _PAGE_DIRTY_HW;
+ }
+ 
+ 
+@@ -163,7 +163,7 @@ static inline int pte_young(pte_t pte)
+ 
+ static inline int pmd_dirty(pmd_t pmd)
+ {
+-	return pmd_flags(pmd) & _PAGE_DIRTY;
++	return pmd_flags(pmd) & _PAGE_DIRTY_HW;
+ }
+ 
+ static inline int pmd_young(pmd_t pmd)
+@@ -173,7 +173,7 @@ static inline int pmd_young(pmd_t pmd)
+ 
+ static inline int pud_dirty(pud_t pud)
+ {
+-	return pud_flags(pud) & _PAGE_DIRTY;
++	return pud_flags(pud) & _PAGE_DIRTY_HW;
+ }
+ 
+ static inline int pud_young(pud_t pud)
+@@ -333,7 +333,7 @@ static inline pte_t pte_clear_uffd_wp(pte_t pte)
+ 
+ static inline pte_t pte_mkclean(pte_t pte)
+ {
+-	return pte_clear_flags(pte, _PAGE_DIRTY);
++	return pte_clear_flags(pte, _PAGE_DIRTY_HW);
+ }
+ 
+ static inline pte_t pte_mkold(pte_t pte)
+@@ -353,7 +353,7 @@ static inline pte_t pte_mkexec(pte_t pte)
+ 
+ static inline pte_t pte_mkdirty(pte_t pte)
+ {
+-	return pte_set_flags(pte, _PAGE_DIRTY | _PAGE_SOFT_DIRTY);
++	return pte_set_flags(pte, _PAGE_DIRTY_HW | _PAGE_SOFT_DIRTY);
+ }
+ 
+ static inline pte_t pte_mkyoung(pte_t pte)
+@@ -434,7 +434,7 @@ static inline pmd_t pmd_mkold(pmd_t pmd)
+ 
+ static inline pmd_t pmd_mkclean(pmd_t pmd)
+ {
+-	return pmd_clear_flags(pmd, _PAGE_DIRTY);
++	return pmd_clear_flags(pmd, _PAGE_DIRTY_HW);
+ }
+ 
+ static inline pmd_t pmd_wrprotect(pmd_t pmd)
+@@ -444,7 +444,7 @@ static inline pmd_t pmd_wrprotect(pmd_t pmd)
+ 
+ static inline pmd_t pmd_mkdirty(pmd_t pmd)
+ {
+-	return pmd_set_flags(pmd, _PAGE_DIRTY | _PAGE_SOFT_DIRTY);
++	return pmd_set_flags(pmd, _PAGE_DIRTY_HW | _PAGE_SOFT_DIRTY);
+ }
+ 
+ static inline pmd_t pmd_mkdevmap(pmd_t pmd)
+@@ -488,7 +488,7 @@ static inline pud_t pud_mkold(pud_t pud)
+ 
+ static inline pud_t pud_mkclean(pud_t pud)
+ {
+-	return pud_clear_flags(pud, _PAGE_DIRTY);
++	return pud_clear_flags(pud, _PAGE_DIRTY_HW);
+ }
+ 
+ static inline pud_t pud_wrprotect(pud_t pud)
+@@ -498,7 +498,7 @@ static inline pud_t pud_wrprotect(pud_t pud)
+ 
+ static inline pud_t pud_mkdirty(pud_t pud)
+ {
+-	return pud_set_flags(pud, _PAGE_DIRTY | _PAGE_SOFT_DIRTY);
++	return pud_set_flags(pud, _PAGE_DIRTY_HW | _PAGE_SOFT_DIRTY);
+ }
+ 
+ static inline pud_t pud_mkdevmap(pud_t pud)
+diff --git a/arch/x86/include/asm/pgtable_types.h b/arch/x86/include/asm/pgtable_types.h
+index b6606fe6cfdf..b82e0f167879 100644
+--- a/arch/x86/include/asm/pgtable_types.h
++++ b/arch/x86/include/asm/pgtable_types.h
+@@ -15,7 +15,7 @@
+ #define _PAGE_BIT_PWT		3	/* page write through */
+ #define _PAGE_BIT_PCD		4	/* page cache disabled */
+ #define _PAGE_BIT_ACCESSED	5	/* was accessed (raised by CPU) */
+-#define _PAGE_BIT_DIRTY		6	/* was written to (raised by CPU) */
++#define _PAGE_BIT_DIRTY_HW	6	/* was written to (raised by CPU) */
+ #define _PAGE_BIT_PSE		7	/* 4 MB (or 2MB) page */
+ #define _PAGE_BIT_PAT		7	/* on 4KB pages */
+ #define _PAGE_BIT_GLOBAL	8	/* Global TLB entry PPro+ */
+@@ -46,7 +46,7 @@
+ #define _PAGE_PWT	(_AT(pteval_t, 1) << _PAGE_BIT_PWT)
+ #define _PAGE_PCD	(_AT(pteval_t, 1) << _PAGE_BIT_PCD)
+ #define _PAGE_ACCESSED	(_AT(pteval_t, 1) << _PAGE_BIT_ACCESSED)
+-#define _PAGE_DIRTY	(_AT(pteval_t, 1) << _PAGE_BIT_DIRTY)
++#define _PAGE_DIRTY_HW	(_AT(pteval_t, 1) << _PAGE_BIT_DIRTY_HW)
+ #define _PAGE_PSE	(_AT(pteval_t, 1) << _PAGE_BIT_PSE)
+ #define _PAGE_GLOBAL	(_AT(pteval_t, 1) << _PAGE_BIT_GLOBAL)
+ #define _PAGE_SOFTW1	(_AT(pteval_t, 1) << _PAGE_BIT_SOFTW1)
+@@ -74,7 +74,7 @@
+ 			 _PAGE_PKEY_BIT3)
+ 
+ #if defined(CONFIG_X86_64) || defined(CONFIG_X86_PAE)
+-#define _PAGE_KNL_ERRATUM_MASK (_PAGE_DIRTY | _PAGE_ACCESSED)
++#define _PAGE_KNL_ERRATUM_MASK (_PAGE_DIRTY_HW | _PAGE_ACCESSED)
+ #else
+ #define _PAGE_KNL_ERRATUM_MASK 0
+ #endif
+@@ -126,7 +126,7 @@
+  * pte_modify() does modify it.
+  */
+ #define _PAGE_CHG_MASK	(PTE_PFN_MASK | _PAGE_PCD | _PAGE_PWT |		\
+-			 _PAGE_SPECIAL | _PAGE_ACCESSED | _PAGE_DIRTY |	\
++			 _PAGE_SPECIAL | _PAGE_ACCESSED | _PAGE_DIRTY_HW |	\
+ 			 _PAGE_SOFT_DIRTY | _PAGE_DEVMAP | _PAGE_ENC |  \
+ 			 _PAGE_UFFD_WP)
+ #define _HPAGE_CHG_MASK (_PAGE_CHG_MASK | _PAGE_PSE)
+@@ -163,7 +163,7 @@ enum page_cache_mode {
+ #define __RW _PAGE_RW
+ #define _USR _PAGE_USER
+ #define ___A _PAGE_ACCESSED
+-#define ___D _PAGE_DIRTY
++#define ___D _PAGE_DIRTY_HW
+ #define ___G _PAGE_GLOBAL
+ #define __NX _PAGE_NX
+ 
+@@ -205,7 +205,6 @@ enum page_cache_mode {
+ #define __PAGE_KERNEL_IO		__PAGE_KERNEL
+ #define __PAGE_KERNEL_IO_NOCACHE	__PAGE_KERNEL_NOCACHE
+ 
+-
+ #ifndef __ASSEMBLY__
+ 
+ #define __PAGE_KERNEL_ENC	(__PAGE_KERNEL    | _ENC)
+diff --git a/arch/x86/kernel/relocate_kernel_64.S b/arch/x86/kernel/relocate_kernel_64.S
+index a4d9a261425b..e3bb4ff95523 100644
+--- a/arch/x86/kernel/relocate_kernel_64.S
++++ b/arch/x86/kernel/relocate_kernel_64.S
+@@ -17,7 +17,7 @@
+  */
+ 
+ #define PTR(x) (x << 3)
+-#define PAGE_ATTR (_PAGE_PRESENT | _PAGE_RW | _PAGE_ACCESSED | _PAGE_DIRTY)
++#define PAGE_ATTR (_PAGE_PRESENT | _PAGE_RW | _PAGE_ACCESSED | _PAGE_DIRTY_HW)
+ 
+ /*
+  * control_page + KEXEC_CONTROL_CODE_MAX_SIZE
+diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
+index 89c766fad889..5d143b2eb3b7 100644
+--- a/arch/x86/kvm/vmx/vmx.c
++++ b/arch/x86/kvm/vmx/vmx.c
+@@ -3500,7 +3500,7 @@ static int init_rmode_identity_map(struct kvm *kvm)
+ 	/* Set up identity-mapping pagetable for EPT in real mode */
+ 	for (i = 0; i < PT32_ENT_PER_PAGE; i++) {
+ 		tmp = (i << 22) + (_PAGE_PRESENT | _PAGE_RW | _PAGE_USER |
+-			_PAGE_ACCESSED | _PAGE_DIRTY | _PAGE_PSE);
++			_PAGE_ACCESSED | _PAGE_DIRTY_HW | _PAGE_PSE);
+ 		r = kvm_write_guest_page(kvm, identity_map_pfn,
+ 				&tmp, i * sizeof(tmp), sizeof(tmp));
+ 		if (r < 0)
+-- 
+2.26.2
+
diff --git a/0017-x86-mm-Remove-_PAGE_DIRTY_HW-from-kernel-RO-pages.patch b/0017-x86-mm-Remove-_PAGE_DIRTY_HW-from-kernel-RO-pages.patch
new file mode 100644
index 000000000..8ba1cd136
--- /dev/null
+++ b/0017-x86-mm-Remove-_PAGE_DIRTY_HW-from-kernel-RO-pages.patch
@@ -0,0 +1,56 @@
+From 59a325f01bf3297438bb7e9e9be1450f1cacb3ec Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 14 Apr 2020 14:48:14 -0700
+Subject: [PATCH 17/47] x86/mm: Remove _PAGE_DIRTY_HW from kernel RO pages
+
+Kernel read-only PTEs are setup as _PAGE_DIRTY_HW.  Since these become
+shadow stack PTEs, remove the dirty bit.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Cc: "H. Peter Anvin" <hpa@zytor.com>
+Cc: Kees Cook <keescook@chromium.org>
+Cc: Thomas Gleixner <tglx@linutronix.de>
+Cc: Dave Hansen <dave.hansen@linux.intel.com>
+Cc: Andy Lutomirski <luto@kernel.org>
+Cc: Ingo Molnar <mingo@redhat.com>
+Cc: Borislav Petkov <bp@alien8.de>
+Cc: Peter Zijlstra <peterz@infradead.org>
+---
+ arch/x86/include/asm/pgtable_types.h | 6 +++---
+ arch/x86/mm/pat/set_memory.c         | 2 +-
+ 2 files changed, 4 insertions(+), 4 deletions(-)
+
+diff --git a/arch/x86/include/asm/pgtable_types.h b/arch/x86/include/asm/pgtable_types.h
+index b82e0f167879..522b80b952f4 100644
+--- a/arch/x86/include/asm/pgtable_types.h
++++ b/arch/x86/include/asm/pgtable_types.h
+@@ -193,10 +193,10 @@ enum page_cache_mode {
+ #define _KERNPG_TABLE		 (__PP|__RW|   0|___A|   0|___D|   0|   0| _ENC)
+ #define _PAGE_TABLE_NOENC	 (__PP|__RW|_USR|___A|   0|___D|   0|   0)
+ #define _PAGE_TABLE		 (__PP|__RW|_USR|___A|   0|___D|   0|   0| _ENC)
+-#define __PAGE_KERNEL_RO	 (__PP|   0|   0|___A|__NX|___D|   0|___G)
+-#define __PAGE_KERNEL_RX	 (__PP|   0|   0|___A|   0|___D|   0|___G)
++#define __PAGE_KERNEL_RO	 (__PP|   0|   0|___A|__NX|   0|   0|___G)
++#define __PAGE_KERNEL_RX	 (__PP|   0|   0|___A|   0|   0|___G)
+ #define __PAGE_KERNEL_NOCACHE	 (__PP|__RW|   0|___A|__NX|___D|   0|___G| __NC)
+-#define __PAGE_KERNEL_VVAR	 (__PP|   0|_USR|___A|__NX|___D|   0|___G)
++#define __PAGE_KERNEL_VVAR	 (__PP|   0|_USR|___A|__NX|   0|   0|___G)
+ #define __PAGE_KERNEL_LARGE	 (__PP|__RW|   0|___A|__NX|___D|_PSE|___G)
+ #define __PAGE_KERNEL_LARGE_EXEC (__PP|__RW|   0|___A|   0|___D|_PSE|___G)
+ #define __PAGE_KERNEL_WP	 (__PP|__RW|   0|___A|__NX|___D|   0|___G| __WP)
+diff --git a/arch/x86/mm/pat/set_memory.c b/arch/x86/mm/pat/set_memory.c
+index b8c55a2e402d..deb8b044f15d 100644
+--- a/arch/x86/mm/pat/set_memory.c
++++ b/arch/x86/mm/pat/set_memory.c
+@@ -1927,7 +1927,7 @@ int set_memory_nx(unsigned long addr, int numpages)
+ 
+ int set_memory_ro(unsigned long addr, int numpages)
+ {
+-	return change_page_attr_clear(&addr, numpages, __pgprot(_PAGE_RW), 0);
++	return change_page_attr_clear(&addr, numpages, __pgprot(_PAGE_RW | _PAGE_DIRTY_HW), 0);
+ }
+ 
+ int set_memory_rw(unsigned long addr, int numpages)
+-- 
+2.26.2
+
diff --git a/0018-x86-mm-Introduce-_PAGE_COW.patch b/0018-x86-mm-Introduce-_PAGE_COW.patch
new file mode 100644
index 000000000..3e4679ceb
--- /dev/null
+++ b/0018-x86-mm-Introduce-_PAGE_COW.patch
@@ -0,0 +1,372 @@
+From 725ee629712261d648961d822a87aca0528a1930 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Wed, 24 Jan 2018 10:27:13 -0800
+Subject: [PATCH 18/47] x86/mm: Introduce _PAGE_COW
+
+There is essentially no room left in the x86 hardware PTEs on some OSes
+(not Linux).  That left the hardware architects looking for a way to
+represent a new memory type (shadow stack) within the existing bits.
+They chose to repurpose a lightly-used state: Write=0,Dirty=1.
+
+The reason it's lightly used is that Dirty=1 is normally set by hardware
+and cannot normally be set by hardware on a Write=0 PTE.  Software must
+normally be involved to create one of these PTEs, so software can simply
+opt to not create them.
+
+But that leaves us with a Linux problem: we need to ensure we never create
+Write=0,Dirty=1 PTEs.  In places where we do create them, we need to find
+an alternative way to represent them _without_ using the same hardware bit
+combination.  Thus, enter _PAGE_COW.  This results in the following:
+
+(a) A modified, copy-on-write (COW) page: (R/O + _PAGE_COW)
+(b) A R/O page that has been COW'ed: (R/O + _PAGE_COW)
+    The user page is in a R/O VMA, and get_user_pages() needs a writable
+    copy.  The page fault handler creates a copy of the page and sets
+    the new copy's PTE as R/O and _PAGE_COW.
+(c) A shadow stack PTE: (R/O + _PAGE_DIRTY_HW)
+(d) A shared shadow stack PTE: (R/O + _PAGE_COW)
+    When a shadow stack page is being shared among processes (this happens
+    at fork()), its PTE is cleared of _PAGE_DIRTY_HW, so the next shadow
+    stack access causes a fault, and the page is duplicated and
+    _PAGE_DIRTY_HW is set again.  This is the COW equivalent for shadow
+    stack pages, even though it's copy-on-access rather than copy-on-write.
+(e) A page where the processor observed a Write=1 PTE, started a write, set
+    Dirty=1, but then observed a Write=0 PTE.  That's possible today, but
+    will not happen on processors that support shadow stack.
+
+Use _PAGE_COW in pte_wrprotect() and _PAGE_DIRTY_HW in pte_mkwrite().
+Apply the same changes to pmd and pud.
+
+When this patch is applied, there are six free bits left in the 64-bit PTE.
+There are no more free bits in the 32-bit PTE (except for PAE) and shadow
+stack is not implemented for the 32-bit kernel.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+
+v10:
+- Change _PAGE_BIT_DIRTY_SW to _PAGE_BIT_COW, as it is used for copy-on-
+  write PTEs.
+- Update pte_write() and treat shadow stack as writable.
+- Change *_mkdirty_shstk() to *_mkwrite_shstk() as these make shadow stack
+  pages writable.
+- Use bit test & shift to move _PAGE_BIT_DIRTY_HW to _PAGE_BIT_COW.
+- Change static_cpu_has() to cpu_feature_enabled().
+- Revise commit log.
+
+v9:
+- Remove pte_move_flags() etc. and put the logic directly in
+  pte_wrprotect()/pte_mkwrite() etc.
+- Change compile-time conditionals to run-time checks.
+- Split out pte_modify()/pmd_modify() to a new patch.
+- Update comments.
+---
+ arch/x86/include/asm/pgtable.h       | 120 ++++++++++++++++++++++++---
+ arch/x86/include/asm/pgtable_types.h |  41 ++++++++-
+ 2 files changed, 150 insertions(+), 11 deletions(-)
+
+diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
+index 90f9a73881ad..5f89035d1e60 100644
+--- a/arch/x86/include/asm/pgtable.h
++++ b/arch/x86/include/asm/pgtable.h
+@@ -122,9 +122,9 @@ extern pmdval_t early_pmd_flags;
+  * The following only work if pte_present() is true.
+  * Undefined behaviour if not..
+  */
+-static inline int pte_dirty(pte_t pte)
++static inline bool pte_dirty(pte_t pte)
+ {
+-	return pte_flags(pte) & _PAGE_DIRTY_HW;
++	return pte_flags(pte) & _PAGE_DIRTY_BITS;
+ }
+ 
+ 
+@@ -161,9 +161,9 @@ static inline int pte_young(pte_t pte)
+ 	return pte_flags(pte) & _PAGE_ACCESSED;
+ }
+ 
+-static inline int pmd_dirty(pmd_t pmd)
++static inline bool pmd_dirty(pmd_t pmd)
+ {
+-	return pmd_flags(pmd) & _PAGE_DIRTY_HW;
++	return pmd_flags(pmd) & _PAGE_DIRTY_BITS;
+ }
+ 
+ static inline int pmd_young(pmd_t pmd)
+@@ -171,9 +171,9 @@ static inline int pmd_young(pmd_t pmd)
+ 	return pmd_flags(pmd) & _PAGE_ACCESSED;
+ }
+ 
+-static inline int pud_dirty(pud_t pud)
++static inline bool pud_dirty(pud_t pud)
+ {
+-	return pud_flags(pud) & _PAGE_DIRTY_HW;
++	return pud_flags(pud) & _PAGE_DIRTY_BITS;
+ }
+ 
+ static inline int pud_young(pud_t pud)
+@@ -183,6 +183,12 @@ static inline int pud_young(pud_t pud)
+ 
+ static inline int pte_write(pte_t pte)
+ {
++	/*
++	 * If _PAGE_DIRTY_HW is set, the PTE must either have
++	 * _PAGE_RW or be a shadow stack PTE, which is logically writable.
++	 */
++	if (cpu_feature_enabled(X86_FEATURE_SHSTK))
++		return pte_flags(pte) & (_PAGE_RW | _PAGE_DIRTY_HW);
+ 	return pte_flags(pte) & _PAGE_RW;
+ }
+ 
+@@ -333,7 +339,7 @@ static inline pte_t pte_clear_uffd_wp(pte_t pte)
+ 
+ static inline pte_t pte_mkclean(pte_t pte)
+ {
+-	return pte_clear_flags(pte, _PAGE_DIRTY_HW);
++	return pte_clear_flags(pte, _PAGE_DIRTY_BITS);
+ }
+ 
+ static inline pte_t pte_mkold(pte_t pte)
+@@ -343,6 +349,17 @@ static inline pte_t pte_mkold(pte_t pte)
+ 
+ static inline pte_t pte_wrprotect(pte_t pte)
+ {
++	/*
++	 * Blindly clearing _PAGE_RW might accidentally create
++	 * a shadow stack PTE (RW=0,Dirty=1).  Move the hardware
++	 * dirty value to the software bit.
++	 */
++	if (cpu_feature_enabled(X86_FEATURE_SHSTK)) {
++		pte.pte |= (pte.pte & _PAGE_DIRTY_HW) >>
++			   _PAGE_BIT_DIRTY_HW << _PAGE_BIT_COW;
++		pte = pte_clear_flags(pte, _PAGE_DIRTY_HW);
++	}
++
+ 	return pte_clear_flags(pte, _PAGE_RW);
+ }
+ 
+@@ -353,6 +370,18 @@ static inline pte_t pte_mkexec(pte_t pte)
+ 
+ static inline pte_t pte_mkdirty(pte_t pte)
+ {
++	pteval_t dirty = _PAGE_DIRTY_HW;
++
++	/* Avoid creating (HW)Dirty=1,Write=0 PTEs */
++	if (cpu_feature_enabled(X86_FEATURE_SHSTK) && !pte_write(pte))
++		dirty = _PAGE_COW;
++
++	return pte_set_flags(pte, dirty | _PAGE_SOFT_DIRTY);
++}
++
++static inline pte_t pte_mkwrite_shstk(pte_t pte)
++{
++	pte = pte_clear_flags(pte, _PAGE_COW);
+ 	return pte_set_flags(pte, _PAGE_DIRTY_HW | _PAGE_SOFT_DIRTY);
+ }
+ 
+@@ -363,6 +392,13 @@ static inline pte_t pte_mkyoung(pte_t pte)
+ 
+ static inline pte_t pte_mkwrite(pte_t pte)
+ {
++	if (cpu_feature_enabled(X86_FEATURE_SHSTK)) {
++		if (pte_flags(pte) & _PAGE_COW) {
++			pte = pte_clear_flags(pte, _PAGE_COW);
++			pte = pte_set_flags(pte, _PAGE_DIRTY_HW);
++		}
++	}
++
+ 	return pte_set_flags(pte, _PAGE_RW);
+ }
+ 
+@@ -434,16 +470,41 @@ static inline pmd_t pmd_mkold(pmd_t pmd)
+ 
+ static inline pmd_t pmd_mkclean(pmd_t pmd)
+ {
+-	return pmd_clear_flags(pmd, _PAGE_DIRTY_HW);
++	return pmd_clear_flags(pmd, _PAGE_DIRTY_BITS);
+ }
+ 
+ static inline pmd_t pmd_wrprotect(pmd_t pmd)
+ {
++	/*
++	 * Blindly clearing _PAGE_RW might accidentally create
++	 * a shadow stack PMD (RW=0,Dirty=1).  Move the hardware
++	 * dirty value to the software bit.
++	 */
++	if (cpu_feature_enabled(X86_FEATURE_SHSTK)) {
++		pmdval_t v = native_pmd_val(pmd);
++
++		v |= (v & _PAGE_DIRTY_HW) >> _PAGE_BIT_DIRTY_HW <<
++		     _PAGE_BIT_COW;
++		pmd = pmd_clear_flags(__pmd(v), _PAGE_DIRTY_HW);
++	}
++
+ 	return pmd_clear_flags(pmd, _PAGE_RW);
+ }
+ 
+ static inline pmd_t pmd_mkdirty(pmd_t pmd)
+ {
++	pmdval_t dirty = _PAGE_DIRTY_HW;
++
++	/* Avoid creating (HW)Dirty=1,Write=0 PMDs */
++	if (cpu_feature_enabled(X86_FEATURE_SHSTK) && !(pmd_flags(pmd) & _PAGE_RW))
++		dirty = _PAGE_COW;
++
++	return pmd_set_flags(pmd, dirty | _PAGE_SOFT_DIRTY);
++}
++
++static inline pmd_t pmd_mkwrite_shstk(pmd_t pmd)
++{
++	pmd = pmd_clear_flags(pmd, _PAGE_COW);
+ 	return pmd_set_flags(pmd, _PAGE_DIRTY_HW | _PAGE_SOFT_DIRTY);
+ }
+ 
+@@ -464,6 +525,13 @@ static inline pmd_t pmd_mkyoung(pmd_t pmd)
+ 
+ static inline pmd_t pmd_mkwrite(pmd_t pmd)
+ {
++	if (cpu_feature_enabled(X86_FEATURE_SHSTK)) {
++		if (pmd_flags(pmd) & _PAGE_COW) {
++			pmd = pmd_clear_flags(pmd, _PAGE_COW);
++			pmd = pmd_set_flags(pmd, _PAGE_DIRTY_HW);
++		}
++	}
++
+ 	return pmd_set_flags(pmd, _PAGE_RW);
+ }
+ 
+@@ -488,17 +556,36 @@ static inline pud_t pud_mkold(pud_t pud)
+ 
+ static inline pud_t pud_mkclean(pud_t pud)
+ {
+-	return pud_clear_flags(pud, _PAGE_DIRTY_HW);
++	return pud_clear_flags(pud, _PAGE_DIRTY_BITS);
+ }
+ 
+ static inline pud_t pud_wrprotect(pud_t pud)
+ {
++	/*
++	 * Blindly clearing _PAGE_RW might accidentally create
++	 * a shadow stack PUD (RW=0,Dirty=1).  Move the hardware
++	 * dirty value to the software bit.
++	 */
++	if (cpu_feature_enabled(X86_FEATURE_SHSTK)) {
++		pudval_t v = native_pud_val(pud);
++
++		v |= (v & _PAGE_DIRTY_HW) >> _PAGE_BIT_DIRTY_HW <<
++		     _PAGE_BIT_COW;
++		pud = pud_clear_flags(__pud(v), _PAGE_DIRTY_HW);
++	}
++
+ 	return pud_clear_flags(pud, _PAGE_RW);
+ }
+ 
+ static inline pud_t pud_mkdirty(pud_t pud)
+ {
+-	return pud_set_flags(pud, _PAGE_DIRTY_HW | _PAGE_SOFT_DIRTY);
++	pudval_t dirty = _PAGE_DIRTY_HW;
++
++	/* Avoid creating (HW)Dirty=1,Write=0 PUDs */
++	if (cpu_feature_enabled(X86_FEATURE_SHSTK) && !(pud_flags(pud) & _PAGE_RW))
++		dirty = _PAGE_COW;
++
++	return pud_set_flags(pud, dirty | _PAGE_SOFT_DIRTY);
+ }
+ 
+ static inline pud_t pud_mkdevmap(pud_t pud)
+@@ -518,6 +605,13 @@ static inline pud_t pud_mkyoung(pud_t pud)
+ 
+ static inline pud_t pud_mkwrite(pud_t pud)
+ {
++	if (cpu_feature_enabled(X86_FEATURE_SHSTK)) {
++		if (pud_flags(pud) & _PAGE_COW) {
++			pud = pud_clear_flags(pud, _PAGE_COW);
++			pud = pud_set_flags(pud, _PAGE_DIRTY_HW);
++		}
++	}
++
+ 	return pud_set_flags(pud, _PAGE_RW);
+ }
+ 
+@@ -1218,6 +1312,12 @@ extern int pmdp_clear_flush_young(struct vm_area_struct *vma,
+ #define pmd_write pmd_write
+ static inline int pmd_write(pmd_t pmd)
+ {
++	/*
++	 * If _PAGE_DIRTY_HW is set, then the PMD must either have
++	 * _PAGE_RW or be a shadow stack PMD, which is logically writable.
++	 */
++	if (cpu_feature_enabled(X86_FEATURE_SHSTK))
++		return pmd_flags(pmd) & (_PAGE_RW | _PAGE_DIRTY_HW);
+ 	return pmd_flags(pmd) & _PAGE_RW;
+ }
+ 
+diff --git a/arch/x86/include/asm/pgtable_types.h b/arch/x86/include/asm/pgtable_types.h
+index 522b80b952f4..74229db078ce 100644
+--- a/arch/x86/include/asm/pgtable_types.h
++++ b/arch/x86/include/asm/pgtable_types.h
+@@ -23,7 +23,8 @@
+ #define _PAGE_BIT_SOFTW2	10	/* " */
+ #define _PAGE_BIT_SOFTW3	11	/* " */
+ #define _PAGE_BIT_PAT_LARGE	12	/* On 2MB or 1GB pages */
+-#define _PAGE_BIT_SOFTW4	58	/* available for programmer */
++#define _PAGE_BIT_SOFTW4	57	/* available for programmer */
++#define _PAGE_BIT_SOFTW5	58	/* available for programmer */
+ #define _PAGE_BIT_PKEY_BIT0	59	/* Protection Keys, bit 1/4 */
+ #define _PAGE_BIT_PKEY_BIT1	60	/* Protection Keys, bit 2/4 */
+ #define _PAGE_BIT_PKEY_BIT2	61	/* Protection Keys, bit 3/4 */
+@@ -36,6 +37,16 @@
+ #define _PAGE_BIT_SOFT_DIRTY	_PAGE_BIT_SOFTW3 /* software dirty tracking */
+ #define _PAGE_BIT_DEVMAP	_PAGE_BIT_SOFTW4
+ 
++/*
++ * This bit indicates a copy-on-write page, and is different from
++ * _PAGE_BIT_SOFT_DIRTY, which tracks which pages a task writes to.
++ */
++#ifdef CONFIG_X86_64
++#define _PAGE_BIT_COW		_PAGE_BIT_SOFTW5 /* copy-on-write */
++#else
++#define _PAGE_BIT_COW		0
++#endif
++
+ /* If _PAGE_BIT_PRESENT is clear, we use these: */
+ /* - if the user mapped it with PROT_NONE; pte_present gives true */
+ #define _PAGE_BIT_PROTNONE	_PAGE_BIT_GLOBAL
+@@ -117,6 +128,34 @@
+ #define _PAGE_DEVMAP	(_AT(pteval_t, 0))
+ #endif
+ 
++/*
++ * _PAGE_COW is used to separate R/O and copy-on-write PTEs created by
++ * software from the shadow stack PTE setting required by the hardware:
++ * (a) A modified, copy-on-write (COW) page: (R/O + _PAGE_COW)
++ * (b) A R/O page that has been COW'ed: (R/O +_PAGE_COW)
++ *     The user page is in a R/O VMA, and get_user_pages() needs a
++ *     writable copy.  The page fault handler creates a copy of the page
++ *     and sets the new copy's PTE as R/O and _PAGE_COW.
++ * (c) A shadow stack PTE: (R/O + _PAGE_DIRTY_HW)
++ * (d) A shared (copy-on-access) shadow stack PTE: (R/O + _PAGE_COW)
++ *     When a shadow stack page is being shared among processes (this
++ *     happens at fork()), its PTE is cleared of _PAGE_DIRTY_HW, so the
++ *     next shadow stack access causes a fault, and the page is duplicated
++ *     and _PAGE_DIRTY_HW is set again.  This is the COW equivalent for
++ *     shadow stack pages, even though it's copy-on-access rather than
++ *     copy-on-write.
++ * (e) A page where the processor observed a Write=1 PTE, started a write,
++ *     set Dirty=1, but then observed a Write=0 PTE.  That's possible
++ *     today, but will not happen on processors that support shadow stack.
++ */
++#ifdef CONFIG_X86_INTEL_SHADOW_STACK_USER
++#define _PAGE_COW	(_AT(pteval_t, 1) << _PAGE_BIT_COW)
++#else
++#define _PAGE_COW	(_AT(pteval_t, 0))
++#endif
++
++#define _PAGE_DIRTY_BITS (_PAGE_DIRTY_HW | _PAGE_COW)
++
+ #define _PAGE_PROTNONE	(_AT(pteval_t, 1) << _PAGE_BIT_PROTNONE)
+ 
+ /*
+-- 
+2.26.2
+
diff --git a/0019-drm-i915-gvt-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_BITS.patch b/0019-drm-i915-gvt-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_BITS.patch
new file mode 100644
index 000000000..bf99c97bd
--- /dev/null
+++ b/0019-drm-i915-gvt-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_BITS.patch
@@ -0,0 +1,37 @@
+From 92be29e61e2aa19145f9b07aa665dce69d4b57e6 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 28 Aug 2018 13:01:49 -0700
+Subject: [PATCH 19/47] drm/i915/gvt: Change _PAGE_DIRTY to _PAGE_DIRTY_BITS
+
+After the introduction of _PAGE_COW, a modified page's PTE can have either
+_PAGE_DIRTY_HW or _PAGE_COW.  Change _PAGE_DIRTY to _PAGE_DIRTY_BITS.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+Cc: David Airlie <airlied@linux.ie>
+Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
+Cc: Jani Nikula <jani.nikula@linux.intel.com>
+Cc: Daniel Vetter <daniel@ffwll.ch>
+Cc: Rodrigo Vivi <rodrigo.vivi@intel.com>
+Cc: Zhenyu Wang <zhenyuw@linux.intel.com>
+Cc: Zhi Wang <zhi.a.wang@intel.com>
+---
+ drivers/gpu/drm/i915/gvt/gtt.c | 2 +-
+ 1 file changed, 1 insertion(+), 1 deletion(-)
+
+diff --git a/drivers/gpu/drm/i915/gvt/gtt.c b/drivers/gpu/drm/i915/gvt/gtt.c
+index 2a4b23f8aa74..789dce23424b 100644
+--- a/drivers/gpu/drm/i915/gvt/gtt.c
++++ b/drivers/gpu/drm/i915/gvt/gtt.c
+@@ -1207,7 +1207,7 @@ static int split_2MB_gtt_entry(struct intel_vgpu *vgpu,
+ 	}
+ 
+ 	/* Clear dirty field. */
+-	se->val64 &= ~_PAGE_DIRTY;
++	se->val64 &= ~_PAGE_DIRTY_BITS;
+ 
+ 	ops->clear_pse(se);
+ 	ops->clear_ips(se);
+-- 
+2.26.2
+
diff --git a/0020-x86-mm-Update-pte_modify-for-_PAGE_COW.patch b/0020-x86-mm-Update-pte_modify-for-_PAGE_COW.patch
new file mode 100644
index 000000000..d58e60781
--- /dev/null
+++ b/0020-x86-mm-Update-pte_modify-for-_PAGE_COW.patch
@@ -0,0 +1,84 @@
+From 9274121b3a80dd2046ed015c8f6dc1caae296332 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Thu, 29 Aug 2019 09:24:13 -0700
+Subject: [PATCH 20/47] x86/mm: Update pte_modify for _PAGE_COW
+
+Pte_modify() changes a PTE to 'newprot'.  It doesn't use the pte_*()
+helpers that a previous patch fixed up, so we need a new site.
+
+Introduce fixup_dirty_pte() to set the dirty bits based on _PAGE_RW, and
+apply the same changes to pmd_modify().
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+
+v10:
+- Change static_cpu_has() to cpu_feature_enabled().
+- Replace _PAGE_CHG_MASK approach with fixup functions.
+---
+ arch/x86/include/asm/pgtable.h | 33 +++++++++++++++++++++++++++++++++
+ 1 file changed, 33 insertions(+)
+
+diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
+index 5f89035d1e60..f4870cd040de 100644
+--- a/arch/x86/include/asm/pgtable.h
++++ b/arch/x86/include/asm/pgtable.h
+@@ -726,6 +726,21 @@ static inline pmd_t pmd_mknotpresent(pmd_t pmd)
+ 
+ static inline u64 flip_protnone_guard(u64 oldval, u64 val, u64 mask);
+ 
++static inline pteval_t fixup_dirty_pte(pteval_t pteval)
++{
++	pte_t pte = __pte(pteval);
++
++	if (pte_dirty(pte)) {
++		pte = pte_mkclean(pte);
++
++		if (pte_flags(pte) & _PAGE_RW)
++			pte = pte_set_flags(pte, _PAGE_DIRTY_HW);
++		else
++			pte = pte_set_flags(pte, _PAGE_COW);
++	}
++	return pte_val(pte);
++}
++
+ static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
+ {
+ 	pteval_t val = pte_val(pte), oldval = val;
+@@ -736,16 +751,34 @@ static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
+ 	 */
+ 	val &= _PAGE_CHG_MASK;
+ 	val |= check_pgprot(newprot) & ~_PAGE_CHG_MASK;
++	val = fixup_dirty_pte(val);
+ 	val = flip_protnone_guard(oldval, val, PTE_PFN_MASK);
+ 	return __pte(val);
+ }
+ 
++static inline int pmd_write(pmd_t pmd);
++static inline pmdval_t fixup_dirty_pmd(pmdval_t pmdval)
++{
++	pmd_t pmd = __pmd(pmdval);
++
++	if (pmd_dirty(pmd)) {
++		pmd = pmd_mkclean(pmd);
++
++		if (pmd_flags(pmd) & _PAGE_RW)
++			pmd = pmd_set_flags(pmd, _PAGE_DIRTY_HW);
++		else
++			pmd = pmd_set_flags(pmd, _PAGE_COW);
++	}
++	return pmd_val(pmd);
++}
++
+ static inline pmd_t pmd_modify(pmd_t pmd, pgprot_t newprot)
+ {
+ 	pmdval_t val = pmd_val(pmd), oldval = val;
+ 
+ 	val &= _HPAGE_CHG_MASK;
+ 	val |= check_pgprot(newprot) & ~_HPAGE_CHG_MASK;
++	val = fixup_dirty_pmd(val);
+ 	val = flip_protnone_guard(oldval, val, PHYSICAL_PMD_PAGE_MASK);
+ 	return __pmd(val);
+ }
+-- 
+2.26.2
+
diff --git a/0021-x86-mm-Update-ptep_set_wrprotect-and-pmdp_set_wrprot.patch b/0021-x86-mm-Update-ptep_set_wrprotect-and-pmdp_set_wrprot.patch
new file mode 100644
index 000000000..2e2c1da7a
--- /dev/null
+++ b/0021-x86-mm-Update-ptep_set_wrprotect-and-pmdp_set_wrprot.patch
@@ -0,0 +1,117 @@
+From fe62117cb5951c82e604a3cb770021910b4e27da Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Fri, 15 Jun 2018 09:33:40 -0700
+Subject: [PATCH 21/47] x86/mm: Update ptep_set_wrprotect() and
+ pmdp_set_wrprotect() for transition from _PAGE_DIRTY_HW to _PAGE_COW
+
+When shadow stack is introduced, [R/O + _PAGE_DIRTY_HW] PTE is reserved
+for shadow stack.  Copy-on-write PTEs have [R/O + _PAGE_COW].
+
+When a PTE goes from [R/W + _PAGE_DIRTY_HW] to [R/O + _PAGE_COW], it could
+become a transient shadow stack PTE in two cases:
+
+The first case is that some processors can start a write but end up seeing
+a read-only PTE by the time they get to the Dirty bit, creating a transient
+shadow stack PTE.  However, this will not occur on processors supporting
+shadow stack, therefore we don't need a TLB flush here.
+
+The second case is that when the software, without atomic, tests & replaces
+_PAGE_DIRTY_HW with _PAGE_COW, a transient shadow stack PTE can exist.
+This is prevented with cmpxchg.
+
+Dave Hansen, Jann Horn, Andy Lutomirski, and Peter Zijlstra provided many
+insights to the issue.  Jann Horn provided the cmpxchg solution.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+
+v10:
+- Replace bit shift with pte_wrprotect()/pmd_wrprotect(), which use bit
+  test & shift.
+- Move READ_ONCE of old_pte into try_cmpxchg() loop.
+- Change static_cpu_has() to cpu_feature_enabled().
+
+v9:
+- Change compile-time conditionals to runtime checks.
+- Fix parameters of try_cmpxchg(): change pte_t/pmd_t to
+  pte_t.pte/pmd_t.pmd.
+
+v4:
+- Implement try_cmpxchg().
+---
+ arch/x86/include/asm/pgtable.h | 52 ++++++++++++++++++++++++++++++++++
+ 1 file changed, 52 insertions(+)
+
+diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
+index f4870cd040de..eaa38adb1038 100644
+--- a/arch/x86/include/asm/pgtable.h
++++ b/arch/x86/include/asm/pgtable.h
+@@ -1316,6 +1316,32 @@ static inline pte_t ptep_get_and_clear_full(struct mm_struct *mm,
+ static inline void ptep_set_wrprotect(struct mm_struct *mm,
+ 				      unsigned long addr, pte_t *ptep)
+ {
++	/*
++	 * Some processors can start a write, but end up seeing a read-only
++	 * PTE by the time they get to the Dirty bit.  In this case, they
++	 * will set the Dirty bit, leaving a read-only, Dirty PTE which
++	 * looks like a shadow stack PTE.
++	 *
++	 * However, this behavior has been improved and will not occur on
++	 * processors supporting shadow stack.  Without this guarantee, a
++	 * transition to a non-present PTE and flush the TLB would be
++	 * needed.
++	 *
++	 * When changing a writable PTE to read-only and if the PTE has
++	 * _PAGE_DIRTY_HW set, move that bit to _PAGE_COW so that the
++	 * PTE is not a shadow stack PTE.
++	 */
++	if (cpu_feature_enabled(X86_FEATURE_SHSTK)) {
++		pte_t old_pte, new_pte;
++
++		do {
++			old_pte = READ_ONCE(*ptep);
++			new_pte = pte_wrprotect(old_pte);
++
++		} while (!try_cmpxchg(&ptep->pte, &old_pte.pte, new_pte.pte));
++
++		return;
++	}
+ 	clear_bit(_PAGE_BIT_RW, (unsigned long *)&ptep->pte);
+ }
+ 
+@@ -1372,6 +1398,32 @@ static inline pud_t pudp_huge_get_and_clear(struct mm_struct *mm,
+ static inline void pmdp_set_wrprotect(struct mm_struct *mm,
+ 				      unsigned long addr, pmd_t *pmdp)
+ {
++	/*
++	 * Some processors can start a write, but end up seeing a read-only
++	 * PMD by the time they get to the Dirty bit.  In this case, they
++	 * will set the Dirty bit, leaving a read-only, Dirty PMD which
++	 * looks like a Shadow Stack PMD.
++	 *
++	 * However, this behavior has been improved and will not occur on
++	 * processors supporting Shadow Stack.  Without this guarantee, a
++	 * transition to a non-present PMD and flush the TLB would be
++	 * needed.
++	 *
++	 * When changing a writable PMD to read-only and if the PMD has
++	 * _PAGE_DIRTY_HW set, we move that bit to _PAGE_COW so that the
++	 * PMD is not a shadow stack PMD.
++	 */
++	if (cpu_feature_enabled(X86_FEATURE_SHSTK)) {
++		pmd_t old_pmd, new_pmd;
++
++		do {
++			old_pmd = READ_ONCE(*pmdp);
++			new_pmd = pmd_wrprotect(old_pmd);
++
++		} while (!try_cmpxchg((pmdval_t *)pmdp, (pmdval_t *)&old_pmd, pmd_val(new_pmd)));
++
++		return;
++	}
+ 	clear_bit(_PAGE_BIT_RW, (unsigned long *)pmdp);
+ }
+ 
+-- 
+2.26.2
+
diff --git a/0022-mm-Introduce-VM_SHSTK-for-shadow-stack-memory.patch b/0022-mm-Introduce-VM_SHSTK-for-shadow-stack-memory.patch
new file mode 100644
index 000000000..a6ddd8985
--- /dev/null
+++ b/0022-mm-Introduce-VM_SHSTK-for-shadow-stack-memory.patch
@@ -0,0 +1,83 @@
+From 0a010dee48e194ef3d88731d58ff5e1cae81805f Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Fri, 12 Jan 2018 15:04:54 -0800
+Subject: [PATCH 22/47] mm: Introduce VM_SHSTK for shadow stack memory
+
+A Shadow Stack PTE must be read-only and have _PAGE_DIRTY set.  However,
+read-only and Dirty PTEs also exist for copy-on-write (COW) pages.  These
+two cases are handled differently for page faults.  Introduce VM_SHSTK to
+track shadow stack VMAs.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+
+v9:
+- Add VM_SHSTK case to arch_vma_name().
+- Revise the commit log to explain why adding a new VM flag.
+---
+ arch/x86/mm/mmap.c | 2 ++
+ fs/proc/task_mmu.c | 3 +++
+ include/linux/mm.h | 8 ++++++++
+ 3 files changed, 13 insertions(+)
+
+diff --git a/arch/x86/mm/mmap.c b/arch/x86/mm/mmap.c
+index cb91eccc4960..fe77fd6debf1 100644
+--- a/arch/x86/mm/mmap.c
++++ b/arch/x86/mm/mmap.c
+@@ -163,6 +163,8 @@ unsigned long get_mmap_base(int is_legacy)
+ 
+ const char *arch_vma_name(struct vm_area_struct *vma)
+ {
++	if (vma->vm_flags & VM_SHSTK)
++		return "[shadow stack]";
+ 	return NULL;
+ }
+ 
+diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c
+index 8d382d4ec067..434692759265 100644
+--- a/fs/proc/task_mmu.c
++++ b/fs/proc/task_mmu.c
+@@ -657,6 +657,9 @@ static void show_smap_vma_flags(struct seq_file *m, struct vm_area_struct *vma)
+ 		[ilog2(VM_PKEY_BIT4)]	= "",
+ #endif
+ #endif /* CONFIG_ARCH_HAS_PKEYS */
++#ifdef CONFIG_X86_INTEL_SHADOW_STACK_USER
++		[ilog2(VM_SHSTK)]	= "ss",
++#endif
+ 	};
+ 	size_t i;
+ 
+diff --git a/include/linux/mm.h b/include/linux/mm.h
+index f3fe7371855c..ac3490df64d8 100644
+--- a/include/linux/mm.h
++++ b/include/linux/mm.h
+@@ -294,11 +294,13 @@ extern unsigned int kobjsize(const void *objp);
+ #define VM_HIGH_ARCH_BIT_2	34	/* bit only usable on 64-bit architectures */
+ #define VM_HIGH_ARCH_BIT_3	35	/* bit only usable on 64-bit architectures */
+ #define VM_HIGH_ARCH_BIT_4	36	/* bit only usable on 64-bit architectures */
++#define VM_HIGH_ARCH_BIT_5	37	/* bit only usable on 64-bit architectures */
+ #define VM_HIGH_ARCH_0	BIT(VM_HIGH_ARCH_BIT_0)
+ #define VM_HIGH_ARCH_1	BIT(VM_HIGH_ARCH_BIT_1)
+ #define VM_HIGH_ARCH_2	BIT(VM_HIGH_ARCH_BIT_2)
+ #define VM_HIGH_ARCH_3	BIT(VM_HIGH_ARCH_BIT_3)
+ #define VM_HIGH_ARCH_4	BIT(VM_HIGH_ARCH_BIT_4)
++#define VM_HIGH_ARCH_5	BIT(VM_HIGH_ARCH_BIT_5)
+ #endif /* CONFIG_ARCH_USES_HIGH_VMA_FLAGS */
+ 
+ #ifdef CONFIG_ARCH_HAS_PKEYS
+@@ -336,6 +338,12 @@ extern unsigned int kobjsize(const void *objp);
+ # define VM_MPX		VM_NONE
+ #endif
+ 
++#ifdef CONFIG_X86_INTEL_SHADOW_STACK_USER
++# define VM_SHSTK	VM_HIGH_ARCH_5
++#else
++# define VM_SHSTK	VM_NONE
++#endif
++
+ #ifndef VM_GROWSUP
+ # define VM_GROWSUP	VM_NONE
+ #endif
+-- 
+2.26.2
+
diff --git a/0023-x86-mm-Shadow-Stack-page-fault-error-checking.patch b/0023-x86-mm-Shadow-Stack-page-fault-error-checking.patch
new file mode 100644
index 000000000..b95a12870
--- /dev/null
+++ b/0023-x86-mm-Shadow-Stack-page-fault-error-checking.patch
@@ -0,0 +1,96 @@
+From afe0845429def9f7412d9a0d3629bea890bf7a56 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 28 Nov 2017 13:01:18 -0800
+Subject: [PATCH 23/47] x86/mm: Shadow Stack page fault error checking
+
+Shadow stack accesses are those that are performed by the CPU where it
+expects to encounter a shadow stack mapping.  These accesses are performed
+implicitly by CALL/RET at the site of the shadow stack pointer.  These
+accesses are made explicitly by shadow stack management instructions like
+WRUSSQ.
+
+Shadow stacks accesses to shadow-stack mapping can see faults in normal,
+valid operation just like regular accesses to regular mappings.  Shadow
+stacks need some of the same features like delayed allocation, swap and
+copy-on-write.
+
+Shadow stack accesses can also result in errors, such as when a shadow
+stack overflows, or if a shadow stack access occurs to a non-shadow-stack
+mapping.
+
+In handling a shadow stack page fault, verify it occurs within a shadow
+stack mapping.  It is always an error otherwise.  For valid shadow stack
+accesses, set FAULT_FLAG_WRITE to effect copy-on-write.  Because clearing
+_PAGE_DIRTY_HW (vs. _PAGE_RW) is used to trigger the fault, shadow stack
+read fault and shadow stack write fault are not differentiated and both are
+handled as a write access.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+
+v10:
+-Revise commit log.
+---
+ arch/x86/include/asm/traps.h |  2 ++
+ arch/x86/mm/fault.c          | 19 +++++++++++++++++++
+ 2 files changed, 21 insertions(+)
+
+diff --git a/arch/x86/include/asm/traps.h b/arch/x86/include/asm/traps.h
+index 9bf804709ee6..b4f4c725a350 100644
+--- a/arch/x86/include/asm/traps.h
++++ b/arch/x86/include/asm/traps.h
+@@ -168,6 +168,7 @@ enum {
+  *   bit 3 ==				1: use of reserved bit detected
+  *   bit 4 ==				1: fault was an instruction fetch
+  *   bit 5 ==				1: protection keys block access
++ *   bit 6 ==				1: shadow stack access fault
+  */
+ enum x86_pf_error_code {
+ 	X86_PF_PROT	=		1 << 0,
+@@ -176,5 +177,6 @@ enum x86_pf_error_code {
+ 	X86_PF_RSVD	=		1 << 3,
+ 	X86_PF_INSTR	=		1 << 4,
+ 	X86_PF_PK	=		1 << 5,
++	X86_PF_SHSTK	=		1 << 6,
+ };
+ #endif /* _ASM_X86_TRAPS_H */
+diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
+index a51df516b87b..a4a3c8f016f0 100644
+--- a/arch/x86/mm/fault.c
++++ b/arch/x86/mm/fault.c
+@@ -1210,6 +1210,17 @@ access_error(unsigned long error_code, struct vm_area_struct *vma)
+ 				       (error_code & X86_PF_INSTR), foreign))
+ 		return 1;
+ 
++	/*
++	 * Verify a shadow stack access is within a shadow stack VMA.
++	 * It is always an error otherwise.  Normal data access to a
++	 * shadow stack area is checked in the case followed.
++	 */
++	if (error_code & X86_PF_SHSTK) {
++		if (!(vma->vm_flags & VM_SHSTK))
++			return 1;
++		return 0;
++	}
++
+ 	if (error_code & X86_PF_WRITE) {
+ 		/* write, present and write, not present: */
+ 		if (unlikely(!(vma->vm_flags & VM_WRITE)))
+@@ -1367,6 +1378,14 @@ void do_user_addr_fault(struct pt_regs *regs,
+ 
+ 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
+ 
++	/*
++	 * Clearing _PAGE_DIRTY_HW is used to detect shadow stack access.
++	 * This method cannot distinguish shadow stack read vs. write.
++	 * For valid shadow stack accesses, set FAULT_FLAG_WRITE to effect
++	 * copy-on-write.
++	 */
++	if (hw_error_code & X86_PF_SHSTK)
++		flags |= FAULT_FLAG_WRITE;
+ 	if (hw_error_code & X86_PF_WRITE)
+ 		flags |= FAULT_FLAG_WRITE;
+ 	if (hw_error_code & X86_PF_INSTR)
+-- 
+2.26.2
+
diff --git a/0024-x86-mm-Update-maybe_mkwrite-for-shadow-stack.patch b/0024-x86-mm-Update-maybe_mkwrite-for-shadow-stack.patch
new file mode 100644
index 000000000..0f1b1dad4
--- /dev/null
+++ b/0024-x86-mm-Update-maybe_mkwrite-for-shadow-stack.patch
@@ -0,0 +1,132 @@
+From 9705338c9d66aa1118c4cf1c2ee6cd09cac55f9d Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 7 Apr 2020 16:03:36 -0700
+Subject: [PATCH 24/47] x86/mm: Update maybe_mkwrite() for shadow stack
+
+Shadow stack memory is writable, but its VMA has VM_SHSTK instead of
+VM_WRITE.  Update maybe_mkwrite() to include the shadow stack.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/Kconfig              |  4 ++++
+ arch/x86/mm/pgtable.c         | 18 ++++++++++++++++++
+ include/asm-generic/pgtable.h | 24 ++++++++++++++++++++++++
+ include/linux/mm.h            |  2 ++
+ mm/huge_memory.c              |  2 ++
+ 5 files changed, 50 insertions(+)
+
+diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
+index 315647350fa2..df24c4a90ea0 100644
+--- a/arch/x86/Kconfig
++++ b/arch/x86/Kconfig
+@@ -1956,6 +1956,9 @@ config AS_HAS_SHADOW_STACK
+ config X86_INTEL_CET
+ 	def_bool n
+ 
++config ARCH_MAYBE_MKWRITE
++	def_bool n
++
+ config ARCH_HAS_SHADOW_STACK
+ 	def_bool n
+ 
+@@ -1966,6 +1969,7 @@ config X86_INTEL_SHADOW_STACK_USER
+ 	depends on AS_HAS_SHADOW_STACK
+ 	select ARCH_USES_HIGH_VMA_FLAGS
+ 	select X86_INTEL_CET
++	select ARCH_MAYBE_MKWRITE
+ 	select ARCH_HAS_SHADOW_STACK
+ 	help
+ 	  Shadow Stacks provides protection against program stack
+diff --git a/arch/x86/mm/pgtable.c b/arch/x86/mm/pgtable.c
+index 7bd2c3a52297..aa4d396ff98d 100644
+--- a/arch/x86/mm/pgtable.c
++++ b/arch/x86/mm/pgtable.c
+@@ -603,6 +603,24 @@ int pmdp_clear_flush_young(struct vm_area_struct *vma,
+ }
+ #endif
+ 
++#ifdef CONFIG_ARCH_MAYBE_MKWRITE
++pte_t arch_maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)
++{
++	if (likely(vma->vm_flags & VM_SHSTK))
++		pte = pte_mkwrite_shstk(pte);
++	return pte;
++}
++
++#ifdef CONFIG_TRANSPARENT_HUGEPAGE
++pmd_t arch_maybe_pmd_mkwrite(pmd_t pmd, struct vm_area_struct *vma)
++{
++	if (likely(vma->vm_flags & VM_SHSTK))
++		pmd = pmd_mkwrite_shstk(pmd);
++	return pmd;
++}
++#endif /* CONFIG_TRANSPARENT_HUGEPAGE */
++#endif /* CONFIG_ARCH_MAYBE_MKWRITE */
++
+ /**
+  * reserve_top_address - reserves a hole in the top of kernel address space
+  * @reserve - size of hole to reserve
+diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h
+index 329b8c8ca703..2c3875724809 100644
+--- a/include/asm-generic/pgtable.h
++++ b/include/asm-generic/pgtable.h
+@@ -1191,6 +1191,30 @@ static inline bool arch_has_pfn_modify_check(void)
+ }
+ #endif /* !_HAVE_ARCH_PFN_MODIFY_ALLOWED */
+ 
++#ifdef CONFIG_MMU
++#ifdef CONFIG_ARCH_MAYBE_MKWRITE
++pte_t arch_maybe_mkwrite(pte_t pte, struct vm_area_struct *vma);
++
++#ifdef CONFIG_TRANSPARENT_HUGEPAGE
++pmd_t arch_maybe_pmd_mkwrite(pmd_t pmd, struct vm_area_struct *vma);
++#endif /* CONFIG_TRANSPARENT_HUGEPAGE */
++
++#else /* !CONFIG_ARCH_MAYBE_MKWRITE */
++static inline pte_t arch_maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)
++{
++	return pte;
++}
++
++#ifdef CONFIG_TRANSPARENT_HUGEPAGE
++static inline pmd_t arch_maybe_pmd_mkwrite(pmd_t pmd, struct vm_area_struct *vma)
++{
++	return pmd;
++}
++#endif /* CONFIG_TRANSPARENT_HUGEPAGE */
++
++#endif /* CONFIG_ARCH_MAYBE_MKWRITE */
++#endif /* CONFIG_MMU */
++
+ /*
+  * Architecture PAGE_KERNEL_* fallbacks
+  *
+diff --git a/include/linux/mm.h b/include/linux/mm.h
+index ac3490df64d8..6c31b5222b0f 100644
+--- a/include/linux/mm.h
++++ b/include/linux/mm.h
+@@ -955,6 +955,8 @@ static inline pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)
+ {
+ 	if (likely(vma->vm_flags & VM_WRITE))
+ 		pte = pte_mkwrite(pte);
++	else
++		pte = arch_maybe_mkwrite(pte, vma);
+ 	return pte;
+ }
+ 
+diff --git a/mm/huge_memory.c b/mm/huge_memory.c
+index 6ecd1045113b..608746bb9d19 100644
+--- a/mm/huge_memory.c
++++ b/mm/huge_memory.c
+@@ -485,6 +485,8 @@ pmd_t maybe_pmd_mkwrite(pmd_t pmd, struct vm_area_struct *vma)
+ {
+ 	if (likely(vma->vm_flags & VM_WRITE))
+ 		pmd = pmd_mkwrite(pmd);
++	else
++		pmd = arch_maybe_pmd_mkwrite(pmd, vma);
+ 	return pmd;
+ }
+ 
+-- 
+2.26.2
+
diff --git a/0025-mm-Fixup-places-that-call-pte_mkwrite-directly.patch b/0025-mm-Fixup-places-that-call-pte_mkwrite-directly.patch
new file mode 100644
index 000000000..230508b79
--- /dev/null
+++ b/0025-mm-Fixup-places-that-call-pte_mkwrite-directly.patch
@@ -0,0 +1,80 @@
+From cd5f5266db8dc418271ebc131313633bfe96e51a Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 13 Apr 2020 13:21:22 -0700
+Subject: [PATCH 25/47] mm: Fixup places that call pte_mkwrite() directly
+
+A shadow stack page is made writable by pte_mkwrite_shstk(), which sets
+_PAGE_DIRTY_HW.  There are a few places that call pte_mkwrite() directly
+and miss the maybe_mkwrite() fixup in the previous patch.  Fix them with
+maybe_mkwrite():
+
+- do_anonymous_page() and migrate_vma_insert_page() check VM_WRITE directly
+  and call pte_mkwrite(), which is the same as maybe_mkwrite().  Change
+  them to maybe_mkwrite().
+
+- In do_numa_page(), if the numa entry 'was-writable', then pte_mkwrite()
+  is called directly.  Fix it by doing maybe_mkwrite().
+
+- In change_pte_range(), pte_mkwrite() is called directly.  Replace it with
+  maybe_mkwrite().
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ mm/memory.c   | 5 ++---
+ mm/migrate.c  | 3 +--
+ mm/mprotect.c | 2 +-
+ 3 files changed, 4 insertions(+), 6 deletions(-)
+
+diff --git a/mm/memory.c b/mm/memory.c
+index f703fe8c8346..b9002f644806 100644
+--- a/mm/memory.c
++++ b/mm/memory.c
+@@ -3373,8 +3373,7 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
+ 	__SetPageUptodate(page);
+ 
+ 	entry = mk_pte(page, vma->vm_page_prot);
+-	if (vma->vm_flags & VM_WRITE)
+-		entry = pte_mkwrite(pte_mkdirty(entry));
++	entry = maybe_mkwrite(pte_mkdirty(entry), vma);
+ 
+ 	vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,
+ 			&vmf->ptl);
+@@ -4033,7 +4032,7 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)
+ 	pte = pte_modify(old_pte, vma->vm_page_prot);
+ 	pte = pte_mkyoung(pte);
+ 	if (was_writable)
+-		pte = pte_mkwrite(pte);
++		pte = maybe_mkwrite(pte, vma);
+ 	ptep_modify_prot_commit(vma, vmf->address, vmf->pte, old_pte, pte);
+ 	update_mmu_cache(vma, vmf->address, vmf->pte);
+ 
+diff --git a/mm/migrate.c b/mm/migrate.c
+index 7160c1556f79..0fa59b1562c6 100644
+--- a/mm/migrate.c
++++ b/mm/migrate.c
+@@ -2805,8 +2805,7 @@ static void migrate_vma_insert_page(struct migrate_vma *migrate,
+ 		}
+ 	} else {
+ 		entry = mk_pte(page, vma->vm_page_prot);
+-		if (vma->vm_flags & VM_WRITE)
+-			entry = pte_mkwrite(pte_mkdirty(entry));
++		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
+ 	}
+ 
+ 	ptep = pte_offset_map_lock(mm, pmdp, addr, &ptl);
+diff --git a/mm/mprotect.c b/mm/mprotect.c
+index 494192ca954b..02762af1057c 100644
+--- a/mm/mprotect.c
++++ b/mm/mprotect.c
+@@ -135,7 +135,7 @@ static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
+ 			if (dirty_accountable && pte_dirty(ptent) &&
+ 					(pte_soft_dirty(ptent) ||
+ 					 !(vma->vm_flags & VM_SOFTDIRTY))) {
+-				ptent = pte_mkwrite(ptent);
++				ptent = maybe_mkwrite(ptent, vma);
+ 			}
+ 			ptep_modify_prot_commit(vma, addr, pte, oldpte, ptent);
+ 			pages++;
+-- 
+2.26.2
+
diff --git a/0026-mm-Add-guard-pages-around-a-shadow-stack.patch b/0026-mm-Add-guard-pages-around-a-shadow-stack.patch
new file mode 100644
index 000000000..e2c4b3872
--- /dev/null
+++ b/0026-mm-Add-guard-pages-around-a-shadow-stack.patch
@@ -0,0 +1,98 @@
+From c2a20059924bcb9c77937a77c9999e9eec5a0a4f Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Fri, 4 Oct 2019 14:00:58 -0700
+Subject: [PATCH 26/47] mm: Add guard pages around a shadow stack.
+
+INCSSP(Q/D) increments shadow stack pointer and 'pops and discards' the
+first and the last elements in the range, effectively touches those memory
+areas.
+
+The maximum moving distance by INCSSPQ is 255 * 8 = 2040 bytes and
+255 * 4 = 1020 bytes by INCSSPD.  Both ranges are far from PAGE_SIZE.
+Thus, putting a gap page on both ends of a shadow stack prevents INCSSP,
+CALL, and RET from going beyond.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+
+v10:
+- Define ARCH_SHADOW_STACK_GUARD_GAP.
+---
+ arch/x86/include/asm/processor.h | 10 ++++++++++
+ include/linux/mm.h               | 24 ++++++++++++++++++++----
+ 2 files changed, 30 insertions(+), 4 deletions(-)
+
+diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
+index 3bcf27caf6c9..eb9536f803f9 100644
+--- a/arch/x86/include/asm/processor.h
++++ b/arch/x86/include/asm/processor.h
+@@ -907,6 +907,16 @@ static inline void spin_lock_prefetch(const void *x)
+ #define STACK_TOP		TASK_SIZE_LOW
+ #define STACK_TOP_MAX		TASK_SIZE_MAX
+ 
++/*
++ * Shadow stack pointer is moved by CALL, JMP, and INCSSP(Q/D).  INCSSPQ
++ * moves shadow stack pointer up to 255 * 8 = ~2 KB (~1KB for INCSSPD) and
++ * touches the first and the last element in the range, which triggers a
++ * page fault if the range is not in a shadow stack.  Because of this,
++ * creating 4-KB guard pages around a shadow stack prevents these
++ * instructions from going beyond.
++ */
++#define ARCH_SHADOW_STACK_GUARD_GAP PAGE_SIZE
++
+ #define INIT_THREAD  {						\
+ 	.addr_limit		= KERNEL_DS,			\
+ }
+diff --git a/include/linux/mm.h b/include/linux/mm.h
+index 6c31b5222b0f..5c623db088eb 100644
+--- a/include/linux/mm.h
++++ b/include/linux/mm.h
+@@ -2641,6 +2641,10 @@ void page_cache_async_readahead(struct address_space *mapping,
+ 				pgoff_t offset,
+ 				unsigned long size);
+ 
++#ifndef ARCH_SHADOW_STACK_GUARD_GAP
++#define ARCH_SHADOW_STACK_GUARD_GAP 0
++#endif
++
+ extern unsigned long stack_guard_gap;
+ /* Generic expand stack which grows the stack according to GROWS{UP,DOWN} */
+ extern int expand_stack(struct vm_area_struct *vma, unsigned long address);
+@@ -2673,9 +2677,15 @@ static inline struct vm_area_struct * find_vma_intersection(struct mm_struct * m
+ static inline unsigned long vm_start_gap(struct vm_area_struct *vma)
+ {
+ 	unsigned long vm_start = vma->vm_start;
++	unsigned long gap = 0;
+ 
+-	if (vma->vm_flags & VM_GROWSDOWN) {
+-		vm_start -= stack_guard_gap;
++	if (vma->vm_flags & VM_GROWSDOWN)
++		gap = stack_guard_gap;
++	else if (vma->vm_flags & VM_SHSTK)
++		gap = ARCH_SHADOW_STACK_GUARD_GAP;
++
++	if (gap != 0) {
++		vm_start -= gap;
+ 		if (vm_start > vma->vm_start)
+ 			vm_start = 0;
+ 	}
+@@ -2685,9 +2695,15 @@ static inline unsigned long vm_start_gap(struct vm_area_struct *vma)
+ static inline unsigned long vm_end_gap(struct vm_area_struct *vma)
+ {
+ 	unsigned long vm_end = vma->vm_end;
++	unsigned long gap = 0;
++
++	if (vma->vm_flags & VM_GROWSUP)
++		gap = stack_guard_gap;
++	else if (vma->vm_flags & VM_SHSTK)
++		gap = ARCH_SHADOW_STACK_GUARD_GAP;
+ 
+-	if (vma->vm_flags & VM_GROWSUP) {
+-		vm_end += stack_guard_gap;
++	if (gap != 0) {
++		vm_end += gap;
+ 		if (vm_end < vma->vm_end)
+ 			vm_end = -PAGE_SIZE;
+ 	}
+-- 
+2.26.2
+
diff --git a/0027-mm-mmap-Add-shadow-stack-pages-to-memory-accounting.patch b/0027-mm-mmap-Add-shadow-stack-pages-to-memory-accounting.patch
new file mode 100644
index 000000000..78a391b55
--- /dev/null
+++ b/0027-mm-mmap-Add-shadow-stack-pages-to-memory-accounting.patch
@@ -0,0 +1,83 @@
+From 699a8340f958398bc90c83fed9ad41da00a77789 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Thu, 13 Sep 2018 12:36:48 -0700
+Subject: [PATCH 27/47] mm/mmap: Add shadow stack pages to memory accounting
+
+Account shadow stack pages to stack memory.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+
+v10:
+- Use arch_shadow_stack_mapping() to make meaning clear.
+
+v8:
+- Change shadow stake pages from data_vm to stack_vm.
+---
+ arch/x86/mm/pgtable.c         |  7 +++++++
+ include/asm-generic/pgtable.h | 11 +++++++++++
+ mm/mmap.c                     |  5 +++++
+ 3 files changed, 23 insertions(+)
+
+diff --git a/arch/x86/mm/pgtable.c b/arch/x86/mm/pgtable.c
+index aa4d396ff98d..f384e0314ba7 100644
+--- a/arch/x86/mm/pgtable.c
++++ b/arch/x86/mm/pgtable.c
+@@ -890,3 +890,10 @@ int pmd_free_pte_page(pmd_t *pmd, unsigned long addr)
+ 
+ #endif /* CONFIG_X86_64 */
+ #endif	/* CONFIG_HAVE_ARCH_HUGE_VMAP */
++
++#ifdef CONFIG_ARCH_HAS_SHADOW_STACK
++bool arch_shadow_stack_mapping(vm_flags_t vm_flags)
++{
++	return (vm_flags & VM_SHSTK);
++}
++#endif
+diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h
+index 2c3875724809..dbd415ab7dd8 100644
+--- a/include/asm-generic/pgtable.h
++++ b/include/asm-generic/pgtable.h
+@@ -1215,6 +1215,17 @@ static inline pmd_t arch_maybe_pmd_mkwrite(pmd_t pmd, struct vm_area_struct *vma
+ #endif /* CONFIG_ARCH_MAYBE_MKWRITE */
+ #endif /* CONFIG_MMU */
+ 
++#ifdef CONFIG_MMU
++#ifdef CONFIG_ARCH_HAS_SHADOW_STACK
++bool arch_shadow_stack_mapping(vm_flags_t vm_flags);
++#else
++static inline bool arch_shadow_stack_mapping(vm_flags_t vm_flags)
++{
++	return false;
++}
++#endif /* CONFIG_ARCH_HAS_SHADOW_STACK */
++#endif /* CONFIG_MMU */
++
+ /*
+  * Architecture PAGE_KERNEL_* fallbacks
+  *
+diff --git a/mm/mmap.c b/mm/mmap.c
+index f609e9ec4a25..70d240b3559c 100644
+--- a/mm/mmap.c
++++ b/mm/mmap.c
+@@ -1681,6 +1681,9 @@ static inline int accountable_mapping(struct file *file, vm_flags_t vm_flags)
+ 	if (file && is_file_hugepages(file))
+ 		return 0;
+ 
++	if (arch_shadow_stack_mapping(vm_flags))
++		return 1;
++
+ 	return (vm_flags & (VM_NORESERVE | VM_SHARED | VM_WRITE)) == VM_WRITE;
+ }
+ 
+@@ -3318,6 +3321,8 @@ void vm_stat_account(struct mm_struct *mm, vm_flags_t flags, long npages)
+ 		mm->stack_vm += npages;
+ 	else if (is_data_mapping(flags))
+ 		mm->data_vm += npages;
++	else if (arch_shadow_stack_mapping(flags))
++		mm->stack_vm += npages;
+ }
+ 
+ static vm_fault_t special_mapping_fault(struct vm_fault *vmf);
+-- 
+2.26.2
+
diff --git a/0028-mm-Update-can_follow_write_pte-for-shadow-stack.patch b/0028-mm-Update-can_follow_write_pte-for-shadow-stack.patch
new file mode 100644
index 000000000..2f7ac3742
--- /dev/null
+++ b/0028-mm-Update-can_follow_write_pte-for-shadow-stack.patch
@@ -0,0 +1,82 @@
+From 3b9f89fdc05ebb1861d163cb52c7d7a9c21a2055 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 3 Jul 2018 13:07:12 -0700
+Subject: [PATCH 28/47] mm: Update can_follow_write_pte() for shadow stack
+
+Can_follow_write_pte() ensures a read-only page is COWed by checking the
+FOLL_COW flag, and uses pte_dirty() to validate the flag is still valid.
+
+Like a writable data page, a shadow stack page is writable, and becomes
+read-only during copy-on-write, but it is always dirty.  Thus, in the
+can_follow_write_pte() check, it belongs to the writable page case and
+should be excluded from the read-only page pte_dirty() check.  Apply
+the same changes to can_follow_write_pmd().
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+
+v10:
+- Reverse name changes to can_follow_write_*().
+---
+ mm/gup.c         | 8 +++++---
+ mm/huge_memory.c | 8 +++++---
+ 2 files changed, 10 insertions(+), 6 deletions(-)
+
+diff --git a/mm/gup.c b/mm/gup.c
+index 87a6a59fe667..304366ba141e 100644
+--- a/mm/gup.c
++++ b/mm/gup.c
+@@ -385,10 +385,12 @@ static int follow_pfn_pte(struct vm_area_struct *vma, unsigned long address,
+  * FOLL_FORCE can write to even unwritable pte's, but only
+  * after we've gone through a COW cycle and they are dirty.
+  */
+-static inline bool can_follow_write_pte(pte_t pte, unsigned int flags)
++static inline bool can_follow_write_pte(pte_t pte, unsigned int flags,
++					struct vm_area_struct *vma)
+ {
+ 	return pte_write(pte) ||
+-		((flags & FOLL_FORCE) && (flags & FOLL_COW) && pte_dirty(pte));
++		((flags & FOLL_FORCE) && (flags & FOLL_COW) &&
++		 !arch_shadow_stack_mapping(vma->vm_flags) && pte_dirty(pte));
+ }
+ 
+ static struct page *follow_page_pte(struct vm_area_struct *vma,
+@@ -431,7 +433,7 @@ static struct page *follow_page_pte(struct vm_area_struct *vma,
+ 	}
+ 	if ((flags & FOLL_NUMA) && pte_protnone(pte))
+ 		goto no_page;
+-	if ((flags & FOLL_WRITE) && !can_follow_write_pte(pte, flags)) {
++	if ((flags & FOLL_WRITE) && !can_follow_write_pte(pte, flags, vma)) {
+ 		pte_unmap_unlock(ptep, ptl);
+ 		return NULL;
+ 	}
+diff --git a/mm/huge_memory.c b/mm/huge_memory.c
+index 608746bb9d19..cb1b0cb4b4eb 100644
+--- a/mm/huge_memory.c
++++ b/mm/huge_memory.c
+@@ -1520,10 +1520,12 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd)
+  * FOLL_FORCE can write to even unwritable pmd's, but only
+  * after we've gone through a COW cycle and they are dirty.
+  */
+-static inline bool can_follow_write_pmd(pmd_t pmd, unsigned int flags)
++static inline bool can_follow_write_pmd(pmd_t pmd, unsigned int flags,
++					struct vm_area_struct *vma)
+ {
+ 	return pmd_write(pmd) ||
+-	       ((flags & FOLL_FORCE) && (flags & FOLL_COW) && pmd_dirty(pmd));
++	       ((flags & FOLL_FORCE) && (flags & FOLL_COW) &&
++		!arch_shadow_stack_mapping(vma->vm_flags) && pmd_dirty(pmd));
+ }
+ 
+ struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,
+@@ -1536,7 +1538,7 @@ struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,
+ 
+ 	assert_spin_locked(pmd_lockptr(mm, pmd));
+ 
+-	if (flags & FOLL_WRITE && !can_follow_write_pmd(*pmd, flags))
++	if (flags & FOLL_WRITE && !can_follow_write_pmd(*pmd, flags, vma))
+ 		goto out;
+ 
+ 	/* Avoid dumping huge zero page */
+-- 
+2.26.2
+
diff --git a/0029-x86-cet-shstk-User-mode-shadow-stack-support.patch b/0029-x86-cet-shstk-User-mode-shadow-stack-support.patch
new file mode 100644
index 000000000..1af57d6ec
--- /dev/null
+++ b/0029-x86-cet-shstk-User-mode-shadow-stack-support.patch
@@ -0,0 +1,371 @@
+From ca1a609ee448e93b86809cc383bbcac891c9f841 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Thu, 22 Aug 2019 10:06:11 -0700
+Subject: [PATCH 29/47] x86/cet/shstk: User-mode shadow stack support
+
+This patch adds basic shadow stack enabling/disabling routines.  A task's
+shadow stack is allocated from memory with VM_SHSTK flag and has a fixed
+size of min(RLIMIT_STACK, 4GB).
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+
+v10:
+- Change no_cet_shstk to no_user_shstk.
+- Limit shadow stack size to 4 GB, and round_up to PAGE_SIZE.
+- Replace checking shstk_enabled with shstk_size being zero.
+- WARN_ON_ONCE() when vm_munmap() fails.
+
+v9:
+- Change cpu_feature_enabled() to static_cpu_has().
+- Merge cet_disable_shstk to cet_disable_free_shstk.
+- Remove the empty slot at the top of the shadow stack, as it is not
+  needed.
+- Move do_mmap_locked() to alloc_shstk(), which is a static function.
+
+v6:
+- Create a function do_mmap_locked() for shadow stack allocation.
+
+v2:
+- Change noshstk to no_cet_shstk.
+---
+ arch/x86/include/asm/cet.h                    |  26 ++++
+ arch/x86/include/asm/disabled-features.h      |   8 +-
+ arch/x86/include/asm/processor.h              |   5 +
+ arch/x86/kernel/Makefile                      |   2 +
+ arch/x86/kernel/cet.c                         | 135 ++++++++++++++++++
+ arch/x86/kernel/cpu/common.c                  |  28 ++++
+ arch/x86/kernel/process.c                     |   1 +
+ .../arch/x86/include/asm/disabled-features.h  |   8 +-
+ 8 files changed, 211 insertions(+), 2 deletions(-)
+ create mode 100644 arch/x86/include/asm/cet.h
+ create mode 100644 arch/x86/kernel/cet.c
+
+diff --git a/arch/x86/include/asm/cet.h b/arch/x86/include/asm/cet.h
+new file mode 100644
+index 000000000000..caac0687c8e4
+--- /dev/null
++++ b/arch/x86/include/asm/cet.h
+@@ -0,0 +1,26 @@
++/* SPDX-License-Identifier: GPL-2.0 */
++#ifndef _ASM_X86_CET_H
++#define _ASM_X86_CET_H
++
++#ifndef __ASSEMBLY__
++#include <linux/types.h>
++
++struct task_struct;
++/*
++ * Per-thread CET status
++ */
++struct cet_status {
++	unsigned long	shstk_base;
++	unsigned long	shstk_size;
++};
++
++#ifdef CONFIG_X86_INTEL_CET
++int cet_setup_shstk(void);
++void cet_disable_free_shstk(struct task_struct *p);
++#else
++static inline void cet_disable_free_shstk(struct task_struct *p) {}
++#endif
++
++#endif /* __ASSEMBLY__ */
++
++#endif /* _ASM_X86_CET_H */
+diff --git a/arch/x86/include/asm/disabled-features.h b/arch/x86/include/asm/disabled-features.h
+index 4ea8584682f9..a0e1b24cfa02 100644
+--- a/arch/x86/include/asm/disabled-features.h
++++ b/arch/x86/include/asm/disabled-features.h
+@@ -56,6 +56,12 @@
+ # define DISABLE_PTI		(1 << (X86_FEATURE_PTI & 31))
+ #endif
+ 
++#ifdef CONFIG_X86_INTEL_SHADOW_STACK_USER
++#define DISABLE_SHSTK	0
++#else
++#define DISABLE_SHSTK	(1<<(X86_FEATURE_SHSTK & 31))
++#endif
++
+ /*
+  * Make sure to add features to the correct mask
+  */
+@@ -75,7 +81,7 @@
+ #define DISABLED_MASK13	0
+ #define DISABLED_MASK14	0
+ #define DISABLED_MASK15	0
+-#define DISABLED_MASK16	(DISABLE_PKU|DISABLE_OSPKE|DISABLE_LA57|DISABLE_UMIP)
++#define DISABLED_MASK16	(DISABLE_PKU|DISABLE_OSPKE|DISABLE_LA57|DISABLE_UMIP|DISABLE_SHSTK)
+ #define DISABLED_MASK17	0
+ #define DISABLED_MASK18	0
+ #define DISABLED_MASK_CHECK BUILD_BUG_ON_ZERO(NCAPINTS != 19)
+diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
+index eb9536f803f9..0ccf1c7ab173 100644
+--- a/arch/x86/include/asm/processor.h
++++ b/arch/x86/include/asm/processor.h
+@@ -27,6 +27,7 @@ struct vm86;
+ #include <asm/unwind_hints.h>
+ #include <asm/vmxfeatures.h>
+ #include <asm/vdso/processor.h>
++#include <asm/cet.h>
+ 
+ #include <linux/personality.h>
+ #include <linux/cache.h>
+@@ -543,6 +544,10 @@ struct thread_struct {
+ 
+ 	unsigned int		sig_on_uaccess_err:1;
+ 
++#ifdef CONFIG_X86_INTEL_CET
++	struct cet_status	cet;
++#endif
++
+ 	/* Floating point and extended processor state */
+ 	struct fpu		fpu;
+ 	/*
+diff --git a/arch/x86/kernel/Makefile b/arch/x86/kernel/Makefile
+index ba89cabe5fcf..e9cc2551573b 100644
+--- a/arch/x86/kernel/Makefile
++++ b/arch/x86/kernel/Makefile
+@@ -144,6 +144,8 @@ obj-$(CONFIG_UNWINDER_ORC)		+= unwind_orc.o
+ obj-$(CONFIG_UNWINDER_FRAME_POINTER)	+= unwind_frame.o
+ obj-$(CONFIG_UNWINDER_GUESS)		+= unwind_guess.o
+ 
++obj-$(CONFIG_X86_INTEL_CET)		+= cet.o
++
+ ###
+ # 64 bit specific files
+ ifeq ($(CONFIG_X86_64),y)
+diff --git a/arch/x86/kernel/cet.c b/arch/x86/kernel/cet.c
+new file mode 100644
+index 000000000000..d8196c8e792a
+--- /dev/null
++++ b/arch/x86/kernel/cet.c
+@@ -0,0 +1,135 @@
++/* SPDX-License-Identifier: GPL-2.0 */
++/*
++ * cet.c - Control-flow Enforcement (CET)
++ *
++ * Copyright (c) 2019, Intel Corporation.
++ * Yu-cheng Yu <yu-cheng.yu@intel.com>
++ */
++
++#include <linux/types.h>
++#include <linux/mm.h>
++#include <linux/mman.h>
++#include <linux/slab.h>
++#include <linux/uaccess.h>
++#include <linux/sched/signal.h>
++#include <linux/compat.h>
++#include <asm/msr.h>
++#include <asm/user.h>
++#include <asm/fpu/internal.h>
++#include <asm/fpu/xstate.h>
++#include <asm/fpu/types.h>
++#include <asm/cet.h>
++
++static void start_update_msrs(void)
++{
++	fpregs_lock();
++	if (test_thread_flag(TIF_NEED_FPU_LOAD))
++		__fpregs_load_activate();
++}
++
++static void end_update_msrs(void)
++{
++	fpregs_unlock();
++}
++
++static unsigned long cet_get_shstk_addr(void)
++{
++	struct fpu *fpu = &current->thread.fpu;
++	unsigned long ssp = 0;
++
++	fpregs_lock();
++
++	if (fpregs_state_valid(fpu, smp_processor_id())) {
++		rdmsrl(MSR_IA32_PL3_SSP, ssp);
++	} else {
++		struct cet_user_state *p;
++
++		p = get_xsave_addr(&fpu->state.xsave, XFEATURE_CET_USER);
++		if (p)
++			ssp = p->user_ssp;
++	}
++
++	fpregs_unlock();
++	return ssp;
++}
++
++static unsigned long alloc_shstk(unsigned long size)
++{
++	struct mm_struct *mm = current->mm;
++	unsigned long addr, populate;
++
++	down_write(&mm->mmap_sem);
++	addr = do_mmap(NULL, 0, size, PROT_READ, MAP_ANONYMOUS | MAP_PRIVATE,
++		       VM_SHSTK, 0, &populate, NULL);
++	up_write(&mm->mmap_sem);
++
++	if (populate)
++		mm_populate(addr, populate);
++
++	return addr;
++}
++
++int cet_setup_shstk(void)
++{
++	unsigned long addr, size;
++	struct cet_status *cet = &current->thread.cet;
++
++	if (!static_cpu_has(X86_FEATURE_SHSTK))
++		return -EOPNOTSUPP;
++
++	size = round_up(min(rlimit(RLIMIT_STACK), 1UL << 32), PAGE_SIZE);
++	addr = alloc_shstk(size);
++
++	if (IS_ERR((void *)addr))
++		return PTR_ERR((void *)addr);
++
++	cet->shstk_base = addr;
++	cet->shstk_size = size;
++
++	start_update_msrs();
++	wrmsrl(MSR_IA32_PL3_SSP, addr + size);
++	wrmsrl(MSR_IA32_U_CET, MSR_IA32_CET_SHSTK_EN);
++	end_update_msrs();
++	return 0;
++}
++
++void cet_disable_free_shstk(struct task_struct *tsk)
++{
++	struct cet_status *cet = &tsk->thread.cet;
++
++	if (!static_cpu_has(X86_FEATURE_SHSTK) ||
++	    !cet->shstk_size || !cet->shstk_base)
++		return;
++
++	if (!tsk->mm || (tsk->mm != current->mm))
++		return;
++
++	if (tsk == current) {
++		u64 msr_val;
++
++		start_update_msrs();
++		rdmsrl(MSR_IA32_U_CET, msr_val);
++		wrmsrl(MSR_IA32_U_CET, msr_val & ~MSR_IA32_CET_SHSTK_EN);
++		wrmsrl(MSR_IA32_PL3_SSP, 0);
++		end_update_msrs();
++	}
++
++	while (1) {
++		int r;
++
++		r = vm_munmap(cet->shstk_base, cet->shstk_size);
++
++		/*
++		 * Retry if mmap_sem is not available.
++		 */
++		if (r == -EINTR) {
++			cond_resched();
++			continue;
++		}
++
++		WARN_ON_ONCE(r);
++		break;
++	}
++	cet->shstk_base = 0;
++	cet->shstk_size = 0;
++}
+diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
+index bed0cb83fe24..1563b472e0f9 100644
+--- a/arch/x86/kernel/cpu/common.c
++++ b/arch/x86/kernel/cpu/common.c
+@@ -55,6 +55,7 @@
+ #include <asm/microcode_intel.h>
+ #include <asm/intel-family.h>
+ #include <asm/cpu_device_id.h>
++#include <asm/cet.h>
+ #include <asm/uv/uv.h>
+ 
+ #include "cpu.h"
+@@ -469,6 +470,32 @@ static __init int setup_disable_pku(char *arg)
+ __setup("nopku", setup_disable_pku);
+ #endif /* CONFIG_X86_64 */
+ 
++static __always_inline void setup_cet(struct cpuinfo_x86 *c)
++{
++	if (!cpu_feature_enabled(X86_FEATURE_SHSTK) &&
++	    !cpu_feature_enabled(X86_FEATURE_IBT))
++		return;
++
++	cr4_set_bits(X86_CR4_CET);
++}
++
++#ifdef CONFIG_X86_INTEL_SHADOW_STACK_USER
++static __init int setup_disable_shstk(char *s)
++{
++	/* require an exact match without trailing characters */
++	if (s[0] != '\0')
++		return 0;
++
++	if (!boot_cpu_has(X86_FEATURE_SHSTK))
++		return 1;
++
++	setup_clear_cpu_cap(X86_FEATURE_SHSTK);
++	pr_info("x86: 'no_user_shstk' specified, disabling user Shadow Stack\n");
++	return 1;
++}
++__setup("no_user_shstk", setup_disable_shstk);
++#endif
++
+ /*
+  * Some CPU features depend on higher CPUID levels, which may not always
+  * be available due to CPUID level capping or broken virtualization
+@@ -1505,6 +1532,7 @@ static void identify_cpu(struct cpuinfo_x86 *c)
+ 	x86_init_rdrand(c);
+ 	x86_init_cache_qos(c);
+ 	setup_pku(c);
++	setup_cet(c);
+ 
+ 	/*
+ 	 * Clear/Set all flags overridden by options, need do it
+diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
+index ce6cd220f722..56587051df5a 100644
+--- a/arch/x86/kernel/process.c
++++ b/arch/x86/kernel/process.c
+@@ -42,6 +42,7 @@
+ #include <asm/spec-ctrl.h>
+ #include <asm/io_bitmap.h>
+ #include <asm/proto.h>
++#include <asm/cet.h>
+ 
+ #include "process.h"
+ 
+diff --git a/tools/arch/x86/include/asm/disabled-features.h b/tools/arch/x86/include/asm/disabled-features.h
+index 4ea8584682f9..a0e1b24cfa02 100644
+--- a/tools/arch/x86/include/asm/disabled-features.h
++++ b/tools/arch/x86/include/asm/disabled-features.h
+@@ -56,6 +56,12 @@
+ # define DISABLE_PTI		(1 << (X86_FEATURE_PTI & 31))
+ #endif
+ 
++#ifdef CONFIG_X86_INTEL_SHADOW_STACK_USER
++#define DISABLE_SHSTK	0
++#else
++#define DISABLE_SHSTK	(1<<(X86_FEATURE_SHSTK & 31))
++#endif
++
+ /*
+  * Make sure to add features to the correct mask
+  */
+@@ -75,7 +81,7 @@
+ #define DISABLED_MASK13	0
+ #define DISABLED_MASK14	0
+ #define DISABLED_MASK15	0
+-#define DISABLED_MASK16	(DISABLE_PKU|DISABLE_OSPKE|DISABLE_LA57|DISABLE_UMIP)
++#define DISABLED_MASK16	(DISABLE_PKU|DISABLE_OSPKE|DISABLE_LA57|DISABLE_UMIP|DISABLE_SHSTK)
+ #define DISABLED_MASK17	0
+ #define DISABLED_MASK18	0
+ #define DISABLED_MASK_CHECK BUILD_BUG_ON_ZERO(NCAPINTS != 19)
+-- 
+2.26.2
+
diff --git a/0030-x86-cet-shstk-Handle-signals-for-shadow-stack.patch b/0030-x86-cet-shstk-Handle-signals-for-shadow-stack.patch
new file mode 100644
index 000000000..462cfa92a
--- /dev/null
+++ b/0030-x86-cet-shstk-Handle-signals-for-shadow-stack.patch
@@ -0,0 +1,569 @@
+From b30f0fddbdd84f37040474a7e07c35b13d3e3158 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Thu, 5 Jan 2017 13:48:31 -0800
+Subject: [PATCH 30/47] x86/cet/shstk: Handle signals for shadow stack
+
+To deliver a signal, create a shadow stack restore token and put a restore
+token and the signal restorer address on the shadow stack.  For sigreturn,
+verify the token and restore the shadow stack pointer.
+
+Introduce WRUSS, which is a kernel-mode instruction but writes directly to
+user shadow stack.  It is used to construct the user signal stack as
+described above.
+
+Introduce a signal context extension struct 'sc_ext', which is used to save
+shadow stack restore token address and WAIT_ENDBR status.  WAIT_ENDBR will
+be introduced later in the Indirect Branch Tracking (IBT) series, but add
+that into sc_ext now to keep the struct stable in case the IBT series is
+applied later.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+
+v10:
+- Combine with WRUSS instruction patch, since it is used only here.
+- Revise signal restore code to the latest supervisor states handling.
+  Move shadow stack restore token checking out of the fast path.
+
+v9:
+- Update CET MSR access according to XSAVES supervisor state changes.
+- Add 'wait_endbr' to struct 'sc_ext'.
+- Update and simplify signal frame allocation, setup, and restoration.
+- Update commit log text.
+
+v2:
+- Move CET status from sigcontext to a separate struct sc_ext, which is
+  located above the fpstate on the signal frame.
+- Add a restore token for sigreturn address.
+---
+ arch/x86/ia32/ia32_signal.c            |  17 +++
+ arch/x86/include/asm/cet.h             |   8 ++
+ arch/x86/include/asm/fpu/internal.h    |  10 ++
+ arch/x86/include/asm/special_insns.h   |  32 +++++
+ arch/x86/include/uapi/asm/sigcontext.h |   9 ++
+ arch/x86/kernel/cet.c                  | 154 +++++++++++++++++++++++++
+ arch/x86/kernel/fpu/signal.c           | 100 ++++++++++++++++
+ arch/x86/kernel/signal.c               |  10 ++
+ 8 files changed, 340 insertions(+)
+
+diff --git a/arch/x86/ia32/ia32_signal.c b/arch/x86/ia32/ia32_signal.c
+index f9d8804144d0..cb19159817cb 100644
+--- a/arch/x86/ia32/ia32_signal.c
++++ b/arch/x86/ia32/ia32_signal.c
+@@ -35,6 +35,7 @@
+ #include <asm/sigframe.h>
+ #include <asm/sighandling.h>
+ #include <asm/smap.h>
++#include <asm/cet.h>
+ 
+ static inline void reload_segments(struct sigcontext_32 *sc)
+ {
+@@ -205,6 +206,7 @@ static void __user *get_sigframe(struct ksignal *ksig, struct pt_regs *regs,
+ 				 void __user **fpstate)
+ {
+ 	unsigned long sp, fx_aligned, math_size;
++	void __user *restorer = NULL;
+ 
+ 	/* Default to using normal stack */
+ 	sp = regs->sp;
+@@ -218,8 +220,23 @@ static void __user *get_sigframe(struct ksignal *ksig, struct pt_regs *regs,
+ 		 ksig->ka.sa.sa_restorer)
+ 		sp = (unsigned long) ksig->ka.sa.sa_restorer;
+ 
++	if (ksig->ka.sa.sa_flags & SA_RESTORER) {
++		restorer = ksig->ka.sa.sa_restorer;
++	} else if (current->mm->context.vdso) {
++		if (ksig->ka.sa.sa_flags & SA_SIGINFO)
++			restorer = current->mm->context.vdso +
++				vdso_image_32.sym___kernel_rt_sigreturn;
++		else
++			restorer = current->mm->context.vdso +
++				vdso_image_32.sym___kernel_sigreturn;
++	}
++
+ 	sp = fpu__alloc_mathframe(sp, 1, &fx_aligned, &math_size);
+ 	*fpstate = (struct _fpstate_32 __user *) sp;
++
++	if (save_cet_to_sigframe(1, *fpstate, (unsigned long)restorer))
++		return (void __user *) -1L;
++
+ 	if (copy_fpstate_to_sigframe(*fpstate, (void __user *)fx_aligned,
+ 				     math_size) < 0)
+ 		return (void __user *) -1L;
+diff --git a/arch/x86/include/asm/cet.h b/arch/x86/include/asm/cet.h
+index caac0687c8e4..56fe08eebae6 100644
+--- a/arch/x86/include/asm/cet.h
++++ b/arch/x86/include/asm/cet.h
+@@ -6,6 +6,8 @@
+ #include <linux/types.h>
+ 
+ struct task_struct;
++struct sc_ext;
++
+ /*
+  * Per-thread CET status
+  */
+@@ -17,8 +19,14 @@ struct cet_status {
+ #ifdef CONFIG_X86_INTEL_CET
+ int cet_setup_shstk(void);
+ void cet_disable_free_shstk(struct task_struct *p);
++int cet_verify_rstor_token(bool ia32, unsigned long ssp, unsigned long *new_ssp);
++void cet_restore_signal(struct sc_ext *sc);
++int cet_setup_signal(bool ia32, unsigned long rstor, struct sc_ext *sc);
+ #else
+ static inline void cet_disable_free_shstk(struct task_struct *p) {}
++static inline void cet_restore_signal(struct sc_ext *sc) { return; }
++static inline int cet_setup_signal(bool ia32, unsigned long rstor,
++				   struct sc_ext *sc) { return -EINVAL; }
+ #endif
+ 
+ #endif /* __ASSEMBLY__ */
+diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
+index 42159f45bf9c..b569ac929ccc 100644
+--- a/arch/x86/include/asm/fpu/internal.h
++++ b/arch/x86/include/asm/fpu/internal.h
+@@ -476,6 +476,16 @@ static inline void copy_kernel_to_fpregs(union fpregs_state *fpstate)
+ 	__copy_kernel_to_fpregs(fpstate, -1);
+ }
+ 
++#ifdef CONFIG_X86_INTEL_CET
++extern int save_cet_to_sigframe(int ia32, void __user *fp,
++				unsigned long restorer);
++#else
++static inline int save_cet_to_sigframe(int ia32, void __user *fp,
++				unsigned long restorer)
++{
++	return 0;
++}
++#endif
+ extern int copy_fpstate_to_sigframe(void __user *buf, void __user *fp, int size);
+ 
+ /*
+diff --git a/arch/x86/include/asm/special_insns.h b/arch/x86/include/asm/special_insns.h
+index 6d37b8fcfc77..1b9b2e79c353 100644
+--- a/arch/x86/include/asm/special_insns.h
++++ b/arch/x86/include/asm/special_insns.h
+@@ -222,6 +222,38 @@ static inline void clwb(volatile void *__p)
+ 		: [pax] "a" (p));
+ }
+ 
++#ifdef CONFIG_X86_INTEL_CET
++#if defined(CONFIG_IA32_EMULATION) || defined(CONFIG_X86_X32)
++static inline int write_user_shstk_32(unsigned long addr, unsigned int val)
++{
++	asm_volatile_goto("1: wrussd %1, (%0)\n"
++			  _ASM_EXTABLE(1b, %l[fail])
++			  :: "r" (addr), "r" (val)
++			  :: fail);
++	return 0;
++fail:
++	return -EPERM;
++}
++#else
++static inline int write_user_shstk_32(unsigned long addr, unsigned int val)
++{
++	WARN_ONCE(1, "%s used but not supported.\n", __func__);
++	return -EFAULT;
++}
++#endif
++
++static inline int write_user_shstk_64(unsigned long addr, unsigned long val)
++{
++	asm_volatile_goto("1: wrussq %1, (%0)\n"
++			  _ASM_EXTABLE(1b, %l[fail])
++			  :: "r" (addr), "r" (val)
++			  :: fail);
++	return 0;
++fail:
++	return -EPERM;
++}
++#endif /* CONFIG_X86_INTEL_CET */
++
+ #define nop() asm volatile ("nop")
+ 
+ 
+diff --git a/arch/x86/include/uapi/asm/sigcontext.h b/arch/x86/include/uapi/asm/sigcontext.h
+index 844d60eb1882..cf2d55db3be4 100644
+--- a/arch/x86/include/uapi/asm/sigcontext.h
++++ b/arch/x86/include/uapi/asm/sigcontext.h
+@@ -196,6 +196,15 @@ struct _xstate {
+ 	/* New processor state extensions go here: */
+ };
+ 
++/*
++ * Located at the end of sigcontext->fpstate, aligned to 8.
++ */
++struct sc_ext {
++	unsigned long total_size;
++	unsigned long ssp;
++	unsigned long wait_endbr;
++};
++
+ /*
+  * The 32-bit signal frame:
+  */
+diff --git a/arch/x86/kernel/cet.c b/arch/x86/kernel/cet.c
+index d8196c8e792a..e95bd2b3c394 100644
+--- a/arch/x86/kernel/cet.c
++++ b/arch/x86/kernel/cet.c
+@@ -19,6 +19,8 @@
+ #include <asm/fpu/xstate.h>
+ #include <asm/fpu/types.h>
+ #include <asm/cet.h>
++#include <asm/special_insns.h>
++#include <uapi/asm/sigcontext.h>
+ 
+ static void start_update_msrs(void)
+ {
+@@ -69,6 +71,80 @@ static unsigned long alloc_shstk(unsigned long size)
+ 	return addr;
+ }
+ 
++#define TOKEN_MODE_MASK	3UL
++#define TOKEN_MODE_64	1UL
++#define IS_TOKEN_64(token) ((token & TOKEN_MODE_MASK) == TOKEN_MODE_64)
++#define IS_TOKEN_32(token) ((token & TOKEN_MODE_MASK) == 0)
++
++/*
++ * Verify the restore token at the address of 'ssp' is
++ * valid and then set shadow stack pointer according to the
++ * token.
++ */
++int cet_verify_rstor_token(bool ia32, unsigned long ssp,
++			   unsigned long *new_ssp)
++{
++	unsigned long token;
++
++	*new_ssp = 0;
++
++	if (!IS_ALIGNED(ssp, 8))
++		return -EINVAL;
++
++	if (get_user(token, (unsigned long __user *)ssp))
++		return -EFAULT;
++
++	/* Is 64-bit mode flag correct? */
++	if (!ia32 && !IS_TOKEN_64(token))
++		return -EINVAL;
++	else if (ia32 && !IS_TOKEN_32(token))
++		return -EINVAL;
++
++	token &= ~TOKEN_MODE_MASK;
++
++	/*
++	 * Restore address properly aligned?
++	 */
++	if ((!ia32 && !IS_ALIGNED(token, 8)) || !IS_ALIGNED(token, 4))
++		return -EINVAL;
++
++	/*
++	 * Token was placed properly?
++	 */
++	if ((ALIGN_DOWN(token, 8) - 8) != ssp)
++		return -EINVAL;
++
++	*new_ssp = token;
++	return 0;
++}
++
++/*
++ * Create a restore token on the shadow stack.
++ * A token is always 8-byte and aligned to 8.
++ */
++static int create_rstor_token(bool ia32, unsigned long ssp,
++			      unsigned long *new_ssp)
++{
++	unsigned long addr;
++
++	*new_ssp = 0;
++
++	if ((!ia32 && !IS_ALIGNED(ssp, 8)) || !IS_ALIGNED(ssp, 4))
++		return -EINVAL;
++
++	addr = ALIGN_DOWN(ssp, 8) - 8;
++
++	/* Is the token for 64-bit? */
++	if (!ia32)
++		ssp |= TOKEN_MODE_64;
++
++	if (write_user_shstk_64(addr, ssp))
++		return -EFAULT;
++
++	*new_ssp = addr;
++	return 0;
++}
++
+ int cet_setup_shstk(void)
+ {
+ 	unsigned long addr, size;
+@@ -133,3 +209,81 @@ void cet_disable_free_shstk(struct task_struct *tsk)
+ 	cet->shstk_base = 0;
+ 	cet->shstk_size = 0;
+ }
++
++/*
++ * Called from __fpu__restore_sig() and XSAVES buffer is protected by
++ * set_thread_flag(TIF_NEED_FPU_LOAD) in the slow path.
++ */
++void cet_restore_signal(struct sc_ext *sc_ext)
++{
++	struct cet_user_state *cet_user_state;
++	struct cet_status *cet = &current->thread.cet;
++	u64 msr_val = 0;
++
++	if (!static_cpu_has(X86_FEATURE_SHSTK))
++		return;
++
++	cet_user_state = get_xsave_addr(&current->thread.fpu.state.xsave,
++					XFEATURE_CET_USER);
++	if (!cet_user_state)
++		return;
++
++	if (cet->shstk_size) {
++		if (test_thread_flag(TIF_NEED_FPU_LOAD))
++			cet_user_state->user_ssp = sc_ext->ssp;
++		else
++			wrmsrl(MSR_IA32_PL3_SSP, sc_ext->ssp);
++
++		msr_val |= MSR_IA32_CET_SHSTK_EN;
++	}
++
++	if (test_thread_flag(TIF_NEED_FPU_LOAD))
++		cet_user_state->user_cet = msr_val;
++	else
++		wrmsrl(MSR_IA32_U_CET, msr_val);
++
++	return;
++}
++
++/*
++ * Setup the shadow stack for the signal handler: first,
++ * create a restore token to keep track of the current ssp,
++ * and then the return address of the signal handler.
++ */
++int cet_setup_signal(bool ia32, unsigned long rstor_addr, struct sc_ext *sc_ext)
++{
++	struct cet_status *cet = &current->thread.cet;
++	unsigned long ssp = 0, new_ssp = 0;
++	int err;
++
++	if (cet->shstk_size) {
++		if (!rstor_addr)
++			return -EINVAL;
++
++		ssp = cet_get_shstk_addr();
++		err = create_rstor_token(ia32, ssp, &new_ssp);
++		if (err)
++			return err;
++
++		if (ia32) {
++			ssp = new_ssp - sizeof(u32);
++			err = write_user_shstk_32(ssp, (unsigned int)rstor_addr);
++		} else {
++			ssp = new_ssp - sizeof(u64);
++			err = write_user_shstk_64(ssp, rstor_addr);
++		}
++
++		if (err)
++			return err;
++
++		sc_ext->ssp = new_ssp;
++	}
++
++	if (ssp) {
++		start_update_msrs();
++		wrmsrl(MSR_IA32_PL3_SSP, ssp);
++		end_update_msrs();
++	}
++
++	return 0;
++}
+diff --git a/arch/x86/kernel/fpu/signal.c b/arch/x86/kernel/fpu/signal.c
+index 003735eec674..163a17772ca0 100644
+--- a/arch/x86/kernel/fpu/signal.c
++++ b/arch/x86/kernel/fpu/signal.c
+@@ -52,6 +52,74 @@ static inline int check_for_xstate(struct fxregs_state __user *buf,
+ 	return 0;
+ }
+ 
++#ifdef CONFIG_X86_INTEL_CET
++int save_cet_to_sigframe(int ia32, void __user *fp, unsigned long restorer)
++{
++	int err = 0;
++
++	if (!current->thread.cet.shstk_size)
++		return 0;
++
++	if (fp) {
++		struct sc_ext ext = {0, 0, 0};
++
++		err = cet_setup_signal(ia32, restorer, &ext);
++		if (!err) {
++			void __user *p = fp;
++
++			ext.total_size = sizeof(ext);
++
++			if (ia32)
++				p += sizeof(struct fregs_state);
++
++			p += fpu_user_xstate_size + FP_XSTATE_MAGIC2_SIZE;
++			p = (void __user *)ALIGN((unsigned long)p, 8);
++
++			if (copy_to_user(p, &ext, sizeof(ext)))
++				return -EFAULT;
++		}
++	}
++
++	return err;
++}
++
++static int get_cet_from_sigframe(int ia32, void __user *fp, struct sc_ext *ext)
++{
++	int err = 0;
++
++	memset(ext, 0, sizeof(*ext));
++
++	if (!current->thread.cet.shstk_size)
++		return 0;
++
++	if (fp) {
++		void __user *p = fp;
++
++		if (ia32)
++			p += sizeof(struct fregs_state);
++
++		p += fpu_user_xstate_size + FP_XSTATE_MAGIC2_SIZE;
++		p = (void __user *)ALIGN((unsigned long)p, 8);
++
++		if (copy_from_user(ext, p, sizeof(*ext)))
++			return -EFAULT;
++
++		if (ext->total_size != sizeof(*ext))
++			return -EFAULT;
++
++		if (current->thread.cet.shstk_size)
++			err = cet_verify_rstor_token(ia32, ext->ssp, &ext->ssp);
++	}
++
++	return err;
++}
++#else
++static int get_cet_from_sigframe(int ia32, void __user *fp, struct sc_ext *ext)
++{
++	return 0;
++}
++#endif
++
+ /*
+  * Signal frame handlers.
+  */
+@@ -294,6 +362,7 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 	struct task_struct *tsk = current;
+ 	struct fpu *fpu = &tsk->thread.fpu;
+ 	struct user_i387_ia32_struct env;
++	struct sc_ext sc_ext;
+ 	u64 user_xfeatures = 0;
+ 	int fx_only = 0;
+ 	int ret = 0;
+@@ -334,6 +403,10 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 	if ((unsigned long)buf_fx % 64)
+ 		fx_only = 1;
+ 
++	ret = get_cet_from_sigframe(ia32_fxstate, buf, &sc_ext);
++	if (ret)
++		return ret;
++
+ 	if (!ia32_fxstate) {
+ 		/*
+ 		 * Attempt to restore the FPU registers directly from user
+@@ -348,6 +421,8 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 		pagefault_enable();
+ 		if (!ret) {
+ 
++			cet_restore_signal(&sc_ext);
++
+ 			/*
+ 			 * Restore supervisor states: previous context switch
+ 			 * etc has done XSAVES and saved the supervisor states
+@@ -422,6 +497,8 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 		if (unlikely(init_bv))
+ 			copy_kernel_to_xregs(&init_fpstate.xsave, init_bv);
+ 
++		cet_restore_signal(&sc_ext);
++
+ 		/*
+ 		 * Restore previously saved supervisor xstates along with
+ 		 * copied-in user xstates.
+@@ -490,12 +567,35 @@ int fpu__restore_sig(void __user *buf, int ia32_frame)
+ 	return __fpu__restore_sig(buf, buf_fx, size);
+ }
+ 
++#ifdef CONFIG_X86_INTEL_CET
++static unsigned long fpu__alloc_sigcontext_ext(unsigned long sp)
++{
++	struct cet_status *cet = &current->thread.cet;
++
++	/*
++	 * sigcontext_ext is at: fpu + fpu_user_xstate_size +
++	 * FP_XSTATE_MAGIC2_SIZE, then aligned to 8.
++	 */
++	if (cet->shstk_size)
++		sp -= (sizeof(struct sc_ext) + 8);
++
++	return sp;
++}
++#else
++static unsigned long fpu__alloc_sigcontext_ext(unsigned long sp)
++{
++	return sp;
++}
++#endif
++
+ unsigned long
+ fpu__alloc_mathframe(unsigned long sp, int ia32_frame,
+ 		     unsigned long *buf_fx, unsigned long *size)
+ {
+ 	unsigned long frame_size = xstate_sigframe_size();
+ 
++	sp = fpu__alloc_sigcontext_ext(sp);
++
+ 	*buf_fx = sp = round_down(sp - frame_size, 64);
+ 	if (ia32_frame && use_fxsr()) {
+ 		frame_size += sizeof(struct fregs_state);
+diff --git a/arch/x86/kernel/signal.c b/arch/x86/kernel/signal.c
+index 0052bbe5dfd4..5ee1b2e51de3 100644
+--- a/arch/x86/kernel/signal.c
++++ b/arch/x86/kernel/signal.c
+@@ -44,6 +44,7 @@
+ #include <asm/syscall.h>
+ #include <asm/sigframe.h>
+ #include <asm/signal.h>
++#include <asm/cet.h>
+ 
+ #ifdef CONFIG_X86_64
+ /*
+@@ -237,6 +238,9 @@ get_sigframe(struct k_sigaction *ka, struct pt_regs *regs, size_t frame_size,
+ 	unsigned long buf_fx = 0;
+ 	int onsigstack = on_sig_stack(sp);
+ 	int ret;
++#ifdef CONFIG_X86_64
++	void __user *restorer = NULL;
++#endif
+ 
+ 	/* redzone */
+ 	if (IS_ENABLED(CONFIG_X86_64))
+@@ -268,6 +272,12 @@ get_sigframe(struct k_sigaction *ka, struct pt_regs *regs, size_t frame_size,
+ 	if (onsigstack && !likely(on_sig_stack(sp)))
+ 		return (void __user *)-1L;
+ 
++#ifdef CONFIG_X86_64
++	if (ka->sa.sa_flags & SA_RESTORER)
++		restorer = ka->sa.sa_restorer;
++	ret = save_cet_to_sigframe(0, *fpstate, (unsigned long)restorer);
++#endif
++
+ 	/* save i387 and extended state */
+ 	ret = copy_fpstate_to_sigframe(*fpstate, (void __user *)buf_fx, math_size);
+ 	if (ret < 0)
+-- 
+2.26.2
+
diff --git a/0031-ELF-UAPI-and-Kconfig-additions-for-ELF-program-prope.patch b/0031-ELF-UAPI-and-Kconfig-additions-for-ELF-program-prope.patch
new file mode 100644
index 000000000..ac4f75a34
--- /dev/null
+++ b/0031-ELF-UAPI-and-Kconfig-additions-for-ELF-program-prope.patch
@@ -0,0 +1,87 @@
+From b917d33bc0bb3db931d0c6a70cb446665483a755 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Fri, 18 Oct 2019 18:25:34 +0100
+Subject: [PATCH 31/47] ELF: UAPI and Kconfig additions for ELF program
+ properties
+
+Introduce basic ELF definitions relating to the NT_GNU_PROPERTY_TYPE_0
+note.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+
+v10:
+- Merge GNU_PROPERTY_X86_FEATURE_1_* from a separate patch.
+---
+ fs/Kconfig.binfmt        |  3 +++
+ include/linux/elf.h      |  8 ++++++++
+ include/uapi/linux/elf.h | 10 ++++++++++
+ 3 files changed, 21 insertions(+)
+
+diff --git a/fs/Kconfig.binfmt b/fs/Kconfig.binfmt
+index 62dc4f577ba1..d2cfe0729a73 100644
+--- a/fs/Kconfig.binfmt
++++ b/fs/Kconfig.binfmt
+@@ -36,6 +36,9 @@ config COMPAT_BINFMT_ELF
+ config ARCH_BINFMT_ELF_STATE
+ 	bool
+ 
++config ARCH_USE_GNU_PROPERTY
++	bool
++
+ config BINFMT_ELF_FDPIC
+ 	bool "Kernel support for FDPIC ELF binaries"
+ 	default y if !BINFMT_ELF
+diff --git a/include/linux/elf.h b/include/linux/elf.h
+index e3649b3e970e..459cddcceaac 100644
+--- a/include/linux/elf.h
++++ b/include/linux/elf.h
+@@ -2,6 +2,7 @@
+ #ifndef _LINUX_ELF_H
+ #define _LINUX_ELF_H
+ 
++#include <linux/types.h>
+ #include <asm/elf.h>
+ #include <uapi/linux/elf.h>
+ 
+@@ -56,4 +57,11 @@ static inline int elf_coredump_extra_notes_write(struct coredump_params *cprm) {
+ extern int elf_coredump_extra_notes_size(void);
+ extern int elf_coredump_extra_notes_write(struct coredump_params *cprm);
+ #endif
++
++/* NT_GNU_PROPERTY_TYPE_0 header */
++struct gnu_property {
++	u32 pr_type;
++	u32 pr_datasz;
++};
++
+ #endif /* _LINUX_ELF_H */
+diff --git a/include/uapi/linux/elf.h b/include/uapi/linux/elf.h
+index 34c02e4290fe..509fd27c6f93 100644
+--- a/include/uapi/linux/elf.h
++++ b/include/uapi/linux/elf.h
+@@ -36,6 +36,7 @@ typedef __s64	Elf64_Sxword;
+ #define PT_LOPROC  0x70000000
+ #define PT_HIPROC  0x7fffffff
+ #define PT_GNU_EH_FRAME		0x6474e550
++#define PT_GNU_PROPERTY		0x6474e553
+ 
+ #define PT_GNU_STACK	(PT_LOOS + 0x474e551)
+ 
+@@ -443,4 +444,13 @@ typedef struct elf64_note {
+   Elf64_Word n_type;	/* Content type */
+ } Elf64_Nhdr;
+ 
++/* .note.gnu.property types */
++#define GNU_PROPERTY_X86_FEATURE_1_AND		0xc0000002
++
++/* Bits of GNU_PROPERTY_X86_FEATURE_1_AND */
++#define GNU_PROPERTY_X86_FEATURE_1_IBT		0x00000001
++#define GNU_PROPERTY_X86_FEATURE_1_SHSTK	0x00000002
++#define GNU_PROPERTY_X86_FEATURE_1_INVAL ~(GNU_PROPERTY_X86_FEATURE_1_IBT | \
++					    GNU_PROPERTY_X86_FEATURE_1_SHSTK)
++
+ #endif /* _UAPI_LINUX_ELF_H */
+-- 
+2.26.2
+
diff --git a/0032-ELF-Add-ELF-program-property-parsing-support.patch b/0032-ELF-Add-ELF-program-property-parsing-support.patch
new file mode 100644
index 000000000..e34da1aa9
--- /dev/null
+++ b/0032-ELF-Add-ELF-program-property-parsing-support.patch
@@ -0,0 +1,310 @@
+From 4a1799148638787d64eaeb67482794034bfe2647 Mon Sep 17 00:00:00 2001
+From: Dave Martin <Dave.Martin@arm.com>
+Date: Mon, 16 Mar 2020 16:50:44 +0000
+Subject: [PATCH 32/47] ELF: Add ELF program property parsing support
+
+ELF program properties will be needed for detecting whether to
+enable optional architecture or ABI features for a new ELF process.
+
+For now, there are no generic properties that we care about, so do
+nothing unless CONFIG_ARCH_USE_GNU_PROPERTY=y.
+
+Otherwise, the presence of properties using the PT_PROGRAM_PROPERTY
+phdrs entry (if any), and notify each property to the arch code.
+
+For now, the added code is not used.
+
+Signed-off-by: Dave Martin <Dave.Martin@arm.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+Signed-off-by: Mark Brown <broonie@kernel.org>
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ fs/binfmt_elf.c          | 127 +++++++++++++++++++++++++++++++++++++++
+ fs/compat_binfmt_elf.c   |   4 ++
+ include/linux/elf.h      |  19 ++++++
+ include/uapi/linux/elf.h |   4 ++
+ 4 files changed, 154 insertions(+)
+
+diff --git a/fs/binfmt_elf.c b/fs/binfmt_elf.c
+index 25d489bc9453..9eebecfcd331 100644
+--- a/fs/binfmt_elf.c
++++ b/fs/binfmt_elf.c
+@@ -40,12 +40,18 @@
+ #include <linux/sched/coredump.h>
+ #include <linux/sched/task_stack.h>
+ #include <linux/sched/cputime.h>
++#include <linux/sizes.h>
++#include <linux/types.h>
+ #include <linux/cred.h>
+ #include <linux/dax.h>
+ #include <linux/uaccess.h>
+ #include <asm/param.h>
+ #include <asm/page.h>
+ 
++#ifndef ELF_COMPAT
++#define ELF_COMPAT 0
++#endif
++
+ #ifndef user_long_t
+ #define user_long_t long
+ #endif
+@@ -682,6 +688,111 @@ static unsigned long load_elf_interp(struct elfhdr *interp_elf_ex,
+  * libraries.  There is no binary dependent code anywhere else.
+  */
+ 
++static int parse_elf_property(const char *data, size_t *off, size_t datasz,
++			      struct arch_elf_state *arch,
++			      bool have_prev_type, u32 *prev_type)
++{
++	size_t o, step;
++	const struct gnu_property *pr;
++	int ret;
++
++	if (*off == datasz)
++		return -ENOENT;
++
++	if (WARN_ON_ONCE(*off > datasz || *off % ELF_GNU_PROPERTY_ALIGN))
++		return -EIO;
++	o = *off;
++	datasz -= *off;
++
++	if (datasz < sizeof(*pr))
++		return -ENOEXEC;
++	pr = (const struct gnu_property *)(data + o);
++	o += sizeof(*pr);
++	datasz -= sizeof(*pr);
++
++	if (pr->pr_datasz > datasz)
++		return -ENOEXEC;
++
++	WARN_ON_ONCE(o % ELF_GNU_PROPERTY_ALIGN);
++	step = round_up(pr->pr_datasz, ELF_GNU_PROPERTY_ALIGN);
++	if (step > datasz)
++		return -ENOEXEC;
++
++	/* Properties are supposed to be unique and sorted on pr_type: */
++	if (have_prev_type && pr->pr_type <= *prev_type)
++		return -ENOEXEC;
++	*prev_type = pr->pr_type;
++
++	ret = arch_parse_elf_property(pr->pr_type, data + o,
++				      pr->pr_datasz, ELF_COMPAT, arch);
++	if (ret)
++		return ret;
++
++	*off = o + step;
++	return 0;
++}
++
++#define NOTE_DATA_SZ SZ_1K
++#define GNU_PROPERTY_TYPE_0_NAME "GNU"
++#define NOTE_NAME_SZ (sizeof(GNU_PROPERTY_TYPE_0_NAME))
++
++static int parse_elf_properties(struct file *f, const struct elf_phdr *phdr,
++				struct arch_elf_state *arch)
++{
++	union {
++		struct elf_note nhdr;
++		char data[NOTE_DATA_SZ];
++	} note;
++	loff_t pos;
++	ssize_t n;
++	size_t off, datasz;
++	int ret;
++	bool have_prev_type;
++	u32 prev_type;
++
++	if (!IS_ENABLED(CONFIG_ARCH_USE_GNU_PROPERTY) || !phdr)
++		return 0;
++
++	/* load_elf_binary() shouldn't call us unless this is true... */
++	if (WARN_ON_ONCE(phdr->p_type != PT_GNU_PROPERTY))
++		return -ENOEXEC;
++
++	/* If the properties are crazy large, that's too bad (for now): */
++	if (phdr->p_filesz > sizeof(note))
++		return -ENOEXEC;
++
++	pos = phdr->p_offset;
++	n = kernel_read(f, &note, phdr->p_filesz, &pos);
++
++	BUILD_BUG_ON(sizeof(note) < sizeof(note.nhdr) + NOTE_NAME_SZ);
++	if (n < 0 || n < sizeof(note.nhdr) + NOTE_NAME_SZ)
++		return -EIO;
++
++	if (note.nhdr.n_type != NT_GNU_PROPERTY_TYPE_0 ||
++	    note.nhdr.n_namesz != NOTE_NAME_SZ ||
++	    strncmp(note.data + sizeof(note.nhdr),
++		    GNU_PROPERTY_TYPE_0_NAME, n - sizeof(note.nhdr)))
++		return -ENOEXEC;
++
++	off = round_up(sizeof(note.nhdr) + NOTE_NAME_SZ,
++		       ELF_GNU_PROPERTY_ALIGN);
++	if (off > n)
++		return -ENOEXEC;
++
++	if (note.nhdr.n_descsz > n - off)
++		return -ENOEXEC;
++	datasz = off + note.nhdr.n_descsz;
++
++	have_prev_type = false;
++	do {
++		ret = parse_elf_property(note.data, &off, datasz, arch,
++					 have_prev_type, &prev_type);
++		have_prev_type = true;
++	} while (!ret);
++
++	return ret == -ENOENT ? 0 : ret;
++}
++
+ static int load_elf_binary(struct linux_binprm *bprm)
+ {
+ 	struct file *interpreter = NULL; /* to shut gcc up */
+@@ -689,6 +800,7 @@ static int load_elf_binary(struct linux_binprm *bprm)
+ 	int load_addr_set = 0;
+ 	unsigned long error;
+ 	struct elf_phdr *elf_ppnt, *elf_phdata, *interp_elf_phdata = NULL;
++	struct elf_phdr *elf_property_phdata = NULL;
+ 	unsigned long elf_bss, elf_brk;
+ 	int bss_prot = 0;
+ 	int retval, i;
+@@ -726,6 +838,11 @@ static int load_elf_binary(struct linux_binprm *bprm)
+ 	for (i = 0; i < elf_ex->e_phnum; i++, elf_ppnt++) {
+ 		char *elf_interpreter;
+ 
++		if (elf_ppnt->p_type == PT_GNU_PROPERTY) {
++			elf_property_phdata = elf_ppnt;
++			continue;
++		}
++
+ 		if (elf_ppnt->p_type != PT_INTERP)
+ 			continue;
+ 
+@@ -819,9 +936,14 @@ static int load_elf_binary(struct linux_binprm *bprm)
+ 			goto out_free_dentry;
+ 
+ 		/* Pass PT_LOPROC..PT_HIPROC headers to arch code */
++		elf_property_phdata = NULL;
+ 		elf_ppnt = interp_elf_phdata;
+ 		for (i = 0; i < interp_elf_ex->e_phnum; i++, elf_ppnt++)
+ 			switch (elf_ppnt->p_type) {
++			case PT_GNU_PROPERTY:
++				elf_property_phdata = elf_ppnt;
++				break;
++
+ 			case PT_LOPROC ... PT_HIPROC:
+ 				retval = arch_elf_pt_proc(interp_elf_ex,
+ 							  elf_ppnt, interpreter,
+@@ -832,6 +954,11 @@ static int load_elf_binary(struct linux_binprm *bprm)
+ 			}
+ 	}
+ 
++	retval = parse_elf_properties(interpreter ?: bprm->file,
++				      elf_property_phdata, &arch_state);
++	if (retval)
++		goto out_free_dentry;
++
+ 	/*
+ 	 * Allow arch code to reject the ELF at this point, whilst it's
+ 	 * still possible to return an error to the code that invoked
+diff --git a/fs/compat_binfmt_elf.c b/fs/compat_binfmt_elf.c
+index aaad4ca1217e..13a087bc816b 100644
+--- a/fs/compat_binfmt_elf.c
++++ b/fs/compat_binfmt_elf.c
+@@ -17,6 +17,8 @@
+ #include <linux/elfcore-compat.h>
+ #include <linux/time.h>
+ 
++#define ELF_COMPAT	1
++
+ /*
+  * Rename the basic ELF layout types to refer to the 32-bit class of files.
+  */
+@@ -28,11 +30,13 @@
+ #undef	elf_shdr
+ #undef	elf_note
+ #undef	elf_addr_t
++#undef	ELF_GNU_PROPERTY_ALIGN
+ #define elfhdr		elf32_hdr
+ #define elf_phdr	elf32_phdr
+ #define elf_shdr	elf32_shdr
+ #define elf_note	elf32_note
+ #define elf_addr_t	Elf32_Addr
++#define ELF_GNU_PROPERTY_ALIGN	ELF32_GNU_PROPERTY_ALIGN
+ 
+ /*
+  * Some data types as stored in coredump.
+diff --git a/include/linux/elf.h b/include/linux/elf.h
+index 459cddcceaac..7bdc6da160c7 100644
+--- a/include/linux/elf.h
++++ b/include/linux/elf.h
+@@ -22,6 +22,9 @@
+ 	SET_PERSONALITY(ex)
+ #endif
+ 
++#define ELF32_GNU_PROPERTY_ALIGN	4
++#define ELF64_GNU_PROPERTY_ALIGN	8
++
+ #if ELF_CLASS == ELFCLASS32
+ 
+ extern Elf32_Dyn _DYNAMIC [];
+@@ -32,6 +35,7 @@ extern Elf32_Dyn _DYNAMIC [];
+ #define elf_addr_t	Elf32_Off
+ #define Elf_Half	Elf32_Half
+ #define Elf_Word	Elf32_Word
++#define ELF_GNU_PROPERTY_ALIGN	ELF32_GNU_PROPERTY_ALIGN
+ 
+ #else
+ 
+@@ -43,6 +47,7 @@ extern Elf64_Dyn _DYNAMIC [];
+ #define elf_addr_t	Elf64_Off
+ #define Elf_Half	Elf64_Half
+ #define Elf_Word	Elf64_Word
++#define ELF_GNU_PROPERTY_ALIGN	ELF64_GNU_PROPERTY_ALIGN
+ 
+ #endif
+ 
+@@ -64,4 +69,18 @@ struct gnu_property {
+ 	u32 pr_datasz;
+ };
+ 
++struct arch_elf_state;
++
++#ifndef CONFIG_ARCH_USE_GNU_PROPERTY
++static inline int arch_parse_elf_property(u32 type, const void *data,
++					  size_t datasz, bool compat,
++					  struct arch_elf_state *arch)
++{
++	return 0;
++}
++#else
++extern int arch_parse_elf_property(u32 type, const void *data, size_t datasz,
++				   bool compat, struct arch_elf_state *arch);
++#endif
++
+ #endif /* _LINUX_ELF_H */
+diff --git a/include/uapi/linux/elf.h b/include/uapi/linux/elf.h
+index 509fd27c6f93..49c026ce8d97 100644
+--- a/include/uapi/linux/elf.h
++++ b/include/uapi/linux/elf.h
+@@ -368,6 +368,7 @@ typedef struct elf64_shdr {
+  * Notes used in ET_CORE. Architectures export some of the arch register sets
+  * using the corresponding note types via the PTRACE_GETREGSET and
+  * PTRACE_SETREGSET requests.
++ * The note name for all these is "LINUX".
+  */
+ #define NT_PRSTATUS	1
+ #define NT_PRFPREG	2
+@@ -430,6 +431,9 @@ typedef struct elf64_shdr {
+ #define NT_MIPS_FP_MODE	0x801		/* MIPS floating-point mode */
+ #define NT_MIPS_MSA	0x802		/* MIPS SIMD registers */
+ 
++/* Note types with note name "GNU" */
++#define NT_GNU_PROPERTY_TYPE_0	5
++
+ /* Note header in a PT_NOTE section */
+ typedef struct elf32_note {
+   Elf32_Word	n_namesz;	/* Name size */
+-- 
+2.26.2
+
diff --git a/0033-ELF-Introduce-arch_setup_elf_property.patch b/0033-ELF-Introduce-arch_setup_elf_property.patch
new file mode 100644
index 000000000..71fbd5d43
--- /dev/null
+++ b/0033-ELF-Introduce-arch_setup_elf_property.patch
@@ -0,0 +1,54 @@
+From d5fa1383166189cf11d9e08e8c6ac690416f7d2c Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 20 Aug 2019 13:20:37 -0700
+Subject: [PATCH 33/47] ELF: Introduce arch_setup_elf_property()
+
+An ELF file's .note.gnu.property indicates architecture features of the
+file.  These features are extracted earlier and stored in the struct
+'arch_elf_state'.  Introduce arch_setup_elf_property() to setup and enable
+these features.  The first use-case of this function is shadow stack.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ fs/binfmt_elf.c     | 4 ++++
+ include/linux/elf.h | 6 ++++++
+ 2 files changed, 10 insertions(+)
+
+diff --git a/fs/binfmt_elf.c b/fs/binfmt_elf.c
+index 9eebecfcd331..91ae3e71e2d5 100644
+--- a/fs/binfmt_elf.c
++++ b/fs/binfmt_elf.c
+@@ -1216,6 +1216,10 @@ static int load_elf_binary(struct linux_binprm *bprm)
+ 
+ 	set_binfmt(&elf_format);
+ 
++	retval = arch_setup_elf_property(&arch_state);
++	if (retval < 0)
++		goto out;
++
+ #ifdef ARCH_HAS_SETUP_ADDITIONAL_PAGES
+ 	retval = arch_setup_additional_pages(bprm, !!interpreter);
+ 	if (retval < 0)
+diff --git a/include/linux/elf.h b/include/linux/elf.h
+index 7bdc6da160c7..81f2161fa4a8 100644
+--- a/include/linux/elf.h
++++ b/include/linux/elf.h
+@@ -78,9 +78,15 @@ static inline int arch_parse_elf_property(u32 type, const void *data,
+ {
+ 	return 0;
+ }
++
++static inline int arch_setup_elf_property(struct arch_elf_state *arch)
++{
++	return 0;
++}
+ #else
+ extern int arch_parse_elf_property(u32 type, const void *data, size_t datasz,
+ 				   bool compat, struct arch_elf_state *arch);
++extern int arch_setup_elf_property(struct arch_elf_state *arch);
+ #endif
+ 
+ #endif /* _LINUX_ELF_H */
+-- 
+2.26.2
+
diff --git a/0034-x86-cet-shstk-ELF-header-parsing-for-shadow-stack.patch b/0034-x86-cet-shstk-ELF-header-parsing-for-shadow-stack.patch
new file mode 100644
index 000000000..b03950227
--- /dev/null
+++ b/0034-x86-cet-shstk-ELF-header-parsing-for-shadow-stack.patch
@@ -0,0 +1,98 @@
+From 8c795861545891784657a459783adb1785028f7f Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 3 Oct 2017 16:07:12 -0700
+Subject: [PATCH 34/47] x86/cet/shstk: ELF header parsing for shadow stack
+
+Check an ELF file's .note.gnu.property, and setup shadow stack if the
+application supports it.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+
+v9:
+- Change cpu_feature_enabled() to static_cpu_has().
+---
+ arch/x86/Kconfig             |  2 ++
+ arch/x86/include/asm/elf.h   | 13 +++++++++++++
+ arch/x86/kernel/process_64.c | 32 ++++++++++++++++++++++++++++++++
+ 3 files changed, 47 insertions(+)
+
+diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
+index df24c4a90ea0..b1d7a2a37e3b 100644
+--- a/arch/x86/Kconfig
++++ b/arch/x86/Kconfig
+@@ -1971,6 +1971,8 @@ config X86_INTEL_SHADOW_STACK_USER
+ 	select X86_INTEL_CET
+ 	select ARCH_MAYBE_MKWRITE
+ 	select ARCH_HAS_SHADOW_STACK
++	select ARCH_USE_GNU_PROPERTY
++	select ARCH_BINFMT_ELF_STATE
+ 	help
+ 	  Shadow Stacks provides protection against program stack
+ 	  corruption.  It's a hardware feature.  This only matters
+diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
+index 69c0f892e310..fac79b621e0a 100644
+--- a/arch/x86/include/asm/elf.h
++++ b/arch/x86/include/asm/elf.h
+@@ -367,6 +367,19 @@ extern int compat_arch_setup_additional_pages(struct linux_binprm *bprm,
+ 					      int uses_interp);
+ #define compat_arch_setup_additional_pages compat_arch_setup_additional_pages
+ 
++#ifdef CONFIG_ARCH_BINFMT_ELF_STATE
++struct arch_elf_state {
++	unsigned int gnu_property;
++};
++
++#define INIT_ARCH_ELF_STATE {	\
++	.gnu_property = 0,	\
++}
++
++#define arch_elf_pt_proc(ehdr, phdr, elf, interp, state) (0)
++#define arch_check_elf(ehdr, interp, interp_ehdr, state) (0)
++#endif
++
+ /* Do not change the values. See get_align_mask() */
+ enum align_flags {
+ 	ALIGN_VA_32	= BIT(0),
+diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
+index 5ef9d8f25b0e..0c0475887d5c 100644
+--- a/arch/x86/kernel/process_64.c
++++ b/arch/x86/kernel/process_64.c
+@@ -730,3 +730,35 @@ unsigned long KSTK_ESP(struct task_struct *task)
+ {
+ 	return task_pt_regs(task)->sp;
+ }
++
++#ifdef CONFIG_ARCH_USE_GNU_PROPERTY
++int arch_parse_elf_property(u32 type, const void *data, size_t datasz,
++			     bool compat, struct arch_elf_state *state)
++{
++	if (type != GNU_PROPERTY_X86_FEATURE_1_AND)
++		return 0;
++
++	if (datasz != sizeof(unsigned int))
++		return -ENOEXEC;
++
++	state->gnu_property = *(unsigned int *)data;
++	return 0;
++}
++
++int arch_setup_elf_property(struct arch_elf_state *state)
++{
++	int r = 0;
++
++	if (!IS_ENABLED(CONFIG_X86_INTEL_CET))
++		return r;
++
++	memset(&current->thread.cet, 0, sizeof(struct cet_status));
++
++	if (static_cpu_has(X86_FEATURE_SHSTK)) {
++		if (state->gnu_property & GNU_PROPERTY_X86_FEATURE_1_SHSTK)
++			r = cet_setup_shstk();
++	}
++
++	return r;
++}
++#endif
+-- 
+2.26.2
+
diff --git a/0035-x86-cet-shstk-Handle-thread-shadow-stack.patch b/0035-x86-cet-shstk-Handle-thread-shadow-stack.patch
new file mode 100644
index 000000000..ff1a7c0b2
--- /dev/null
+++ b/0035-x86-cet-shstk-Handle-thread-shadow-stack.patch
@@ -0,0 +1,149 @@
+From 099768f8bcc8bca0d73b98119882f54c9da3984c Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Thu, 3 May 2018 12:40:57 -0700
+Subject: [PATCH 35/47] x86/cet/shstk: Handle thread shadow stack
+
+The kernel allocates (and frees on thread exit) a new shadow stack for a
+pthread child.
+
+    It is possible for the kernel to complete the clone syscall and set the
+    child's shadow stack pointer to NULL and let the child thread allocate
+    a shadow stack for itself.  There are two issues in this approach: It
+    is not compatible with existing code that does inline syscall and it
+    cannot handle signals before the child can successfully allocate a
+    shadow stack.
+
+A 64-bit shadow stack has a size of min(RLIMIT_STACK, 4 GB).  A compat-mode
+thread shadow stack has a size of 1/4 min(RLIMIT_STACK, 4 GB).  This allows
+more threads to run in a 32-bit address space.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+
+v10:
+- Limit shadow stack size to 4 GB.
+---
+ arch/x86/include/asm/cet.h         |  2 ++
+ arch/x86/include/asm/mmu_context.h |  3 +++
+ arch/x86/kernel/cet.c              | 41 ++++++++++++++++++++++++++++++
+ arch/x86/kernel/process.c          |  7 +++++
+ 4 files changed, 53 insertions(+)
+
+diff --git a/arch/x86/include/asm/cet.h b/arch/x86/include/asm/cet.h
+index 56fe08eebae6..71dc92acd2f2 100644
+--- a/arch/x86/include/asm/cet.h
++++ b/arch/x86/include/asm/cet.h
+@@ -18,11 +18,13 @@ struct cet_status {
+ 
+ #ifdef CONFIG_X86_INTEL_CET
+ int cet_setup_shstk(void);
++int cet_setup_thread_shstk(struct task_struct *p);
+ void cet_disable_free_shstk(struct task_struct *p);
+ int cet_verify_rstor_token(bool ia32, unsigned long ssp, unsigned long *new_ssp);
+ void cet_restore_signal(struct sc_ext *sc);
+ int cet_setup_signal(bool ia32, unsigned long rstor, struct sc_ext *sc);
+ #else
++static inline int cet_setup_thread_shstk(struct task_struct *p) { return 0; }
+ static inline void cet_disable_free_shstk(struct task_struct *p) {}
+ static inline void cet_restore_signal(struct sc_ext *sc) { return; }
+ static inline int cet_setup_signal(bool ia32, unsigned long rstor,
+diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h
+index 4e55370e48e8..bb7a4a2d6923 100644
+--- a/arch/x86/include/asm/mmu_context.h
++++ b/arch/x86/include/asm/mmu_context.h
+@@ -12,6 +12,7 @@
+ #include <asm/pgalloc.h>
+ #include <asm/tlbflush.h>
+ #include <asm/paravirt.h>
++#include <asm/cet.h>
+ #include <asm/debugreg.h>
+ 
+ extern atomic64_t last_mm_ctx_id;
+@@ -155,6 +156,8 @@ do {						\
+ #else
+ #define deactivate_mm(tsk, mm)			\
+ do {						\
++	if (!tsk->vfork_done)			\
++		cet_disable_free_shstk(tsk);	\
+ 	load_gs_index(0);			\
+ 	loadsegment(fs, 0);			\
+ } while (0)
+diff --git a/arch/x86/kernel/cet.c b/arch/x86/kernel/cet.c
+index e95bd2b3c394..736652b9b73f 100644
+--- a/arch/x86/kernel/cet.c
++++ b/arch/x86/kernel/cet.c
+@@ -169,6 +169,47 @@ int cet_setup_shstk(void)
+ 	return 0;
+ }
+ 
++int cet_setup_thread_shstk(struct task_struct *tsk)
++{
++	unsigned long addr, size;
++	struct cet_user_state *state;
++	struct cet_status *cet = &tsk->thread.cet;
++
++	if (!cet->shstk_size)
++		return 0;
++
++	state = get_xsave_addr(&tsk->thread.fpu.state.xsave,
++			       XFEATURE_CET_USER);
++
++	if (!state)
++		return -EINVAL;
++
++	/* Cap shadow stack size to 4 GB */
++	size = min(rlimit(RLIMIT_STACK), 1UL << 32);
++
++	/*
++	 * Compat-mode pthreads share a limited address space.
++	 * If each function call takes an average of four slots
++	 * stack space, we need 1/4 of stack size for shadow stack.
++	 */
++	if (in_compat_syscall())
++		size /= 4;
++	size = round_up(size, PAGE_SIZE);
++	addr = alloc_shstk(size);
++
++	if (IS_ERR((void *)addr)) {
++		cet->shstk_base = 0;
++		cet->shstk_size = 0;
++		return PTR_ERR((void *)addr);
++	}
++
++	fpu__prepare_write(&tsk->thread.fpu);
++	state->user_ssp = (u64)(addr + size);
++	cet->shstk_base = addr;
++	cet->shstk_size = size;
++	return 0;
++}
++
+ void cet_disable_free_shstk(struct task_struct *tsk)
+ {
+ 	struct cet_status *cet = &tsk->thread.cet;
+diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
+index 56587051df5a..aa896833f074 100644
+--- a/arch/x86/kernel/process.c
++++ b/arch/x86/kernel/process.c
+@@ -109,6 +109,7 @@ void exit_thread(struct task_struct *tsk)
+ 
+ 	free_vm86(t);
+ 
++	cet_disable_free_shstk(tsk);
+ 	fpu__drop(fpu);
+ }
+ 
+@@ -179,6 +180,12 @@ int copy_thread_tls(unsigned long clone_flags, unsigned long sp,
+ 	if (clone_flags & CLONE_SETTLS)
+ 		ret = set_new_tls(p, tls);
+ 
++#ifdef CONFIG_X86_64
++	/* Allocate a new shadow stack for pthread */
++	if (!ret && (clone_flags & (CLONE_VFORK | CLONE_VM)) == CLONE_VM)
++		ret = cet_setup_thread_shstk(p);
++#endif
++
+ 	if (!ret && unlikely(test_tsk_thread_flag(current, TIF_IO_BITMAP)))
+ 		io_bitmap_share(p);
+ 
+-- 
+2.26.2
+
diff --git a/0036-x86-cet-shstk-Add-arch_prctl-functions-for-shadow-st.patch b/0036-x86-cet-shstk-Add-arch_prctl-functions-for-shadow-st.patch
new file mode 100644
index 000000000..15d289219
--- /dev/null
+++ b/0036-x86-cet-shstk-Add-arch_prctl-functions-for-shadow-st.patch
@@ -0,0 +1,282 @@
+From e87c40b026539d4a8774044e3c463940c759fd6c Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Thu, 3 May 2018 13:04:29 -0700
+Subject: [PATCH 36/47] x86/cet/shstk: Add arch_prctl functions for shadow
+ stack
+
+arch_prctl(ARCH_X86_CET_STATUS, u64 *args)
+    Get CET feature status.
+
+    The parameter 'args' is a pointer to a user buffer.  The kernel returns
+    the following information:
+
+    *args = shadow stack/IBT status
+    *(args + 1) = shadow stack base address
+    *(args + 2) = shadow stack size
+
+arch_prctl(ARCH_X86_CET_DISABLE, u64 features)
+    Disable CET features specified in 'features'.  Return -EPERM if CET is
+    locked.
+
+arch_prctl(ARCH_X86_CET_LOCK)
+    Lock in CET features.
+
+arch_prctl(ARCH_X86_CET_ALLOC_SHSTK, u64 *args)
+    Allocate a new shadow stack.
+
+    The parameter 'args' is a pointer to a user buffer containing the
+    desired size to allocate.  The kernel returns the allocated shadow
+    stack address in *args.
+
+Also change do_arch_prctl_common()'s parameter 'cpuid_enabled' to
+'arg2', as it is now also passed to prctl_cet().
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+
+v11:
+- Check input for invalid features.
+- Fix prctl_cet() return values.
+
+v10:
+- Verify CET is enabled before handling arch_prctl.
+- Change input parameters from unsigned long to u64, to make it clear they
+  are 64-bit.
+---
+ arch/x86/include/asm/cet.h              |  4 ++
+ arch/x86/include/uapi/asm/prctl.h       |  5 ++
+ arch/x86/kernel/Makefile                |  2 +-
+ arch/x86/kernel/cet.c                   | 29 ++++++++
+ arch/x86/kernel/cet_prctl.c             | 91 +++++++++++++++++++++++++
+ arch/x86/kernel/process.c               |  6 +-
+ tools/arch/x86/include/uapi/asm/prctl.h |  5 ++
+ 7 files changed, 138 insertions(+), 4 deletions(-)
+ create mode 100644 arch/x86/kernel/cet_prctl.c
+
+diff --git a/arch/x86/include/asm/cet.h b/arch/x86/include/asm/cet.h
+index 71dc92acd2f2..99e6e741d28c 100644
+--- a/arch/x86/include/asm/cet.h
++++ b/arch/x86/include/asm/cet.h
+@@ -14,16 +14,20 @@ struct sc_ext;
+ struct cet_status {
+ 	unsigned long	shstk_base;
+ 	unsigned long	shstk_size;
++	unsigned int	locked:1;
+ };
+ 
+ #ifdef CONFIG_X86_INTEL_CET
++int prctl_cet(int option, u64 arg2);
+ int cet_setup_shstk(void);
+ int cet_setup_thread_shstk(struct task_struct *p);
++int cet_alloc_shstk(unsigned long *arg);
+ void cet_disable_free_shstk(struct task_struct *p);
+ int cet_verify_rstor_token(bool ia32, unsigned long ssp, unsigned long *new_ssp);
+ void cet_restore_signal(struct sc_ext *sc);
+ int cet_setup_signal(bool ia32, unsigned long rstor, struct sc_ext *sc);
+ #else
++static inline int prctl_cet(int option, u64 arg2) { return -EINVAL; }
+ static inline int cet_setup_thread_shstk(struct task_struct *p) { return 0; }
+ static inline void cet_disable_free_shstk(struct task_struct *p) {}
+ static inline void cet_restore_signal(struct sc_ext *sc) { return; }
+diff --git a/arch/x86/include/uapi/asm/prctl.h b/arch/x86/include/uapi/asm/prctl.h
+index 5a6aac9fa41f..d962f0ec9ccf 100644
+--- a/arch/x86/include/uapi/asm/prctl.h
++++ b/arch/x86/include/uapi/asm/prctl.h
+@@ -14,4 +14,9 @@
+ #define ARCH_MAP_VDSO_32	0x2002
+ #define ARCH_MAP_VDSO_64	0x2003
+ 
++#define ARCH_X86_CET_STATUS		0x3001
++#define ARCH_X86_CET_DISABLE		0x3002
++#define ARCH_X86_CET_LOCK		0x3003
++#define ARCH_X86_CET_ALLOC_SHSTK	0x3004
++
+ #endif /* _ASM_X86_PRCTL_H */
+diff --git a/arch/x86/kernel/Makefile b/arch/x86/kernel/Makefile
+index e9cc2551573b..0b621e2afbdc 100644
+--- a/arch/x86/kernel/Makefile
++++ b/arch/x86/kernel/Makefile
+@@ -144,7 +144,7 @@ obj-$(CONFIG_UNWINDER_ORC)		+= unwind_orc.o
+ obj-$(CONFIG_UNWINDER_FRAME_POINTER)	+= unwind_frame.o
+ obj-$(CONFIG_UNWINDER_GUESS)		+= unwind_guess.o
+ 
+-obj-$(CONFIG_X86_INTEL_CET)		+= cet.o
++obj-$(CONFIG_X86_INTEL_CET)		+= cet.o cet_prctl.o
+ 
+ ###
+ # 64 bit specific files
+diff --git a/arch/x86/kernel/cet.c b/arch/x86/kernel/cet.c
+index 736652b9b73f..b994f0a1e78e 100644
+--- a/arch/x86/kernel/cet.c
++++ b/arch/x86/kernel/cet.c
+@@ -145,6 +145,35 @@ static int create_rstor_token(bool ia32, unsigned long ssp,
+ 	return 0;
+ }
+ 
++int cet_alloc_shstk(unsigned long *arg)
++{
++	unsigned long len = *arg;
++	unsigned long addr;
++	unsigned long token;
++	unsigned long ssp;
++
++	addr = alloc_shstk(round_up(len, PAGE_SIZE));
++
++	if (IS_ERR((void *)addr))
++		return PTR_ERR((void *)addr);
++
++	/* Restore token is 8 bytes and aligned to 8 bytes */
++	ssp = addr + len;
++	token = ssp;
++
++	if (!in_ia32_syscall())
++		token |= TOKEN_MODE_64;
++	ssp -= 8;
++
++	if (write_user_shstk_64(ssp, token)) {
++		vm_munmap(addr, len);
++		return -EINVAL;
++	}
++
++	*arg = addr;
++	return 0;
++}
++
+ int cet_setup_shstk(void)
+ {
+ 	unsigned long addr, size;
+diff --git a/arch/x86/kernel/cet_prctl.c b/arch/x86/kernel/cet_prctl.c
+new file mode 100644
+index 000000000000..844ff7f6d129
+--- /dev/null
++++ b/arch/x86/kernel/cet_prctl.c
+@@ -0,0 +1,91 @@
++/* SPDX-License-Identifier: GPL-2.0 */
++
++#include <linux/errno.h>
++#include <linux/uaccess.h>
++#include <linux/prctl.h>
++#include <linux/compat.h>
++#include <linux/mman.h>
++#include <linux/elfcore.h>
++#include <asm/processor.h>
++#include <asm/prctl.h>
++#include <asm/cet.h>
++
++/* See Documentation/x86/intel_cet.rst. */
++
++static int copy_status_to_user(struct cet_status *cet, u64 arg2)
++{
++	u64 buf[3] = {0, 0, 0};
++
++	if (cet->shstk_size) {
++		buf[0] |= GNU_PROPERTY_X86_FEATURE_1_SHSTK;
++		buf[1] = (u64)cet->shstk_base;
++		buf[2] = (u64)cet->shstk_size;
++	}
++
++	return copy_to_user((u64 __user *)arg2, buf, sizeof(buf));
++}
++
++static int handle_alloc_shstk(u64 arg2)
++{
++	int err = 0;
++	unsigned long arg;
++	unsigned long addr = 0;
++	unsigned long size = 0;
++
++	if (get_user(arg, (unsigned long __user *)arg2))
++		return -EFAULT;
++
++	size = arg;
++	err = cet_alloc_shstk(&arg);
++	if (err)
++		return err;
++
++	addr = arg;
++	if (put_user((u64)addr, (u64 __user *)arg2)) {
++		vm_munmap(addr, size);
++		return -EFAULT;
++	}
++
++	return 0;
++}
++
++int prctl_cet(int option, u64 arg2)
++{
++	struct cet_status *cet;
++
++	/*
++	 * GLIBC's ENOTSUPP == EOPNOTSUPP == 95, and it does not recognize
++	 * the kernel's ENOTSUPP (524).  So return EOPNOTSUPP here.
++	 */
++	if (!IS_ENABLED(CONFIG_X86_INTEL_CET))
++		return -EOPNOTSUPP;
++
++	cet = &current->thread.cet;
++
++	if (option == ARCH_X86_CET_STATUS)
++		return copy_status_to_user(cet, arg2);
++
++	if (!static_cpu_has(X86_FEATURE_SHSTK))
++		return -EOPNOTSUPP;
++
++	switch (option) {
++	case ARCH_X86_CET_DISABLE:
++		if (cet->locked)
++			return -EPERM;
++		if (arg2 & GNU_PROPERTY_X86_FEATURE_1_INVAL)
++			return -EINVAL;
++		if (arg2 & GNU_PROPERTY_X86_FEATURE_1_SHSTK)
++			cet_disable_free_shstk(current);
++		return 0;
++
++	case ARCH_X86_CET_LOCK:
++		cet->locked = 1;
++		return 0;
++
++	case ARCH_X86_CET_ALLOC_SHSTK:
++		return handle_alloc_shstk(arg2);
++
++	default:
++		return -ENOSYS;
++	}
++}
+diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
+index aa896833f074..c50ace4e6c52 100644
+--- a/arch/x86/kernel/process.c
++++ b/arch/x86/kernel/process.c
+@@ -989,14 +989,14 @@ unsigned long get_wchan(struct task_struct *p)
+ }
+ 
+ long do_arch_prctl_common(struct task_struct *task, int option,
+-			  unsigned long cpuid_enabled)
++			  unsigned long arg2)
+ {
+ 	switch (option) {
+ 	case ARCH_GET_CPUID:
+ 		return get_cpuid_mode();
+ 	case ARCH_SET_CPUID:
+-		return set_cpuid_mode(task, cpuid_enabled);
++		return set_cpuid_mode(task, arg2);
+ 	}
+ 
+-	return -EINVAL;
++	return prctl_cet(option, arg2);
+ }
+diff --git a/tools/arch/x86/include/uapi/asm/prctl.h b/tools/arch/x86/include/uapi/asm/prctl.h
+index 5a6aac9fa41f..d962f0ec9ccf 100644
+--- a/tools/arch/x86/include/uapi/asm/prctl.h
++++ b/tools/arch/x86/include/uapi/asm/prctl.h
+@@ -14,4 +14,9 @@
+ #define ARCH_MAP_VDSO_32	0x2002
+ #define ARCH_MAP_VDSO_64	0x2003
+ 
++#define ARCH_X86_CET_STATUS		0x3001
++#define ARCH_X86_CET_DISABLE		0x3002
++#define ARCH_X86_CET_LOCK		0x3003
++#define ARCH_X86_CET_ALLOC_SHSTK	0x3004
++
+ #endif /* _ASM_X86_PRCTL_H */
+-- 
+2.26.2
+
diff --git a/0037-x86-cet-ibt-Add-Kconfig-option-for-user-mode-Indirec.patch b/0037-x86-cet-ibt-Add-Kconfig-option-for-user-mode-Indirec.patch
new file mode 100644
index 000000000..6469be780
--- /dev/null
+++ b/0037-x86-cet-ibt-Add-Kconfig-option-for-user-mode-Indirec.patch
@@ -0,0 +1,52 @@
+From 5c3f3ca62796d7360a3fc4abf29119b40d72267e Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Wed, 4 Oct 2017 12:35:32 -0700
+Subject: [PATCH 37/47] x86/cet/ibt: Add Kconfig option for user-mode Indirect
+ Branch Tracking
+
+Introduce Kconfig option X86_INTEL_BRANCH_TRACKING_USER.
+
+Indirect Branch Tracking (IBT) provides protection against CALL-/JMP-
+oriented programming attacks.  It is active when the kernel has this
+feature enabled, and the processor and the application support it.
+When this feature is enabled, legacy non-IBT applications continue to
+work, but without IBT protection.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+
+v10:
+- Change build-time CET check to config depends on.
+---
+ arch/x86/Kconfig | 16 ++++++++++++++++
+ 1 file changed, 16 insertions(+)
+
+diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
+index b1d7a2a37e3b..8670b1d5e34f 100644
+--- a/arch/x86/Kconfig
++++ b/arch/x86/Kconfig
+@@ -1984,6 +1984,22 @@ config X86_INTEL_SHADOW_STACK_USER
+ 
+ 	  If unsure, say y.
+ 
++config X86_INTEL_BRANCH_TRACKING_USER
++	prompt "Intel Indirect Branch Tracking for user-mode"
++	def_bool n
++	depends on CPU_SUP_INTEL && X86_64
++	depends on $(cc-option,-fcf-protection)
++	select X86_INTEL_CET
++	help
++	  Indirect Branch Tracking (IBT) provides protection against
++	  CALL-/JMP-oriented programming attacks.  It is active when
++	  the kernel has this feature enabled, and the processor and
++	  the application support it.  When this feature is enabled,
++	  legacy non-IBT applications continue to work, but without
++	  IBT protection.
++
++	  If unsure, say y
++
+ config EFI
+ 	bool "EFI runtime service support"
+ 	depends on ACPI
+-- 
+2.26.2
+
diff --git a/0038-x86-cet-ibt-User-mode-Indirect-Branch-Tracking-suppo.patch b/0038-x86-cet-ibt-User-mode-Indirect-Branch-Tracking-suppo.patch
new file mode 100644
index 000000000..179d71d71
--- /dev/null
+++ b/0038-x86-cet-ibt-User-mode-Indirect-Branch-Tracking-suppo.patch
@@ -0,0 +1,178 @@
+From 58eae73614da310ee02f46ee5aa1434a89c5e273 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Thu, 3 May 2018 13:30:56 -0700
+Subject: [PATCH 38/47] x86/cet/ibt: User-mode Indirect Branch Tracking support
+
+Introduce user-mode Indirect Branch Tracking (IBT) support.  Update setup
+routines to include IBT.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+
+v10:
+- Change no_cet_ibt to no_user_ibt.
+
+v9:
+- Change cpu_feature_enabled() to static_cpu_has().
+
+v2:
+- Change noibt to no_cet_ibt.
+---
+ arch/x86/include/asm/cet.h                    |  3 ++
+ arch/x86/include/asm/disabled-features.h      |  8 ++++-
+ arch/x86/kernel/cet.c                         | 33 +++++++++++++++++++
+ arch/x86/kernel/cpu/common.c                  | 17 ++++++++++
+ .../arch/x86/include/asm/disabled-features.h  |  8 ++++-
+ 5 files changed, 67 insertions(+), 2 deletions(-)
+
+diff --git a/arch/x86/include/asm/cet.h b/arch/x86/include/asm/cet.h
+index 99e6e741d28c..f163c805a559 100644
+--- a/arch/x86/include/asm/cet.h
++++ b/arch/x86/include/asm/cet.h
+@@ -15,6 +15,7 @@ struct cet_status {
+ 	unsigned long	shstk_base;
+ 	unsigned long	shstk_size;
+ 	unsigned int	locked:1;
++	unsigned int	ibt_enabled:1;
+ };
+ 
+ #ifdef CONFIG_X86_INTEL_CET
+@@ -26,6 +27,8 @@ void cet_disable_free_shstk(struct task_struct *p);
+ int cet_verify_rstor_token(bool ia32, unsigned long ssp, unsigned long *new_ssp);
+ void cet_restore_signal(struct sc_ext *sc);
+ int cet_setup_signal(bool ia32, unsigned long rstor, struct sc_ext *sc);
++int cet_setup_ibt(void);
++void cet_disable_ibt(void);
+ #else
+ static inline int prctl_cet(int option, u64 arg2) { return -EINVAL; }
+ static inline int cet_setup_thread_shstk(struct task_struct *p) { return 0; }
+diff --git a/arch/x86/include/asm/disabled-features.h b/arch/x86/include/asm/disabled-features.h
+index a0e1b24cfa02..52c9c07cfacc 100644
+--- a/arch/x86/include/asm/disabled-features.h
++++ b/arch/x86/include/asm/disabled-features.h
+@@ -62,6 +62,12 @@
+ #define DISABLE_SHSTK	(1<<(X86_FEATURE_SHSTK & 31))
+ #endif
+ 
++#ifdef CONFIG_X86_INTEL_BRANCH_TRACKING_USER
++#define DISABLE_IBT	0
++#else
++#define DISABLE_IBT	(1<<(X86_FEATURE_IBT & 31))
++#endif
++
+ /*
+  * Make sure to add features to the correct mask
+  */
+@@ -83,7 +89,7 @@
+ #define DISABLED_MASK15	0
+ #define DISABLED_MASK16	(DISABLE_PKU|DISABLE_OSPKE|DISABLE_LA57|DISABLE_UMIP|DISABLE_SHSTK)
+ #define DISABLED_MASK17	0
+-#define DISABLED_MASK18	0
++#define DISABLED_MASK18	(DISABLE_IBT)
+ #define DISABLED_MASK_CHECK BUILD_BUG_ON_ZERO(NCAPINTS != 19)
+ 
+ #endif /* _ASM_X86_DISABLED_FEATURES_H */
+diff --git a/arch/x86/kernel/cet.c b/arch/x86/kernel/cet.c
+index b994f0a1e78e..fe4ff3590331 100644
+--- a/arch/x86/kernel/cet.c
++++ b/arch/x86/kernel/cet.c
+@@ -13,6 +13,8 @@
+ #include <linux/uaccess.h>
+ #include <linux/sched/signal.h>
+ #include <linux/compat.h>
++#include <linux/vmalloc.h>
++#include <linux/bitops.h>
+ #include <asm/msr.h>
+ #include <asm/user.h>
+ #include <asm/fpu/internal.h>
+@@ -357,3 +359,34 @@ int cet_setup_signal(bool ia32, unsigned long rstor_addr, struct sc_ext *sc_ext)
+ 
+ 	return 0;
+ }
++
++int cet_setup_ibt(void)
++{
++	u64 msr_val;
++
++	if (!static_cpu_has(X86_FEATURE_IBT))
++		return -EOPNOTSUPP;
++
++	start_update_msrs();
++	rdmsrl(MSR_IA32_U_CET, msr_val);
++	msr_val |= (MSR_IA32_CET_ENDBR_EN | MSR_IA32_CET_NO_TRACK_EN);
++	wrmsrl(MSR_IA32_U_CET, msr_val);
++	end_update_msrs();
++	current->thread.cet.ibt_enabled = 1;
++	return 0;
++}
++
++void cet_disable_ibt(void)
++{
++	u64 msr_val;
++
++	if (!static_cpu_has(X86_FEATURE_IBT))
++		return;
++
++	start_update_msrs();
++	rdmsrl(MSR_IA32_U_CET, msr_val);
++	msr_val &= MSR_IA32_CET_SHSTK_EN;
++	wrmsrl(MSR_IA32_U_CET, msr_val);
++	end_update_msrs();
++	current->thread.cet.ibt_enabled = 0;
++}
+diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
+index 1563b472e0f9..896a07675eee 100644
+--- a/arch/x86/kernel/cpu/common.c
++++ b/arch/x86/kernel/cpu/common.c
+@@ -496,6 +496,23 @@ static __init int setup_disable_shstk(char *s)
+ __setup("no_user_shstk", setup_disable_shstk);
+ #endif
+ 
++#ifdef CONFIG_X86_INTEL_BRANCH_TRACKING_USER
++static __init int setup_disable_ibt(char *s)
++{
++	/* require an exact match without trailing characters */
++	if (s[0] != '\0')
++		return 0;
++
++	if (!boot_cpu_has(X86_FEATURE_IBT))
++		return 1;
++
++	setup_clear_cpu_cap(X86_FEATURE_IBT);
++	pr_info("x86: 'no_user_ibt' specified, disabling user Branch Tracking\n");
++	return 1;
++}
++__setup("no_user_ibt", setup_disable_ibt);
++#endif
++
+ /*
+  * Some CPU features depend on higher CPUID levels, which may not always
+  * be available due to CPUID level capping or broken virtualization
+diff --git a/tools/arch/x86/include/asm/disabled-features.h b/tools/arch/x86/include/asm/disabled-features.h
+index a0e1b24cfa02..52c9c07cfacc 100644
+--- a/tools/arch/x86/include/asm/disabled-features.h
++++ b/tools/arch/x86/include/asm/disabled-features.h
+@@ -62,6 +62,12 @@
+ #define DISABLE_SHSTK	(1<<(X86_FEATURE_SHSTK & 31))
+ #endif
+ 
++#ifdef CONFIG_X86_INTEL_BRANCH_TRACKING_USER
++#define DISABLE_IBT	0
++#else
++#define DISABLE_IBT	(1<<(X86_FEATURE_IBT & 31))
++#endif
++
+ /*
+  * Make sure to add features to the correct mask
+  */
+@@ -83,7 +89,7 @@
+ #define DISABLED_MASK15	0
+ #define DISABLED_MASK16	(DISABLE_PKU|DISABLE_OSPKE|DISABLE_LA57|DISABLE_UMIP|DISABLE_SHSTK)
+ #define DISABLED_MASK17	0
+-#define DISABLED_MASK18	0
++#define DISABLED_MASK18	(DISABLE_IBT)
+ #define DISABLED_MASK_CHECK BUILD_BUG_ON_ZERO(NCAPINTS != 19)
+ 
+ #endif /* _ASM_X86_DISABLED_FEATURES_H */
+-- 
+2.26.2
+
diff --git a/0039-x86-cet-ibt-Handle-signals-for-Indirect-Branch-Track.patch b/0039-x86-cet-ibt-Handle-signals-for-Indirect-Branch-Track.patch
new file mode 100644
index 000000000..0a4d73519
--- /dev/null
+++ b/0039-x86-cet-ibt-Handle-signals-for-Indirect-Branch-Track.patch
@@ -0,0 +1,105 @@
+From 06687de015e319f7ad4c32f431cab3c4c6592569 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 28 May 2019 12:29:14 -0700
+Subject: [PATCH 39/47] x86/cet/ibt: Handle signals for Indirect Branch
+ Tracking
+
+Indirect Branch Tracking setting does not change in signal delivering or
+sigreturn; except the WAIT_ENDBR status.  In general, a task is in
+WAIT_ENDBR after an indirect CALL/JMP and before the next instruction
+starts.
+
+WAIT_ENDBR status can be read from MSR_IA32_U_CET.  It is reset for signal
+delivering, but preserved on a task's stack and restored for sigreturn.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+
+v9:
+- Fix missing WAIT_ENDBR in signal handling.
+---
+ arch/x86/kernel/cet.c        | 27 +++++++++++++++++++++++++--
+ arch/x86/kernel/fpu/signal.c |  8 +++++---
+ 2 files changed, 30 insertions(+), 5 deletions(-)
+
+diff --git a/arch/x86/kernel/cet.c b/arch/x86/kernel/cet.c
+index fe4ff3590331..b1d592699964 100644
+--- a/arch/x86/kernel/cet.c
++++ b/arch/x86/kernel/cet.c
+@@ -309,6 +309,13 @@ void cet_restore_signal(struct sc_ext *sc_ext)
+ 		msr_val |= MSR_IA32_CET_SHSTK_EN;
+ 	}
+ 
++	if (cet->ibt_enabled) {
++		msr_val |= (MSR_IA32_CET_ENDBR_EN | MSR_IA32_CET_NO_TRACK_EN);
++
++		if (sc_ext->wait_endbr)
++			msr_val |= MSR_IA32_CET_WAIT_ENDBR;
++	}
++
+ 	if (test_thread_flag(TIF_NEED_FPU_LOAD))
+ 		cet_user_state->user_cet = msr_val;
+ 	else
+@@ -351,9 +358,25 @@ int cet_setup_signal(bool ia32, unsigned long rstor_addr, struct sc_ext *sc_ext)
+ 		sc_ext->ssp = new_ssp;
+ 	}
+ 
+-	if (ssp) {
++	if (ssp || cet->ibt_enabled) {
++
+ 		start_update_msrs();
+-		wrmsrl(MSR_IA32_PL3_SSP, ssp);
++
++		if (ssp)
++			wrmsrl(MSR_IA32_PL3_SSP, ssp);
++
++		if (cet->ibt_enabled) {
++			u64 r;
++
++			rdmsrl(MSR_IA32_U_CET, r);
++
++			if (r & MSR_IA32_CET_WAIT_ENDBR) {
++				sc_ext->wait_endbr = 1;
++				r &= ~MSR_IA32_CET_WAIT_ENDBR;
++				wrmsrl(MSR_IA32_U_CET, r);
++			}
++		}
++
+ 		end_update_msrs();
+ 	}
+ 
+diff --git a/arch/x86/kernel/fpu/signal.c b/arch/x86/kernel/fpu/signal.c
+index 163a17772ca0..43e4464674c2 100644
+--- a/arch/x86/kernel/fpu/signal.c
++++ b/arch/x86/kernel/fpu/signal.c
+@@ -57,7 +57,8 @@ int save_cet_to_sigframe(int ia32, void __user *fp, unsigned long restorer)
+ {
+ 	int err = 0;
+ 
+-	if (!current->thread.cet.shstk_size)
++	if (!current->thread.cet.shstk_size &&
++	    !current->thread.cet.ibt_enabled)
+ 		return 0;
+ 
+ 	if (fp) {
+@@ -89,7 +90,8 @@ static int get_cet_from_sigframe(int ia32, void __user *fp, struct sc_ext *ext)
+ 
+ 	memset(ext, 0, sizeof(*ext));
+ 
+-	if (!current->thread.cet.shstk_size)
++	if (!current->thread.cet.shstk_size &&
++	    !current->thread.cet.ibt_enabled)
+ 		return 0;
+ 
+ 	if (fp) {
+@@ -576,7 +578,7 @@ static unsigned long fpu__alloc_sigcontext_ext(unsigned long sp)
+ 	 * sigcontext_ext is at: fpu + fpu_user_xstate_size +
+ 	 * FP_XSTATE_MAGIC2_SIZE, then aligned to 8.
+ 	 */
+-	if (cet->shstk_size)
++	if (cet->shstk_size || cet->ibt_enabled)
+ 		sp -= (sizeof(struct sc_ext) + 8);
+ 
+ 	return sp;
+-- 
+2.26.2
+
diff --git a/0040-x86-cet-ibt-ELF-header-parsing-for-Indirect-Branch-T.patch b/0040-x86-cet-ibt-ELF-header-parsing-for-Indirect-Branch-T.patch
new file mode 100644
index 000000000..15602260f
--- /dev/null
+++ b/0040-x86-cet-ibt-ELF-header-parsing-for-Indirect-Branch-T.patch
@@ -0,0 +1,52 @@
+From 777d0c94585c2f82e7f570020cc5e3df3569ea81 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 30 Apr 2019 15:16:22 -0700
+Subject: [PATCH 40/47] x86/cet/ibt: ELF header parsing for Indirect Branch
+ Tracking
+
+Update arch_setup_elf_property() for Indirect Branch Tracking.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+
+v9:
+- Change cpu_feature_enabled() to static_cpu_has().
+---
+ arch/x86/Kconfig             | 2 ++
+ arch/x86/kernel/process_64.c | 8 ++++++++
+ 2 files changed, 10 insertions(+)
+
+diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
+index 8670b1d5e34f..bb0e2a9feab7 100644
+--- a/arch/x86/Kconfig
++++ b/arch/x86/Kconfig
+@@ -1990,6 +1990,8 @@ config X86_INTEL_BRANCH_TRACKING_USER
+ 	depends on CPU_SUP_INTEL && X86_64
+ 	depends on $(cc-option,-fcf-protection)
+ 	select X86_INTEL_CET
++	select ARCH_USE_GNU_PROPERTY
++	select ARCH_BINFMT_ELF_STATE
+ 	help
+ 	  Indirect Branch Tracking (IBT) provides protection against
+ 	  CALL-/JMP-oriented programming attacks.  It is active when
+diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
+index 0c0475887d5c..d21c9cd90ced 100644
+--- a/arch/x86/kernel/process_64.c
++++ b/arch/x86/kernel/process_64.c
+@@ -759,6 +759,14 @@ int arch_setup_elf_property(struct arch_elf_state *state)
+ 			r = cet_setup_shstk();
+ 	}
+ 
++	if (r < 0)
++		return r;
++
++	if (static_cpu_has(X86_FEATURE_IBT)) {
++		if (state->gnu_property & GNU_PROPERTY_X86_FEATURE_1_IBT)
++			r = cet_setup_ibt();
++	}
++
+ 	return r;
+ }
+ #endif
+-- 
+2.26.2
+
diff --git a/0041-x86-cet-ibt-Add-arch_prctl-functions-for-Indirect-Br.patch b/0041-x86-cet-ibt-Add-arch_prctl-functions-for-Indirect-Br.patch
new file mode 100644
index 000000000..d29114eb9
--- /dev/null
+++ b/0041-x86-cet-ibt-Add-arch_prctl-functions-for-Indirect-Br.patch
@@ -0,0 +1,51 @@
+From 0e6450cf06c92f42840ab364588504e66c8d9f67 Mon Sep 17 00:00:00 2001
+From: "H.J. Lu" <hjl.tools@gmail.com>
+Date: Tue, 21 Aug 2018 14:13:05 -0700
+Subject: [PATCH 41/47] x86/cet/ibt: Add arch_prctl functions for Indirect
+ Branch Tracking
+
+Update ARCH_X86_CET_STATUS and ARCH_X86_CET_DISABLE for Indirect Branch
+Tracking.
+
+Signed-off-by: H.J. Lu <hjl.tools@gmail.com>
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/kernel/cet_prctl.c | 8 +++++++-
+ 1 file changed, 7 insertions(+), 1 deletion(-)
+
+diff --git a/arch/x86/kernel/cet_prctl.c b/arch/x86/kernel/cet_prctl.c
+index 844ff7f6d129..81c8d76cac18 100644
+--- a/arch/x86/kernel/cet_prctl.c
++++ b/arch/x86/kernel/cet_prctl.c
+@@ -22,6 +22,9 @@ static int copy_status_to_user(struct cet_status *cet, u64 arg2)
+ 		buf[2] = (u64)cet->shstk_size;
+ 	}
+ 
++	if (cet->ibt_enabled)
++		buf[0] |= GNU_PROPERTY_X86_FEATURE_1_IBT;
++
+ 	return copy_to_user((u64 __user *)arg2, buf, sizeof(buf));
+ }
+ 
+@@ -65,7 +68,8 @@ int prctl_cet(int option, u64 arg2)
+ 	if (option == ARCH_X86_CET_STATUS)
+ 		return copy_status_to_user(cet, arg2);
+ 
+-	if (!static_cpu_has(X86_FEATURE_SHSTK))
++	if (!static_cpu_has(X86_FEATURE_SHSTK) &&
++	    !static_cpu_has(X86_FEATURE_IBT))
+ 		return -EOPNOTSUPP;
+ 
+ 	switch (option) {
+@@ -76,6 +80,8 @@ int prctl_cet(int option, u64 arg2)
+ 			return -EINVAL;
+ 		if (arg2 & GNU_PROPERTY_X86_FEATURE_1_SHSTK)
+ 			cet_disable_free_shstk(current);
++		if (arg2 & GNU_PROPERTY_X86_FEATURE_1_IBT)
++			cet_disable_ibt();
+ 		return 0;
+ 
+ 	case ARCH_X86_CET_LOCK:
+-- 
+2.26.2
+
diff --git a/0042-x86-cet-Add-PTRACE-interface-for-CET.patch b/0042-x86-cet-Add-PTRACE-interface-for-CET.patch
new file mode 100644
index 000000000..48da92f7b
--- /dev/null
+++ b/0042-x86-cet-Add-PTRACE-interface-for-CET.patch
@@ -0,0 +1,151 @@
+From c45c0ee32e70d7712f021a9998d9a34b6bde9227 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 23 Apr 2018 12:55:13 -0700
+Subject: [PATCH 42/47] x86/cet: Add PTRACE interface for CET
+
+Add REGSET_CET64/REGSET_CET32 to get/set CET MSRs:
+
+    IA32_U_CET (user-mode CET settings) and
+    IA32_PL3_SSP (user-mode Shadow Stack)
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/include/asm/fpu/regset.h |  7 ++---
+ arch/x86/kernel/fpu/regset.c      | 45 +++++++++++++++++++++++++++++++
+ arch/x86/kernel/ptrace.c          | 16 +++++++++++
+ include/uapi/linux/elf.h          |  1 +
+ 4 files changed, 66 insertions(+), 3 deletions(-)
+
+diff --git a/arch/x86/include/asm/fpu/regset.h b/arch/x86/include/asm/fpu/regset.h
+index d5bdffb9d27f..edad0d889084 100644
+--- a/arch/x86/include/asm/fpu/regset.h
++++ b/arch/x86/include/asm/fpu/regset.h
+@@ -7,11 +7,12 @@
+ 
+ #include <linux/regset.h>
+ 
+-extern user_regset_active_fn regset_fpregs_active, regset_xregset_fpregs_active;
++extern user_regset_active_fn regset_fpregs_active, regset_xregset_fpregs_active,
++				cetregs_active;
+ extern user_regset_get_fn fpregs_get, xfpregs_get, fpregs_soft_get,
+-				xstateregs_get;
++				xstateregs_get, cetregs_get;
+ extern user_regset_set_fn fpregs_set, xfpregs_set, fpregs_soft_set,
+-				 xstateregs_set;
++				 xstateregs_set, cetregs_set;
+ 
+ /*
+  * xstateregs_active == regset_fpregs_active. Please refer to the comment
+diff --git a/arch/x86/kernel/fpu/regset.c b/arch/x86/kernel/fpu/regset.c
+index bd1d0649f8ce..dcb86ccd2b7e 100644
+--- a/arch/x86/kernel/fpu/regset.c
++++ b/arch/x86/kernel/fpu/regset.c
+@@ -156,6 +156,51 @@ int xstateregs_set(struct task_struct *target, const struct user_regset *regset,
+ 	return ret;
+ }
+ 
++int cetregs_active(struct task_struct *target, const struct user_regset *regset)
++{
++#ifdef CONFIG_X86_INTEL_CET
++	if (target->thread.cet.shstk_size || target->thread.cet.ibt_enabled)
++		return regset->n;
++#endif
++	return 0;
++}
++
++int cetregs_get(struct task_struct *target, const struct user_regset *regset,
++		unsigned int pos, unsigned int count,
++		void *kbuf, void __user *ubuf)
++{
++	struct fpu *fpu = &target->thread.fpu;
++	struct cet_user_state *cetregs;
++
++	if (!boot_cpu_has(X86_FEATURE_SHSTK))
++		return -ENODEV;
++
++	fpu__prepare_read(fpu);
++	cetregs = get_xsave_addr(&fpu->state.xsave, XFEATURE_CET_USER);
++	if (!cetregs)
++		return -EFAULT;
++
++	return user_regset_copyout(&pos, &count, &kbuf, &ubuf, cetregs, 0, -1);
++}
++
++int cetregs_set(struct task_struct *target, const struct user_regset *regset,
++		  unsigned int pos, unsigned int count,
++		  const void *kbuf, const void __user *ubuf)
++{
++	struct fpu *fpu = &target->thread.fpu;
++	struct cet_user_state *cetregs;
++
++	if (!boot_cpu_has(X86_FEATURE_SHSTK))
++		return -ENODEV;
++
++	fpu__prepare_write(fpu);
++	cetregs = get_xsave_addr(&fpu->state.xsave, XFEATURE_CET_USER);
++	if (!cetregs)
++		return -EFAULT;
++
++	return user_regset_copyin(&pos, &count, &kbuf, &ubuf, cetregs, 0, -1);
++}
++
+ #if defined CONFIG_X86_32 || defined CONFIG_IA32_EMULATION
+ 
+ /*
+diff --git a/arch/x86/kernel/ptrace.c b/arch/x86/kernel/ptrace.c
+index f0e1ddbc2fd7..c362abdf6ef1 100644
+--- a/arch/x86/kernel/ptrace.c
++++ b/arch/x86/kernel/ptrace.c
+@@ -53,7 +53,9 @@ enum x86_regset {
+ 	REGSET_IOPERM64 = REGSET_XFP,
+ 	REGSET_XSTATE,
+ 	REGSET_TLS,
++	REGSET_CET64 = REGSET_TLS,
+ 	REGSET_IOPERM32,
++	REGSET_CET32,
+ };
+ 
+ struct pt_regs_offset {
+@@ -1255,6 +1257,13 @@ static struct user_regset x86_64_regsets[] __ro_after_init = {
+ 		.size = sizeof(long), .align = sizeof(long),
+ 		.active = ioperm_active, .get = ioperm_get
+ 	},
++	[REGSET_CET64] = {
++		.core_note_type = NT_X86_CET,
++		.n = sizeof(struct cet_user_state) / sizeof(u64),
++		.size = sizeof(u64), .align = sizeof(u64),
++		.active = cetregs_active, .get = cetregs_get,
++		.set = cetregs_set
++	},
+ };
+ 
+ static const struct user_regset_view user_x86_64_view = {
+@@ -1310,6 +1319,13 @@ static struct user_regset x86_32_regsets[] __ro_after_init = {
+ 		.size = sizeof(u32), .align = sizeof(u32),
+ 		.active = ioperm_active, .get = ioperm_get
+ 	},
++	[REGSET_CET32] = {
++		.core_note_type = NT_X86_CET,
++		.n = sizeof(struct cet_user_state) / sizeof(u64),
++		.size = sizeof(u64), .align = sizeof(u64),
++		.active = cetregs_active, .get = cetregs_get,
++		.set = cetregs_set
++	},
+ };
+ 
+ static const struct user_regset_view user_x86_32_view = {
+diff --git a/include/uapi/linux/elf.h b/include/uapi/linux/elf.h
+index 49c026ce8d97..6502906ef47c 100644
+--- a/include/uapi/linux/elf.h
++++ b/include/uapi/linux/elf.h
+@@ -402,6 +402,7 @@ typedef struct elf64_shdr {
+ #define NT_386_TLS	0x200		/* i386 TLS slots (struct user_desc) */
+ #define NT_386_IOPERM	0x201		/* x86 io permission bitmap (1=deny) */
+ #define NT_X86_XSTATE	0x202		/* x86 extended state using xsave */
++#define NT_X86_CET	0x203		/* x86 cet state */
+ #define NT_S390_HIGH_GPRS	0x300	/* s390 upper register halves */
+ #define NT_S390_TIMER	0x301		/* s390 timer register */
+ #define NT_S390_TODCMP	0x302		/* s390 TOD clock comparator register */
+-- 
+2.26.2
+
diff --git a/0043-x86-vdso-32-Add-ENDBR32-to-__kernel_vsyscall-entry-p.patch b/0043-x86-vdso-32-Add-ENDBR32-to-__kernel_vsyscall-entry-p.patch
new file mode 100644
index 000000000..b4b764a85
--- /dev/null
+++ b/0043-x86-vdso-32-Add-ENDBR32-to-__kernel_vsyscall-entry-p.patch
@@ -0,0 +1,32 @@
+From ad9ffb6c49123c8f94e10b2a8405f455de24dcf7 Mon Sep 17 00:00:00 2001
+From: "H.J. Lu" <hjl.tools@gmail.com>
+Date: Fri, 28 Sep 2018 06:21:50 -0700
+Subject: [PATCH 43/47] x86/vdso/32: Add ENDBR32 to __kernel_vsyscall entry
+ point
+
+Add ENDBR32 to __kernel_vsyscall entry point.
+
+Signed-off-by: H.J. Lu <hjl.tools@gmail.com>
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Acked-by: Andy Lutomirski <luto@kernel.org>
+---
+ arch/x86/entry/vdso/vdso32/system_call.S | 3 +++
+ 1 file changed, 3 insertions(+)
+
+diff --git a/arch/x86/entry/vdso/vdso32/system_call.S b/arch/x86/entry/vdso/vdso32/system_call.S
+index de1fff7188aa..5cf74ebd4746 100644
+--- a/arch/x86/entry/vdso/vdso32/system_call.S
++++ b/arch/x86/entry/vdso/vdso32/system_call.S
+@@ -14,6 +14,9 @@
+ 	ALIGN
+ __kernel_vsyscall:
+ 	CFI_STARTPROC
++#ifdef CONFIG_X86_INTEL_BRANCH_TRACKING_USER
++	endbr32
++#endif
+ 	/*
+ 	 * Reshuffle regs so that all of any of the entry instructions
+ 	 * will preserve enough state.
+-- 
+2.26.2
+
diff --git a/0044-x86-vdso-Insert-endbr32-endbr64-to-vDSO.patch b/0044-x86-vdso-Insert-endbr32-endbr64-to-vDSO.patch
new file mode 100644
index 000000000..76a005f4d
--- /dev/null
+++ b/0044-x86-vdso-Insert-endbr32-endbr64-to-vDSO.patch
@@ -0,0 +1,35 @@
+From 348373388783ef8fc26c0cc7f2a6022fdc34c7fc Mon Sep 17 00:00:00 2001
+From: "H.J. Lu" <hjl.tools@gmail.com>
+Date: Fri, 16 Mar 2018 04:18:48 -0700
+Subject: [PATCH 44/47] x86/vdso: Insert endbr32/endbr64 to vDSO
+
+When Indirect Branch Tracking (IBT) is enabled, vDSO functions may be
+called indirectly, and must have ENDBR32 or ENDBR64 as the first
+instruction.  The compiler must support -fcf-protection=branch so that it
+can be used to compile vDSO.
+
+Signed-off-by: H.J. Lu <hjl.tools@gmail.com>
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Acked-by: Andy Lutomirski <luto@kernel.org>
+---
+ arch/x86/entry/vdso/Makefile | 4 ++++
+ 1 file changed, 4 insertions(+)
+
+diff --git a/arch/x86/entry/vdso/Makefile b/arch/x86/entry/vdso/Makefile
+index 433a1259f61d..6de3493acfe4 100644
+--- a/arch/x86/entry/vdso/Makefile
++++ b/arch/x86/entry/vdso/Makefile
+@@ -120,6 +120,10 @@ $(obj)/%-x32.o: $(obj)/%.o FORCE
+ 
+ targets += vdsox32.lds $(vobjx32s-y)
+ 
++ifdef CONFIG_X86_INTEL_BRANCH_TRACKING_USER
++    $(obj)/vclock_gettime.o $(obj)/vgetcpu.o $(obj)/vdso32/vclock_gettime.o: KBUILD_CFLAGS += -fcf-protection=branch
++endif
++
+ $(obj)/%.so: OBJCOPYFLAGS := -S
+ $(obj)/%.so: $(obj)/%.so.dbg FORCE
+ 	$(call if_changed,objcopy)
+-- 
+2.26.2
+
diff --git a/0045-x86-Disallow-vsyscall-emulation-when-CET-is-enabled.patch b/0045-x86-Disallow-vsyscall-emulation-when-CET-is-enabled.patch
new file mode 100644
index 000000000..7c929b96f
--- /dev/null
+++ b/0045-x86-Disallow-vsyscall-emulation-when-CET-is-enabled.patch
@@ -0,0 +1,59 @@
+From db93aeb7cf62532a740a82c6194424309b9272bb Mon Sep 17 00:00:00 2001
+From: "H.J. Lu" <hjl.tools@gmail.com>
+Date: Wed, 29 Jan 2020 08:44:11 -0800
+Subject: [PATCH 45/47] x86: Disallow vsyscall emulation when CET is enabled
+
+Emulation of the legacy vsyscall page is required by some programs built
+before 2013.  Newer programs after 2013 don't use it.  Disallow vsyscall
+emulation when Control-flow Enforcement (CET) is enabled to enhance
+security.
+
+Signed-off-by: H.J. Lu <hjl.tools@gmail.com>
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/Kconfig | 8 ++++++--
+ 1 file changed, 6 insertions(+), 2 deletions(-)
+
+diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
+index bb0e2a9feab7..a1d82f0bea6b 100644
+--- a/arch/x86/Kconfig
++++ b/arch/x86/Kconfig
+@@ -1211,7 +1211,7 @@ config X86_ESPFIX64
+ config X86_VSYSCALL_EMULATION
+ 	bool "Enable vsyscall emulation" if EXPERT
+ 	default y
+-	depends on X86_64
++	depends on X86_64 && !X86_INTEL_CET
+ 	---help---
+ 	 This enables emulation of the legacy vsyscall page.  Disabling
+ 	 it is roughly equivalent to booting with vsyscall=none, except
+@@ -1226,6 +1226,8 @@ config X86_VSYSCALL_EMULATION
+ 	 Disabling this option saves about 7K of kernel size and
+ 	 possibly 4K of additional runtime pagetable memory.
+ 
++	 This option is disabled when Intel CET is enabled.
++
+ config X86_IOPL_IOPERM
+ 	bool "IOPERM and IOPL Emulation"
+ 	default y
+@@ -2382,7 +2384,7 @@ config COMPAT_VDSO
+ 
+ choice
+ 	prompt "vsyscall table for legacy applications"
+-	depends on X86_64
++	depends on X86_64 && !X86_INTEL_CET
+ 	default LEGACY_VSYSCALL_XONLY
+ 	help
+ 	  Legacy user code that does not know how to find the vDSO expects
+@@ -2399,6 +2401,8 @@ choice
+ 
+ 	  If unsure, select "Emulate execution only".
+ 
++	  This option is not enabled when Intel CET is enabled.
++
+ 	config LEGACY_VSYSCALL_EMULATE
+ 		bool "Full emulation"
+ 		help
+-- 
+2.26.2
+
diff --git a/0046-powerpc-Keep-.rela-sections-when-CONFIG_RELOCATABLE-.patch b/0046-powerpc-Keep-.rela-sections-when-CONFIG_RELOCATABLE-.patch
new file mode 100644
index 000000000..c09e10bd7
--- /dev/null
+++ b/0046-powerpc-Keep-.rela-sections-when-CONFIG_RELOCATABLE-.patch
@@ -0,0 +1,56 @@
+From eecc5257e67990c5277abc0f9e69bb5fc8c13a99 Mon Sep 17 00:00:00 2001
+From: "H.J. Lu" <hjl.tools@gmail.com>
+Date: Mon, 27 Apr 2020 14:04:48 -0700
+Subject: [PATCH 46/47] powerpc: Keep .rela* sections when CONFIG_RELOCATABLE
+ is defined
+
+arch/powerpc/kernel/vmlinux.lds.S has
+
+ #ifdef CONFIG_RELOCATABLE
+ ...
+        .rela.dyn : AT(ADDR(.rela.dyn) - LOAD_OFFSET)
+        {
+                __rela_dyn_start = .;
+                *(.rela*)
+        }
+ #endif
+ ...
+        DISCARDS
+        /DISCARD/ : {
+                *(*.EMB.apuinfo)
+                *(.glink .iplt .plt .rela* .comment)
+                *(.gnu.version*)
+                *(.gnu.attributes)
+                *(.eh_frame)
+        }
+
+Since .rela* sections are needed when CONFIG_RELOCATABLE is defined,
+don't discard .rela* sections if CONFIG_RELOCATABLE is defined.
+
+Signed-off-by: H.J. Lu <hjl.tools@gmail.com>
+Acked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
+---
+ arch/powerpc/kernel/vmlinux.lds.S | 5 ++++-
+ 1 file changed, 4 insertions(+), 1 deletion(-)
+
+diff --git a/arch/powerpc/kernel/vmlinux.lds.S b/arch/powerpc/kernel/vmlinux.lds.S
+index 31a0f201fb6f..4ba07734a210 100644
+--- a/arch/powerpc/kernel/vmlinux.lds.S
++++ b/arch/powerpc/kernel/vmlinux.lds.S
+@@ -366,9 +366,12 @@ SECTIONS
+ 	DISCARDS
+ 	/DISCARD/ : {
+ 		*(*.EMB.apuinfo)
+-		*(.glink .iplt .plt .rela* .comment)
++		*(.glink .iplt .plt .comment)
+ 		*(.gnu.version*)
+ 		*(.gnu.attributes)
+ 		*(.eh_frame)
++#ifndef CONFIG_RELOCATABLE
++		*(.rela*)
++#endif
+ 	}
+ }
+-- 
+2.26.2
+
diff --git a/0047-Discard-.note.gnu.property-sections-in-generic-NOTES.patch b/0047-Discard-.note.gnu.property-sections-in-generic-NOTES.patch
new file mode 100644
index 000000000..37a422891
--- /dev/null
+++ b/0047-Discard-.note.gnu.property-sections-in-generic-NOTES.patch
@@ -0,0 +1,81 @@
+From 6b194ba038dc1936e319028eb95dd1cfe56aea8e Mon Sep 17 00:00:00 2001
+From: "H.J. Lu" <hjl.tools@gmail.com>
+Date: Thu, 30 Jan 2020 12:39:09 -0800
+Subject: [PATCH 47/47] Discard .note.gnu.property sections in generic NOTES
+
+With the command-line option, -mx86-used-note=yes, the x86 assembler
+in binutils 2.32 and above generates a program property note in a note
+section, .note.gnu.property, to encode used x86 ISAs and features.  But
+kernel linker script only contains a single NOTE segment:
+
+PHDRS {
+ text PT_LOAD FLAGS(5);
+ data PT_LOAD FLAGS(6);
+ percpu PT_LOAD FLAGS(6);
+ init PT_LOAD FLAGS(7);
+ note PT_NOTE FLAGS(0);
+}
+SECTIONS
+{
+...
+ .notes : AT(ADDR(.notes) - 0xffffffff80000000) { __start_notes = .; KEEP(*(.not
+e.*)) __stop_notes = .; } :text :note
+...
+}
+
+The NOTE segment generated by kernel linker script is aligned to 4 bytes.
+But .note.gnu.property section must be aligned to 8 bytes on x86-64 and
+we get
+
+[hjl@gnu-skx-1 linux]$ readelf -n vmlinux
+
+Displaying notes found in: .notes
+  Owner                Data size Description
+  Xen                  0x00000006 Unknown note type: (0x00000006)
+   description data: 6c 69 6e 75 78 00
+  Xen                  0x00000004 Unknown note type: (0x00000007)
+   description data: 32 2e 36 00
+  xen-3.0              0x00000005 Unknown note type: (0x006e6558)
+   description data: 08 00 00 00 03
+readelf: Warning: note with invalid namesz and/or descsz found at offset 0x50
+readelf: Warning:  type: 0xffffffff, namesize: 0x006e6558, descsize:
+0x80000000, alignment: 8
+[hjl@gnu-skx-1 linux]$
+
+Since note.gnu.property section in kernel image is never used, this patch
+discards .note.gnu.property sections in kernel linker script by adding
+
+/DISCARD/ : {
+  *(.note.gnu.property)
+}
+
+before kernel NOTE segment in generic NOTES.
+
+Signed-off-by: H.J. Lu <hjl.tools@gmail.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+---
+ include/asm-generic/vmlinux.lds.h | 7 +++++++
+ 1 file changed, 7 insertions(+)
+
+diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
+index 71e387a5fe90..95cd678428f4 100644
+--- a/include/asm-generic/vmlinux.lds.h
++++ b/include/asm-generic/vmlinux.lds.h
+@@ -833,7 +833,14 @@
+ #define TRACEDATA
+ #endif
+ 
++/*
++ * Discard .note.gnu.property sections which are unused and have
++ * different alignment requirement from kernel note sections.
++ */
+ #define NOTES								\
++	/DISCARD/ : {							\
++		*(.note.gnu.property)					\
++	}								\
+ 	.notes : AT(ADDR(.notes) - LOAD_OFFSET) {			\
+ 		__start_notes = .;					\
+ 		KEEP(*(.note.*))					\
+-- 
+2.26.2
+
diff --git a/kernel.spec b/kernel.spec
index 564cb8e83..8f74b5912 100644
--- a/kernel.spec
+++ b/kernel.spec
@@ -1,3 +1,51 @@
+Patch200001: 0001-x86-fpu-xstate-Rename-validate_xstate_header-to-vali.patch
+Patch200002: 0002-x86-fpu-xstate-Define-new-macros-for-supervisor-and-.patch
+Patch200003: 0003-x86-fpu-xstate-Separate-user-and-supervisor-xfeature.patch
+Patch200004: 0004-x86-fpu-xstate-Introduce-XSAVES-supervisor-states.patch
+Patch200005: 0005-x86-fpu-xstate-Define-new-functions-for-clearing-fpr.patch
+Patch200006: 0006-x86-fpu-xstate-Update-sanitize_restored_xstate-for-s.patch
+Patch200007: 0007-x86-fpu-xstate-Update-copy_kernel_to_xregs_err-for-X.patch
+Patch200008: 0008-x86-fpu-Introduce-copy_supervisor_to_kernel.patch
+Patch200009: 0009-x86-fpu-xstate-Preserve-supervisor-states-for-slow-p.patch
+Patch200010: 0010-x86-fpu-xstate-Restore-supervisor-states-for-signal-.patch
+Patch200011: 0011-Documentation-x86-Add-CET-description.patch
+Patch200012: 0012-x86-cpufeatures-Add-CET-CPU-feature-flags-for-Contro.patch
+Patch200013: 0013-x86-fpu-xstate-Introduce-CET-MSR-XSAVES-supervisor-s.patch
+Patch200014: 0014-x86-cet-Add-control-protection-fault-handler.patch
+Patch200015: 0015-x86-cet-shstk-Add-Kconfig-option-for-user-mode-Shado.patch
+Patch200016: 0016-x86-mm-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_HW.patch
+Patch200017: 0017-x86-mm-Remove-_PAGE_DIRTY_HW-from-kernel-RO-pages.patch
+Patch200018: 0018-x86-mm-Introduce-_PAGE_COW.patch
+Patch200019: 0019-drm-i915-gvt-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_BITS.patch
+Patch200020: 0020-x86-mm-Update-pte_modify-for-_PAGE_COW.patch
+Patch200021: 0021-x86-mm-Update-ptep_set_wrprotect-and-pmdp_set_wrprot.patch
+Patch200022: 0022-mm-Introduce-VM_SHSTK-for-shadow-stack-memory.patch
+Patch200023: 0023-x86-mm-Shadow-Stack-page-fault-error-checking.patch
+Patch200024: 0024-x86-mm-Update-maybe_mkwrite-for-shadow-stack.patch
+Patch200025: 0025-mm-Fixup-places-that-call-pte_mkwrite-directly.patch
+Patch200026: 0026-mm-Add-guard-pages-around-a-shadow-stack.patch
+Patch200027: 0027-mm-mmap-Add-shadow-stack-pages-to-memory-accounting.patch
+Patch200028: 0028-mm-Update-can_follow_write_pte-for-shadow-stack.patch
+Patch200029: 0029-x86-cet-shstk-User-mode-shadow-stack-support.patch
+Patch200030: 0030-x86-cet-shstk-Handle-signals-for-shadow-stack.patch
+Patch200031: 0031-ELF-UAPI-and-Kconfig-additions-for-ELF-program-prope.patch
+Patch200032: 0032-ELF-Add-ELF-program-property-parsing-support.patch
+Patch200033: 0033-ELF-Introduce-arch_setup_elf_property.patch
+Patch200034: 0034-x86-cet-shstk-ELF-header-parsing-for-shadow-stack.patch
+Patch200035: 0035-x86-cet-shstk-Handle-thread-shadow-stack.patch
+Patch200036: 0036-x86-cet-shstk-Add-arch_prctl-functions-for-shadow-st.patch
+Patch200037: 0037-x86-cet-ibt-Add-Kconfig-option-for-user-mode-Indirec.patch
+Patch200038: 0038-x86-cet-ibt-User-mode-Indirect-Branch-Tracking-suppo.patch
+Patch200039: 0039-x86-cet-ibt-Handle-signals-for-Indirect-Branch-Track.patch
+Patch200040: 0040-x86-cet-ibt-ELF-header-parsing-for-Indirect-Branch-T.patch
+Patch200041: 0041-x86-cet-ibt-Add-arch_prctl-functions-for-Indirect-Br.patch
+Patch200042: 0042-x86-cet-Add-PTRACE-interface-for-CET.patch
+Patch200043: 0043-x86-vdso-32-Add-ENDBR32-to-__kernel_vsyscall-entry-p.patch
+Patch200044: 0044-x86-vdso-Insert-endbr32-endbr64-to-vDSO.patch
+Patch200045: 0045-x86-Disallow-vsyscall-emulation-when-CET-is-enabled.patch
+Patch200046: 0046-powerpc-Keep-.rela-sections-when-CONFIG_RELOCATABLE-.patch
+Patch200047: 0047-Discard-.note.gnu.property-sections-in-generic-NOTES.patch
+
 # We have to override the new %%install behavior because, well... the kernel is special.
 %global __spec_install_pre %{___build_pre}
 
-- 
2.26.2

