From ee57c1845e240b751af48653024c314be422c5e6 Mon Sep 17 00:00:00 2001
From: "H.J. Lu" <hjl.tools@gmail.com>
Date: Tue, 14 Apr 2020 10:43:52 -0700
Subject: [PATCH 3/5] Apply Intel CET patches for Linux kernel 5.6

---
 ...ix-last_good_offset-in-setup_xstate_.patch |  90 +++
 ...ix-XSAVES-offsets-in-setup_xstate_co.patch | 112 ++++
 ...arn-when-checking-alignment-of-disab.patch |  50 ++
 ...ename-validate_xstate_header-to-vali.patch |  98 +++
 ...efine-new-macros-for-supervisor-and-.patch | 184 ++++++
 ...eparate-user-and-supervisor-xfeature.patch | 353 ++++++++++
 ...e-Introduce-XSAVES-supervisor-states.patch |  85 +++
 ...efine-new-functions-for-clearing-fpr.patch | 182 +++++
 ...pdate-sanitize_restored_xstate-for-s.patch | 137 ++++
 ...pdate-copy_kernel_to_xregs_err-for-X.patch |  47 ++
 ...-Introduce-copy_supervisor_to_kernel.patch | 148 +++++
 ...estore-supervisor-xstates-for-__fpu_.patch | 100 +++
 ...ocumentation-x86-Add-CET-description.patch | 195 ++++++
 ...Add-CET-CPU-feature-flags-for-Contro.patch |  55 ++
 ...ntroduce-CET-MSR-XSAVES-supervisor-s.patch | 203 ++++++
 ...Add-control-protection-fault-handler.patch | 188 ++++++
 ...d-Kconfig-option-for-user-mode-Shado.patch |  83 +++
 ...uce-VM_SHSTK-for-Shadow-Stack-memory.patch |  83 +++
 ...dd-guard-pages-around-a-Shadow-Stack.patch | 100 +++
 ...Change-_PAGE_DIRTY-to-_PAGE_DIRTY_HW.patch | 196 ++++++
 0021-x86-mm-Introduce-_PAGE_DIRTY_SW.patch    | 327 +++++++++
 ...e_modify-pmd_modify-and-_PAGE_CHG_MA.patch |  71 ++
 ...ange-_PAGE_DIRTY-to-_PAGE_DIRTY_BITS.patch |  30 +
 ...ep_set_wrprotect-and-pmdp_set_wrprot.patch | 126 ++++
 ...adow-Stack-page-fault-error-checking.patch |  79 +++
 0026-mm-Handle-Shadow-Stack-page-fault.patch  | 121 ++++
 ...-THP-HugeTLB-Shadow-Stack-page-fault.patch | 100 +++
 ...an_follow_write_pte-for-Shadow-Stack.patch | 158 +++++
 ...shstk-User-mode-Shadow-Stack-support.patch | 356 ++++++++++
 ...et-shstk-Introduce-WRUSS-instruction.patch |  66 ++
 ...hstk-Handle-signals-for-Shadow-Stack.patch | 496 ++++++++++++++
 ...nfig-additions-for-ELF-program-prope.patch |  85 +++
 ...ELF-program-property-parsing-support.patch | 310 +++++++++
 ...LF-Introduce-arch_setup_elf_property.patch |  55 ++
 ...-ELF-header-parsing-for-Shadow-Stack.patch |  95 +++
 ...cet-shstk-Handle-thread-Shadow-Stack.patch | 161 +++++
 ...dow-Stack-pages-to-memory-accounting.patch |  41 ++
 ...d-arch_prctl-functions-for-Shadow-St.patch | 240 +++++++
 ...Kconfig-option-for-user-mode-Indirec.patch |  52 ++
 ...-mode-Indirect-Branch-Tracking-suppo.patch | 180 +++++
 ...le-signals-for-Indirect-Branch-Track.patch | 108 +++
 ...header-parsing-for-Indirect-Branch-T.patch |  52 ++
 ...arch_prctl-functions-for-Indirect-Br.patch |  40 ++
 ...x86-cet-Add-PTRACE-interface-for-CET.patch | 147 ++++
 ...trol-flow-Enforcement-CET-instructio.patch | 165 +++++
 ...ols-Add-CET-instructions-to-the-new-.patch | 625 ++++++++++++++++++
 ...-.note.gnu.property-sections-in-vDSO.patch |  69 ++
 ...IME_DISCARD_EXIT-to-generic-DISCARDS.patch |  54 ++
 ...u.property-sections-in-generic-NOTES.patch |  81 +++
 ...ENDBR32-to-__kernel_vsyscall-entry-p.patch |  32 +
 ...-vdso-Insert-endbr32-endbr64-to-vDSO.patch |  35 +
 ...yscall-emulation-when-CET-is-enabled.patch |  59 ++
 kernel.spec                                   |  53 ++
 53 files changed, 7358 insertions(+)
 create mode 100644 0001-x86-fpu-xstate-Fix-last_good_offset-in-setup_xstate_.patch
 create mode 100644 0002-x86-fpu-xstate-Fix-XSAVES-offsets-in-setup_xstate_co.patch
 create mode 100644 0003-x86-fpu-xstate-Warn-when-checking-alignment-of-disab.patch
 create mode 100644 0004-x86-fpu-xstate-Rename-validate_xstate_header-to-vali.patch
 create mode 100644 0005-x86-fpu-xstate-Define-new-macros-for-supervisor-and-.patch
 create mode 100644 0006-x86-fpu-xstate-Separate-user-and-supervisor-xfeature.patch
 create mode 100644 0007-x86-fpu-xstate-Introduce-XSAVES-supervisor-states.patch
 create mode 100644 0008-x86-fpu-xstate-Define-new-functions-for-clearing-fpr.patch
 create mode 100644 0009-x86-fpu-xstate-Update-sanitize_restored_xstate-for-s.patch
 create mode 100644 0010-x86-fpu-xstate-Update-copy_kernel_to_xregs_err-for-X.patch
 create mode 100644 0011-x86-fpu-Introduce-copy_supervisor_to_kernel.patch
 create mode 100644 0012-x86-fpu-xstate-Restore-supervisor-xstates-for-__fpu_.patch
 create mode 100644 0013-Documentation-x86-Add-CET-description.patch
 create mode 100644 0014-x86-cpufeatures-Add-CET-CPU-feature-flags-for-Contro.patch
 create mode 100644 0015-x86-fpu-xstate-Introduce-CET-MSR-XSAVES-supervisor-s.patch
 create mode 100644 0016-x86-cet-Add-control-protection-fault-handler.patch
 create mode 100644 0017-x86-cet-shstk-Add-Kconfig-option-for-user-mode-Shado.patch
 create mode 100644 0018-mm-Introduce-VM_SHSTK-for-Shadow-Stack-memory.patch
 create mode 100644 0019-Add-guard-pages-around-a-Shadow-Stack.patch
 create mode 100644 0020-x86-mm-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_HW.patch
 create mode 100644 0021-x86-mm-Introduce-_PAGE_DIRTY_SW.patch
 create mode 100644 0022-x86-mm-Update-pte_modify-pmd_modify-and-_PAGE_CHG_MA.patch
 create mode 100644 0023-drm-i915-gvt-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_BITS.patch
 create mode 100644 0024-x86-mm-Modify-ptep_set_wrprotect-and-pmdp_set_wrprot.patch
 create mode 100644 0025-x86-mm-Shadow-Stack-page-fault-error-checking.patch
 create mode 100644 0026-mm-Handle-Shadow-Stack-page-fault.patch
 create mode 100644 0027-mm-Handle-THP-HugeTLB-Shadow-Stack-page-fault.patch
 create mode 100644 0028-mm-Update-can_follow_write_pte-for-Shadow-Stack.patch
 create mode 100644 0029-x86-cet-shstk-User-mode-Shadow-Stack-support.patch
 create mode 100644 0030-x86-cet-shstk-Introduce-WRUSS-instruction.patch
 create mode 100644 0031-x86-cet-shstk-Handle-signals-for-Shadow-Stack.patch
 create mode 100644 0032-ELF-UAPI-and-Kconfig-additions-for-ELF-program-prope.patch
 create mode 100644 0033-ELF-Add-ELF-program-property-parsing-support.patch
 create mode 100644 0034-ELF-Introduce-arch_setup_elf_property.patch
 create mode 100644 0035-x86-cet-shstk-ELF-header-parsing-for-Shadow-Stack.patch
 create mode 100644 0036-x86-cet-shstk-Handle-thread-Shadow-Stack.patch
 create mode 100644 0037-mm-mmap-Add-Shadow-Stack-pages-to-memory-accounting.patch
 create mode 100644 0038-x86-cet-shstk-Add-arch_prctl-functions-for-Shadow-St.patch
 create mode 100644 0039-x86-cet-ibt-Add-Kconfig-option-for-user-mode-Indirec.patch
 create mode 100644 0040-x86-cet-ibt-User-mode-Indirect-Branch-Tracking-suppo.patch
 create mode 100644 0041-x86-cet-ibt-Handle-signals-for-Indirect-Branch-Track.patch
 create mode 100644 0042-x86-cet-ibt-ELF-header-parsing-for-Indirect-Branch-T.patch
 create mode 100644 0043-x86-cet-ibt-Add-arch_prctl-functions-for-Indirect-Br.patch
 create mode 100644 0044-x86-cet-Add-PTRACE-interface-for-CET.patch
 create mode 100644 0045-x86-insn-Add-Control-flow-Enforcement-CET-instructio.patch
 create mode 100644 0046-x86-insn-perf-tools-Add-CET-instructions-to-the-new-.patch
 create mode 100644 0047-x86-Discard-.note.gnu.property-sections-in-vDSO.patch
 create mode 100644 0048-Add-RUNTIME_DISCARD_EXIT-to-generic-DISCARDS.patch
 create mode 100644 0049-Discard-.note.gnu.property-sections-in-generic-NOTES.patch
 create mode 100644 0050-x86-vdso-32-Add-ENDBR32-to-__kernel_vsyscall-entry-p.patch
 create mode 100644 0051-x86-vdso-Insert-endbr32-endbr64-to-vDSO.patch
 create mode 100644 0052-x86-Disallow-vsyscall-emulation-when-CET-is-enabled.patch

diff --git a/0001-x86-fpu-xstate-Fix-last_good_offset-in-setup_xstate_.patch b/0001-x86-fpu-xstate-Fix-last_good_offset-in-setup_xstate_.patch
new file mode 100644
index 000000000..53328493e
--- /dev/null
+++ b/0001-x86-fpu-xstate-Fix-last_good_offset-in-setup_xstate_.patch
@@ -0,0 +1,90 @@
+From 8b344cfb53bf9d405044ae8a6316d8a624179d99 Mon Sep 17 00:00:00 2001
+From: tip-bot2 for Yu-cheng Yu <tip-bot2@linutronix.de>
+Date: Wed, 12 Feb 2020 14:50:50 +0000
+Subject: [PATCH 01/52] x86/fpu/xstate: Fix last_good_offset in
+ setup_xstate_features()
+
+The following commit has been merged into the x86/fpu branch of tip:
+
+Commit-ID:     c12e13dcd814023a903399ec5ac2e7082d664b8b
+Gitweb:        https://git.kernel.org/tip/c12e13dcd814023a903399ec5ac2e7082d664b8b
+Author:        Yu-cheng Yu <yu-cheng.yu@intel.com>
+AuthorDate:    Thu, 09 Jan 2020 13:14:50 -08:00
+Committer:     Borislav Petkov <bp@suse.de>
+CommitterDate: Tue, 11 Feb 2020 19:54:04 +01:00
+
+x86/fpu/xstate: Fix last_good_offset in setup_xstate_features()
+
+The function setup_xstate_features() uses CPUID to find each xfeature's
+standard-format offset and size.  Since XSAVES always uses the compacted
+format, supervisor xstates are *NEVER* in the standard-format and their
+offsets are left as -1's.  However, they are still being tracked as
+last_good_offset.
+
+Fix it by tracking only user xstate offsets.
+
+ [ bp: Use xfeature_is_supervisor() and save an indentation level. Drop
+   now unused xfeature_is_user(). ]
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Signed-off-by: Borislav Petkov <bp@suse.de>
+Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
+Link: https://lkml.kernel.org/r/20200109211452.27369-2-yu-cheng.yu@intel.com
+---
+ arch/x86/kernel/fpu/xstate.c | 27 +++++++++++++--------------
+ 1 file changed, 13 insertions(+), 14 deletions(-)
+
+diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c
+index a1806598aaa4..fe67cabfb4a1 100644
+--- a/arch/x86/kernel/fpu/xstate.c
++++ b/arch/x86/kernel/fpu/xstate.c
+@@ -120,11 +120,6 @@ static bool xfeature_is_supervisor(int xfeature_nr)
+ 	return ecx & 1;
+ }
+ 
+-static bool xfeature_is_user(int xfeature_nr)
+-{
+-	return !xfeature_is_supervisor(xfeature_nr);
+-}
+-
+ /*
+  * When executing XSAVEOPT (or other optimized XSAVE instructions), if
+  * a processor implementation detects that an FPU state component is still
+@@ -265,21 +260,25 @@ static void __init setup_xstate_features(void)
+ 
+ 		cpuid_count(XSTATE_CPUID, i, &eax, &ebx, &ecx, &edx);
+ 
++		xstate_sizes[i] = eax;
++
+ 		/*
+-		 * If an xfeature is supervisor state, the offset
+-		 * in EBX is invalid. We leave it to -1.
++		 * If an xfeature is supervisor state, the offset in EBX is
++		 * invalid, leave it to -1.
+ 		 */
+-		if (xfeature_is_user(i))
+-			xstate_offsets[i] = ebx;
++		if (xfeature_is_supervisor(i))
++			continue;
++
++		xstate_offsets[i] = ebx;
+ 
+-		xstate_sizes[i] = eax;
+ 		/*
+-		 * In our xstate size checks, we assume that the
+-		 * highest-numbered xstate feature has the
+-		 * highest offset in the buffer.  Ensure it does.
++		 * In our xstate size checks, we assume that the highest-numbered
++		 * xstate feature has the highest offset in the buffer.  Ensure
++		 * it does.
+ 		 */
+ 		WARN_ONCE(last_good_offset > xstate_offsets[i],
+-			"x86/fpu: misordered xstate at %d\n", last_good_offset);
++			  "x86/fpu: misordered xstate at %d\n", last_good_offset);
++
+ 		last_good_offset = xstate_offsets[i];
+ 	}
+ }
+-- 
+2.26.0
+
diff --git a/0002-x86-fpu-xstate-Fix-XSAVES-offsets-in-setup_xstate_co.patch b/0002-x86-fpu-xstate-Fix-XSAVES-offsets-in-setup_xstate_co.patch
new file mode 100644
index 000000000..0a1a80449
--- /dev/null
+++ b/0002-x86-fpu-xstate-Fix-XSAVES-offsets-in-setup_xstate_co.patch
@@ -0,0 +1,112 @@
+From f838e88f4cee1bcaafb56a40003a00dc584de7bf Mon Sep 17 00:00:00 2001
+From: tip-bot2 for Yu-cheng Yu <tip-bot2@linutronix.de>
+Date: Wed, 12 Feb 2020 14:50:49 +0000
+Subject: [PATCH 02/52] x86/fpu/xstate: Fix XSAVES offsets in
+ setup_xstate_comp()
+
+The following commit has been merged into the x86/fpu branch of tip:
+
+Commit-ID:     49a91d61aed1db01097b51a24c77137eb348a0bf
+Gitweb:        https://git.kernel.org/tip/49a91d61aed1db01097b51a24c77137eb348a0bf
+Author:        Yu-cheng Yu <yu-cheng.yu@intel.com>
+AuthorDate:    Thu, 09 Jan 2020 13:14:51 -08:00
+Committer:     Borislav Petkov <bp@suse.de>
+CommitterDate: Wed, 12 Feb 2020 15:43:31 +01:00
+
+x86/fpu/xstate: Fix XSAVES offsets in setup_xstate_comp()
+
+In setup_xstate_comp(), each XSAVES component offset starts from the
+end of its preceding component plus alignment. A disabled feature does
+not take space and its offset should be set to the end of its preceding
+one with no alignment. However, in this case, alignment is incorrectly
+added to the offset, which can cause the next component to have a wrong
+offset.
+
+This problem has not been visible because currently there is no xfeature
+requiring alignment.
+
+Fix it by tracking the next starting offset only from enabled
+xfeatures. To make it clear, also change the function name to
+setup_xstate_comp_offsets().
+
+ [ bp: Fix a typo in the comment above it, while at it. ]
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Signed-off-by: Borislav Petkov <bp@suse.de>
+Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
+Link: https://lkml.kernel.org/r/20200109211452.27369-3-yu-cheng.yu@intel.com
+---
+ arch/x86/kernel/fpu/xstate.c | 32 ++++++++++++--------------------
+ 1 file changed, 12 insertions(+), 20 deletions(-)
+
+diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c
+index fe67cabfb4a1..edcaacd583d8 100644
+--- a/arch/x86/kernel/fpu/xstate.c
++++ b/arch/x86/kernel/fpu/xstate.c
+@@ -337,11 +337,11 @@ static int xfeature_is_aligned(int xfeature_nr)
+ /*
+  * This function sets up offsets and sizes of all extended states in
+  * xsave area. This supports both standard format and compacted format
+- * of the xsave aread.
++ * of the xsave area.
+  */
+-static void __init setup_xstate_comp(void)
++static void __init setup_xstate_comp_offsets(void)
+ {
+-	unsigned int xstate_comp_sizes[XFEATURE_MAX];
++	unsigned int next_offset;
+ 	int i;
+ 
+ 	/*
+@@ -355,31 +355,23 @@ static void __init setup_xstate_comp(void)
+ 
+ 	if (!boot_cpu_has(X86_FEATURE_XSAVES)) {
+ 		for (i = FIRST_EXTENDED_XFEATURE; i < XFEATURE_MAX; i++) {
+-			if (xfeature_enabled(i)) {
++			if (xfeature_enabled(i))
+ 				xstate_comp_offsets[i] = xstate_offsets[i];
+-				xstate_comp_sizes[i] = xstate_sizes[i];
+-			}
+ 		}
+ 		return;
+ 	}
+ 
+-	xstate_comp_offsets[FIRST_EXTENDED_XFEATURE] =
+-		FXSAVE_SIZE + XSAVE_HDR_SIZE;
++	next_offset = FXSAVE_SIZE + XSAVE_HDR_SIZE;
+ 
+ 	for (i = FIRST_EXTENDED_XFEATURE; i < XFEATURE_MAX; i++) {
+-		if (xfeature_enabled(i))
+-			xstate_comp_sizes[i] = xstate_sizes[i];
+-		else
+-			xstate_comp_sizes[i] = 0;
++		if (!xfeature_enabled(i))
++			continue;
+ 
+-		if (i > FIRST_EXTENDED_XFEATURE) {
+-			xstate_comp_offsets[i] = xstate_comp_offsets[i-1]
+-					+ xstate_comp_sizes[i-1];
++		if (xfeature_is_aligned(i))
++			next_offset = ALIGN(next_offset, 64);
+ 
+-			if (xfeature_is_aligned(i))
+-				xstate_comp_offsets[i] =
+-					ALIGN(xstate_comp_offsets[i], 64);
+-		}
++		xstate_comp_offsets[i] = next_offset;
++		next_offset += xstate_sizes[i];
+ 	}
+ }
+ 
+@@ -773,7 +765,7 @@ void __init fpu__init_system_xstate(void)
+ 
+ 	fpu__init_prepare_fx_sw_frame();
+ 	setup_init_fpu_buf();
+-	setup_xstate_comp();
++	setup_xstate_comp_offsets();
+ 	print_xstate_offset_size();
+ 
+ 	pr_info("x86/fpu: Enabled xstate features 0x%llx, context size is %d bytes, using '%s' format.\n",
+-- 
+2.26.0
+
diff --git a/0003-x86-fpu-xstate-Warn-when-checking-alignment-of-disab.patch b/0003-x86-fpu-xstate-Warn-when-checking-alignment-of-disab.patch
new file mode 100644
index 000000000..c10616b98
--- /dev/null
+++ b/0003-x86-fpu-xstate-Warn-when-checking-alignment-of-disab.patch
@@ -0,0 +1,50 @@
+From fed1c7ac2113928002fbb75c621c2add91290680 Mon Sep 17 00:00:00 2001
+From: tip-bot2 for Yu-cheng Yu <tip-bot2@linutronix.de>
+Date: Wed, 12 Feb 2020 14:50:49 +0000
+Subject: [PATCH 03/52] x86/fpu/xstate: Warn when checking alignment of
+ disabled xfeatures
+
+The following commit has been merged into the x86/fpu branch of tip:
+
+Commit-ID:     e70b100806d63fb79775858ea92e1a716da46186
+Gitweb:        https://git.kernel.org/tip/e70b100806d63fb79775858ea92e1a716da46186
+Author:        Yu-cheng Yu <yu-cheng.yu@intel.com>
+AuthorDate:    Thu, 09 Jan 2020 13:14:52 -08:00
+Committer:     Borislav Petkov <bp@suse.de>
+CommitterDate: Wed, 12 Feb 2020 15:43:34 +01:00
+
+x86/fpu/xstate: Warn when checking alignment of disabled xfeatures
+
+An XSAVES component's alignment/offset is meaningful only when the
+feature is enabled. Return zero and WARN_ONCE on checking alignment of
+disabled features.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Signed-off-by: Borislav Petkov <bp@suse.de>
+Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
+Link: https://lkml.kernel.org/r/20200109211452.27369-4-yu-cheng.yu@intel.com
+---
+ arch/x86/kernel/fpu/xstate.c | 7 +++++++
+ 1 file changed, 7 insertions(+)
+
+diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c
+index edcaacd583d8..73fe5979629c 100644
+--- a/arch/x86/kernel/fpu/xstate.c
++++ b/arch/x86/kernel/fpu/xstate.c
+@@ -325,6 +325,13 @@ static int xfeature_is_aligned(int xfeature_nr)
+ 	u32 eax, ebx, ecx, edx;
+ 
+ 	CHECK_XFEATURE(xfeature_nr);
++
++	if (!xfeature_enabled(xfeature_nr)) {
++		WARN_ONCE(1, "Checking alignment of disabled xfeature %d\n",
++			  xfeature_nr);
++		return 0;
++	}
++
+ 	cpuid_count(XSTATE_CPUID, xfeature_nr, &eax, &ebx, &ecx, &edx);
+ 	/*
+ 	 * The value returned by ECX[1] indicates the alignment
+-- 
+2.26.0
+
diff --git a/0004-x86-fpu-xstate-Rename-validate_xstate_header-to-vali.patch b/0004-x86-fpu-xstate-Rename-validate_xstate_header-to-vali.patch
new file mode 100644
index 000000000..792e15358
--- /dev/null
+++ b/0004-x86-fpu-xstate-Rename-validate_xstate_header-to-vali.patch
@@ -0,0 +1,98 @@
+From 7902602a6b4ecbf5f4819029b74749ac416f37ce Mon Sep 17 00:00:00 2001
+From: Fenghua Yu <fenghua.yu@intel.com>
+Date: Wed, 13 Dec 2017 16:08:28 -0800
+Subject: [PATCH 04/52] x86/fpu/xstate: Rename validate_xstate_header() to
+ validate_user_xstate_header()
+
+The function validate_xstate_header() validates an xstate header coming
+from userspace (PTRACE or sigreturn).  To make it clear, rename it to
+validate_user_xstate_header().
+
+v3:
+- Change validate_xstate_header_from_user() to validate_user_xstate_header().
+
+Suggested-by: Dave Hansen <dave.hansen@intel.com>
+Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
+Reviewed-by: Tony Luck <tony.luck@intel.com>
+Reviewed-by: Borislav Petkov <bp@suse.de>
+---
+ arch/x86/include/asm/fpu/xstate.h | 2 +-
+ arch/x86/kernel/fpu/regset.c      | 2 +-
+ arch/x86/kernel/fpu/signal.c      | 2 +-
+ arch/x86/kernel/fpu/xstate.c      | 6 +++---
+ 4 files changed, 6 insertions(+), 6 deletions(-)
+
+diff --git a/arch/x86/include/asm/fpu/xstate.h b/arch/x86/include/asm/fpu/xstate.h
+index c6136d79f8c0..fc4db51f3b53 100644
+--- a/arch/x86/include/asm/fpu/xstate.h
++++ b/arch/x86/include/asm/fpu/xstate.h
+@@ -56,6 +56,6 @@ int copy_kernel_to_xstate(struct xregs_state *xsave, const void *kbuf);
+ int copy_user_to_xstate(struct xregs_state *xsave, const void __user *ubuf);
+ 
+ /* Validate an xstate header supplied by userspace (ptrace or sigreturn) */
+-extern int validate_xstate_header(const struct xstate_header *hdr);
++int validate_user_xstate_header(const struct xstate_header *hdr);
+ 
+ #endif
+diff --git a/arch/x86/kernel/fpu/regset.c b/arch/x86/kernel/fpu/regset.c
+index d652b939ccfb..bd1d0649f8ce 100644
+--- a/arch/x86/kernel/fpu/regset.c
++++ b/arch/x86/kernel/fpu/regset.c
+@@ -139,7 +139,7 @@ int xstateregs_set(struct task_struct *target, const struct user_regset *regset,
+ 	} else {
+ 		ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, xsave, 0, -1);
+ 		if (!ret)
+-			ret = validate_xstate_header(&xsave->header);
++			ret = validate_user_xstate_header(&xsave->header);
+ 	}
+ 
+ 	/*
+diff --git a/arch/x86/kernel/fpu/signal.c b/arch/x86/kernel/fpu/signal.c
+index 400a05e1c1c5..585e3651b98f 100644
+--- a/arch/x86/kernel/fpu/signal.c
++++ b/arch/x86/kernel/fpu/signal.c
+@@ -366,7 +366,7 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 			ret = __copy_from_user(&fpu->state.xsave, buf_fx, state_size);
+ 
+ 			if (!ret && state_size > offsetof(struct xregs_state, header))
+-				ret = validate_xstate_header(&fpu->state.xsave.header);
++				ret = validate_user_xstate_header(&fpu->state.xsave.header);
+ 		}
+ 		if (ret)
+ 			goto err_out;
+diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c
+index 73fe5979629c..5a580e403044 100644
+--- a/arch/x86/kernel/fpu/xstate.c
++++ b/arch/x86/kernel/fpu/xstate.c
+@@ -472,7 +472,7 @@ int using_compacted_format(void)
+ }
+ 
+ /* Validate an xstate header supplied by userspace (ptrace or sigreturn) */
+-int validate_xstate_header(const struct xstate_header *hdr)
++int validate_user_xstate_header(const struct xstate_header *hdr)
+ {
+ 	/* No unknown or supervisor features may be set */
+ 	if (hdr->xfeatures & (~xfeatures_mask | XFEATURE_MASK_SUPERVISOR))
+@@ -1142,7 +1142,7 @@ int copy_kernel_to_xstate(struct xregs_state *xsave, const void *kbuf)
+ 
+ 	memcpy(&hdr, kbuf + offset, size);
+ 
+-	if (validate_xstate_header(&hdr))
++	if (validate_user_xstate_header(&hdr))
+ 		return -EINVAL;
+ 
+ 	for (i = 0; i < XFEATURE_MAX; i++) {
+@@ -1196,7 +1196,7 @@ int copy_user_to_xstate(struct xregs_state *xsave, const void __user *ubuf)
+ 	if (__copy_from_user(&hdr, ubuf + offset, size))
+ 		return -EFAULT;
+ 
+-	if (validate_xstate_header(&hdr))
++	if (validate_user_xstate_header(&hdr))
+ 		return -EINVAL;
+ 
+ 	for (i = 0; i < XFEATURE_MAX; i++) {
+-- 
+2.26.0
+
diff --git a/0005-x86-fpu-xstate-Define-new-macros-for-supervisor-and-.patch b/0005-x86-fpu-xstate-Define-new-macros-for-supervisor-and-.patch
new file mode 100644
index 000000000..5508e8fcd
--- /dev/null
+++ b/0005-x86-fpu-xstate-Define-new-macros-for-supervisor-and-.patch
@@ -0,0 +1,184 @@
+From 899252853e9aae71e07b923d061bf68da48546c9 Mon Sep 17 00:00:00 2001
+From: Fenghua Yu <fenghua.yu@intel.com>
+Date: Thu, 10 Nov 2016 10:13:56 -0800
+Subject: [PATCH 05/52] x86/fpu/xstate: Define new macros for supervisor and
+ user xstates
+
+XCNTXT_MASK is 'all supported xfeatures' before introducing supervisor
+xstates.  Rename it to XFEATURE_MASK_USER_SUPPORTED to make clear that
+these are user xstates.
+
+XFEATURE_MASK_SUPERVISOR is replaced with the following:
+- XFEATURE_MASK_SUPERVISOR_SUPPORTED: Currently nothing.  ENQCMD and
+  Control-flow Enforcement Technology (CET) will be introduced in separate
+  series.
+- XFEATURE_MASK_SUPERVISOR_UNSUPPORTED: Currently only Processor Trace.
+- XFEATURE_MASK_SUPERVISOR_ALL: the combination of above.
+
+v3:
+- Change SUPPORTED_XFEATURES_*, UNSUPPORTED_XFEATURES_*, ALL_XFEATURES_* to
+  XFEATURE_MASK_*.
+
+Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
+Co-developed-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
+Reviewed-by: Tony Luck <tony.luck@intel.com>
+---
+ arch/x86/include/asm/fpu/xstate.h | 36 ++++++++++++++++++++-----------
+ arch/x86/kernel/fpu/init.c        |  3 ++-
+ arch/x86/kernel/fpu/xstate.c      | 26 +++++++++++-----------
+ 3 files changed, 38 insertions(+), 27 deletions(-)
+
+diff --git a/arch/x86/include/asm/fpu/xstate.h b/arch/x86/include/asm/fpu/xstate.h
+index fc4db51f3b53..b08fa823425f 100644
+--- a/arch/x86/include/asm/fpu/xstate.h
++++ b/arch/x86/include/asm/fpu/xstate.h
+@@ -21,19 +21,29 @@
+ #define XSAVE_YMM_SIZE	    256
+ #define XSAVE_YMM_OFFSET    (XSAVE_HDR_SIZE + XSAVE_HDR_OFFSET)
+ 
+-/* Supervisor features */
+-#define XFEATURE_MASK_SUPERVISOR (XFEATURE_MASK_PT)
+-
+-/* All currently supported features */
+-#define XCNTXT_MASK		(XFEATURE_MASK_FP | \
+-				 XFEATURE_MASK_SSE | \
+-				 XFEATURE_MASK_YMM | \
+-				 XFEATURE_MASK_OPMASK | \
+-				 XFEATURE_MASK_ZMM_Hi256 | \
+-				 XFEATURE_MASK_Hi16_ZMM	 | \
+-				 XFEATURE_MASK_PKRU | \
+-				 XFEATURE_MASK_BNDREGS | \
+-				 XFEATURE_MASK_BNDCSR)
++/* All currently supported user features */
++#define XFEATURE_MASK_USER_SUPPORTED (XFEATURE_MASK_FP | \
++				      XFEATURE_MASK_SSE | \
++				      XFEATURE_MASK_YMM | \
++				      XFEATURE_MASK_OPMASK | \
++				      XFEATURE_MASK_ZMM_Hi256 | \
++				      XFEATURE_MASK_Hi16_ZMM	 | \
++				      XFEATURE_MASK_PKRU | \
++				      XFEATURE_MASK_BNDREGS | \
++				      XFEATURE_MASK_BNDCSR)
++
++/* All currently supported supervisor features */
++#define XFEATURE_MASK_SUPERVISOR_SUPPORTED (0)
++
++/*
++ * Unsupported supervisor features. When a supervisor feature in this mask is
++ * supported in the future, move it to the supported supervisor feature mask.
++ */
++#define XFEATURE_MASK_SUPERVISOR_UNSUPPORTED (XFEATURE_MASK_PT)
++
++/* All supervisor states including supported and unsupported states. */
++#define XFEATURE_MASK_SUPERVISOR_ALL (XFEATURE_MASK_SUPERVISOR_SUPPORTED | \
++				      XFEATURE_MASK_SUPERVISOR_UNSUPPORTED)
+ 
+ #ifdef CONFIG_X86_64
+ #define REX_PREFIX	"0x48, "
+diff --git a/arch/x86/kernel/fpu/init.c b/arch/x86/kernel/fpu/init.c
+index 6ce7e0a23268..61ddc3a5e5c2 100644
+--- a/arch/x86/kernel/fpu/init.c
++++ b/arch/x86/kernel/fpu/init.c
+@@ -224,7 +224,8 @@ static void __init fpu__init_system_xstate_size_legacy(void)
+  */
+ u64 __init fpu__get_supported_xfeatures_mask(void)
+ {
+-	return XCNTXT_MASK;
++	return XFEATURE_MASK_USER_SUPPORTED |
++	       XFEATURE_MASK_SUPERVISOR_SUPPORTED;
+ }
+ 
+ /* Legacy code to initialize eager fpu mode. */
+diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c
+index 5a580e403044..497d43335073 100644
+--- a/arch/x86/kernel/fpu/xstate.c
++++ b/arch/x86/kernel/fpu/xstate.c
+@@ -208,14 +208,13 @@ void fpu__init_cpu_xstate(void)
+ 	if (!boot_cpu_has(X86_FEATURE_XSAVE) || !xfeatures_mask)
+ 		return;
+ 	/*
+-	 * Make it clear that XSAVES supervisor states are not yet
+-	 * implemented should anyone expect it to work by changing
+-	 * bits in XFEATURE_MASK_* macros and XCR0.
++	 * Unsupported supervisor xstates should not be found in
++	 * the xfeatures mask.
+ 	 */
+-	WARN_ONCE((xfeatures_mask & XFEATURE_MASK_SUPERVISOR),
+-		"x86/fpu: XSAVES supervisor states are not yet implemented.\n");
++	WARN_ONCE((xfeatures_mask & XFEATURE_MASK_SUPERVISOR_UNSUPPORTED),
++		  "x86/fpu: Found unsupported supervisor xstates.\n");
+ 
+-	xfeatures_mask &= ~XFEATURE_MASK_SUPERVISOR;
++	xfeatures_mask &= ~XFEATURE_MASK_SUPERVISOR_UNSUPPORTED;
+ 
+ 	cr4_set_bits(X86_CR4_OSXSAVE);
+ 	xsetbv(XCR_XFEATURE_ENABLED_MASK, xfeatures_mask);
+@@ -438,7 +437,7 @@ static int xfeature_uncompacted_offset(int xfeature_nr)
+ 	 * format. Checking a supervisor state's uncompacted offset is
+ 	 * an error.
+ 	 */
+-	if (XFEATURE_MASK_SUPERVISOR & BIT_ULL(xfeature_nr)) {
++	if (XFEATURE_MASK_SUPERVISOR_ALL & BIT_ULL(xfeature_nr)) {
+ 		WARN_ONCE(1, "No fixed offset for xstate %d\n", xfeature_nr);
+ 		return -1;
+ 	}
+@@ -475,7 +474,7 @@ int using_compacted_format(void)
+ int validate_user_xstate_header(const struct xstate_header *hdr)
+ {
+ 	/* No unknown or supervisor features may be set */
+-	if (hdr->xfeatures & (~xfeatures_mask | XFEATURE_MASK_SUPERVISOR))
++	if (hdr->xfeatures & ~(xfeatures_mask & XFEATURE_MASK_USER_SUPPORTED))
+ 		return -EINVAL;
+ 
+ 	/* Userspace must use the uncompacted format */
+@@ -768,7 +767,8 @@ void __init fpu__init_system_xstate(void)
+ 	 * Update info used for ptrace frames; use standard-format size and no
+ 	 * supervisor xstates:
+ 	 */
+-	update_regset_xstate_info(fpu_user_xstate_size,	xfeatures_mask & ~XFEATURE_MASK_SUPERVISOR);
++	update_regset_xstate_info(fpu_user_xstate_size,
++				  xfeatures_mask & XFEATURE_MASK_USER_SUPPORTED);
+ 
+ 	fpu__init_prepare_fx_sw_frame();
+ 	setup_init_fpu_buf();
+@@ -991,7 +991,7 @@ int copy_xstate_to_kernel(void *kbuf, struct xregs_state *xsave, unsigned int of
+ 	 */
+ 	memset(&header, 0, sizeof(header));
+ 	header.xfeatures = xsave->header.xfeatures;
+-	header.xfeatures &= ~XFEATURE_MASK_SUPERVISOR;
++	header.xfeatures &= XFEATURE_MASK_USER_SUPPORTED;
+ 
+ 	/*
+ 	 * Copy xregs_state->header:
+@@ -1075,7 +1075,7 @@ int copy_xstate_to_user(void __user *ubuf, struct xregs_state *xsave, unsigned i
+ 	 */
+ 	memset(&header, 0, sizeof(header));
+ 	header.xfeatures = xsave->header.xfeatures;
+-	header.xfeatures &= ~XFEATURE_MASK_SUPERVISOR;
++	header.xfeatures &= XFEATURE_MASK_USER_SUPPORTED;
+ 
+ 	/*
+ 	 * Copy xregs_state->header:
+@@ -1168,7 +1168,7 @@ int copy_kernel_to_xstate(struct xregs_state *xsave, const void *kbuf)
+ 	 * The state that came in from userspace was user-state only.
+ 	 * Mask all the user states out of 'xfeatures':
+ 	 */
+-	xsave->header.xfeatures &= XFEATURE_MASK_SUPERVISOR;
++	xsave->header.xfeatures &= XFEATURE_MASK_SUPERVISOR_ALL;
+ 
+ 	/*
+ 	 * Add back in the features that came in from userspace:
+@@ -1224,7 +1224,7 @@ int copy_user_to_xstate(struct xregs_state *xsave, const void __user *ubuf)
+ 	 * The state that came in from userspace was user-state only.
+ 	 * Mask all the user states out of 'xfeatures':
+ 	 */
+-	xsave->header.xfeatures &= XFEATURE_MASK_SUPERVISOR;
++	xsave->header.xfeatures &= XFEATURE_MASK_SUPERVISOR_ALL;
+ 
+ 	/*
+ 	 * Add back in the features that came in from userspace:
+-- 
+2.26.0
+
diff --git a/0006-x86-fpu-xstate-Separate-user-and-supervisor-xfeature.patch b/0006-x86-fpu-xstate-Separate-user-and-supervisor-xfeature.patch
new file mode 100644
index 000000000..93ea49d6e
--- /dev/null
+++ b/0006-x86-fpu-xstate-Separate-user-and-supervisor-xfeature.patch
@@ -0,0 +1,353 @@
+From 3c61fb887abcc4dc61105aa4eac15bce6617bbda Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Wed, 13 Dec 2017 16:08:28 -0800
+Subject: [PATCH 06/52] x86/fpu/xstate: Separate user and supervisor xfeatures
+ mask
+
+Before the introduction of XSAVES supervisor states, 'xfeatures_mask' is
+used at various places to determine XSAVE buffer components and XCR0 bits.
+It contains only user xstates.  To support supervisor xstates, it is
+necessary to separate user and supervisor xstates:
+
+- First, change 'xfeatures_mask' to 'xfeatures_mask_all', which represents
+  the full set of bits that should ever be set in a kernel XSAVE buffer.
+- Introduce xfeatures_mask_supervisor() and xfeatures_mask_user() to
+  extract relevant xfeatures from xfeatures_mask_all.
+
+v3:
+- Change xfeature_enabled() type from static int to static bool while at
+  it.
+
+v2:
+- Fix typo in commit log.
+- Move xfeatures_mask_supervisor() from xstate.c to xstate.h.
+- Remove printing of user xstates from fpu__init_system_xstate().
+
+Co-developed-by: Fenghua Yu <fenghua.yu@intel.com>
+Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
+Reviewed-by: Tony Luck <tony.luck@intel.com>
+---
+ arch/x86/include/asm/fpu/internal.h |  2 +-
+ arch/x86/include/asm/fpu/xstate.h   | 13 ++++-
+ arch/x86/kernel/fpu/signal.c        | 16 +++++--
+ arch/x86/kernel/fpu/xstate.c        | 73 +++++++++++++++++------------
+ 4 files changed, 67 insertions(+), 37 deletions(-)
+
+diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
+index 44c48e34d799..ccb1bb32ad7d 100644
+--- a/arch/x86/include/asm/fpu/internal.h
++++ b/arch/x86/include/asm/fpu/internal.h
+@@ -92,7 +92,7 @@ static inline void fpstate_init_xstate(struct xregs_state *xsave)
+ 	 * XRSTORS requires these bits set in xcomp_bv, or it will
+ 	 * trigger #GP:
+ 	 */
+-	xsave->header.xcomp_bv = XCOMP_BV_COMPACTED_FORMAT | xfeatures_mask;
++	xsave->header.xcomp_bv = XCOMP_BV_COMPACTED_FORMAT | xfeatures_mask_all;
+ }
+ 
+ static inline void fpstate_init_fxstate(struct fxregs_state *fx)
+diff --git a/arch/x86/include/asm/fpu/xstate.h b/arch/x86/include/asm/fpu/xstate.h
+index b08fa823425f..92104b298d77 100644
+--- a/arch/x86/include/asm/fpu/xstate.h
++++ b/arch/x86/include/asm/fpu/xstate.h
+@@ -51,7 +51,18 @@
+ #define REX_PREFIX
+ #endif
+ 
+-extern u64 xfeatures_mask;
++extern u64 xfeatures_mask_all;
++
++static inline u64 xfeatures_mask_supervisor(void)
++{
++	return xfeatures_mask_all & XFEATURE_MASK_SUPERVISOR_SUPPORTED;
++}
++
++static inline u64 xfeatures_mask_user(void)
++{
++	return xfeatures_mask_all & XFEATURE_MASK_USER_SUPPORTED;
++}
++
+ extern u64 xstate_fx_sw_bytes[USER_XSTATE_FX_SW_WORDS];
+ 
+ extern void __init update_regset_xstate_info(unsigned int size,
+diff --git a/arch/x86/kernel/fpu/signal.c b/arch/x86/kernel/fpu/signal.c
+index 585e3651b98f..3df0cfae535f 100644
+--- a/arch/x86/kernel/fpu/signal.c
++++ b/arch/x86/kernel/fpu/signal.c
+@@ -252,13 +252,17 @@ sanitize_restored_xstate(union fpregs_state *state,
+  */
+ static int copy_user_to_fpregs_zeroing(void __user *buf, u64 xbv, int fx_only)
+ {
++	u64 init_bv;
++
+ 	if (use_xsave()) {
+ 		if (fx_only) {
+-			u64 init_bv = xfeatures_mask & ~XFEATURE_MASK_FPSSE;
++			init_bv = xfeatures_mask_user() & ~XFEATURE_MASK_FPSSE;
++
+ 			copy_kernel_to_xregs(&init_fpstate.xsave, init_bv);
+ 			return copy_user_to_fxregs(buf);
+ 		} else {
+-			u64 init_bv = xfeatures_mask & ~xbv;
++			init_bv = xfeatures_mask_user() & ~xbv;
++
+ 			if (unlikely(init_bv))
+ 				copy_kernel_to_xregs(&init_fpstate.xsave, init_bv);
+ 			return copy_user_to_xregs(buf, xbv);
+@@ -358,7 +362,7 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 
+ 
+ 	if (use_xsave() && !fx_only) {
+-		u64 init_bv = xfeatures_mask & ~xfeatures;
++		u64 init_bv = xfeatures_mask_user() & ~xfeatures;
+ 
+ 		if (using_compacted_format()) {
+ 			ret = copy_user_to_xstate(&fpu->state.xsave, buf_fx);
+@@ -389,7 +393,9 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 
+ 		fpregs_lock();
+ 		if (use_xsave()) {
+-			u64 init_bv = xfeatures_mask & ~XFEATURE_MASK_FPSSE;
++			u64 init_bv;
++
++			init_bv = xfeatures_mask_user() & ~XFEATURE_MASK_FPSSE;
+ 			copy_kernel_to_xregs(&init_fpstate.xsave, init_bv);
+ 		}
+ 
+@@ -465,7 +471,7 @@ void fpu__init_prepare_fx_sw_frame(void)
+ 
+ 	fx_sw_reserved.magic1 = FP_XSTATE_MAGIC1;
+ 	fx_sw_reserved.extended_size = size;
+-	fx_sw_reserved.xfeatures = xfeatures_mask;
++	fx_sw_reserved.xfeatures = xfeatures_mask_user();
+ 	fx_sw_reserved.xstate_size = fpu_user_xstate_size;
+ 
+ 	if (IS_ENABLED(CONFIG_IA32_EMULATION) ||
+diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c
+index 497d43335073..bb8b53223925 100644
+--- a/arch/x86/kernel/fpu/xstate.c
++++ b/arch/x86/kernel/fpu/xstate.c
+@@ -54,9 +54,10 @@ static short xsave_cpuid_features[] __initdata = {
+ };
+ 
+ /*
+- * Mask of xstate features supported by the CPU and the kernel:
++ * This represents the full set of bits that should ever be set in a kernel
++ * XSAVE buffer, both supervisor and user xstates.
+  */
+-u64 xfeatures_mask __read_mostly;
++u64 xfeatures_mask_all __read_mostly;
+ 
+ static unsigned int xstate_offsets[XFEATURE_MAX] = { [ 0 ... XFEATURE_MAX - 1] = -1};
+ static unsigned int xstate_sizes[XFEATURE_MAX]   = { [ 0 ... XFEATURE_MAX - 1] = -1};
+@@ -76,7 +77,7 @@ unsigned int fpu_user_xstate_size;
+  */
+ int cpu_has_xfeatures(u64 xfeatures_needed, const char **feature_name)
+ {
+-	u64 xfeatures_missing = xfeatures_needed & ~xfeatures_mask;
++	u64 xfeatures_missing = xfeatures_needed & ~xfeatures_mask_all;
+ 
+ 	if (unlikely(feature_name)) {
+ 		long xfeature_idx, max_idx;
+@@ -150,7 +151,7 @@ void fpstate_sanitize_xstate(struct fpu *fpu)
+ 	 * None of the feature bits are in init state. So nothing else
+ 	 * to do for us, as the memory layout is up to date.
+ 	 */
+-	if ((xfeatures & xfeatures_mask) == xfeatures_mask)
++	if ((xfeatures & xfeatures_mask_all) == xfeatures_mask_all)
+ 		return;
+ 
+ 	/*
+@@ -177,7 +178,7 @@ void fpstate_sanitize_xstate(struct fpu *fpu)
+ 	 * in a special way already:
+ 	 */
+ 	feature_bit = 0x2;
+-	xfeatures = (xfeatures_mask & ~xfeatures) >> 2;
++	xfeatures = (xfeatures_mask_user() & ~xfeatures) >> 2;
+ 
+ 	/*
+ 	 * Update all the remaining memory layouts according to their
+@@ -205,19 +206,28 @@ void fpstate_sanitize_xstate(struct fpu *fpu)
+  */
+ void fpu__init_cpu_xstate(void)
+ {
+-	if (!boot_cpu_has(X86_FEATURE_XSAVE) || !xfeatures_mask)
++	u64 unsup_bits;
++
++	if (!boot_cpu_has(X86_FEATURE_XSAVE) || !xfeatures_mask_all)
+ 		return;
+ 	/*
+ 	 * Unsupported supervisor xstates should not be found in
+ 	 * the xfeatures mask.
+ 	 */
+-	WARN_ONCE((xfeatures_mask & XFEATURE_MASK_SUPERVISOR_UNSUPPORTED),
+-		  "x86/fpu: Found unsupported supervisor xstates.\n");
++	unsup_bits = xfeatures_mask_all & XFEATURE_MASK_SUPERVISOR_UNSUPPORTED;
++	WARN_ONCE(unsup_bits, "x86/fpu: Found unsupported supervisor xstates: 0x%llx\n",
++		  unsup_bits);
+ 
+-	xfeatures_mask &= ~XFEATURE_MASK_SUPERVISOR_UNSUPPORTED;
++	xfeatures_mask_all &= ~XFEATURE_MASK_SUPERVISOR_UNSUPPORTED;
+ 
+ 	cr4_set_bits(X86_CR4_OSXSAVE);
+-	xsetbv(XCR_XFEATURE_ENABLED_MASK, xfeatures_mask);
++
++	/*
++	 * XCR_XFEATURE_ENABLED_MASK (aka. XCR0) sets user features
++	 * managed by XSAVE{C, OPT, S} and XRSTOR{S}.  Only XSAVE user
++	 * states can be set here.
++	 */
++	xsetbv(XCR_XFEATURE_ENABLED_MASK, xfeatures_mask_user());
+ }
+ 
+ /*
+@@ -225,9 +235,9 @@ void fpu__init_cpu_xstate(void)
+  * functions here: one for user xstates and the other for
+  * system xstates.  For now, they are the same.
+  */
+-static int xfeature_enabled(enum xfeature xfeature)
++static bool xfeature_enabled(enum xfeature xfeature)
+ {
+-	return !!(xfeatures_mask & (1UL << xfeature));
++	return xfeatures_mask_all & BIT_ULL(xfeature);
+ }
+ 
+ /*
+@@ -414,7 +424,7 @@ static void __init setup_init_fpu_buf(void)
+ 
+ 	if (boot_cpu_has(X86_FEATURE_XSAVES))
+ 		init_fpstate.xsave.header.xcomp_bv = XCOMP_BV_COMPACTED_FORMAT |
+-						     xfeatures_mask;
++						     xfeatures_mask_all;
+ 
+ 	/*
+ 	 * Init all the features state with header.xfeatures being 0x0
+@@ -474,7 +484,7 @@ int using_compacted_format(void)
+ int validate_user_xstate_header(const struct xstate_header *hdr)
+ {
+ 	/* No unknown or supervisor features may be set */
+-	if (hdr->xfeatures & ~(xfeatures_mask & XFEATURE_MASK_USER_SUPPORTED))
++	if (hdr->xfeatures & ~xfeatures_mask_user())
+ 		return -EINVAL;
+ 
+ 	/* Userspace must use the uncompacted format */
+@@ -609,7 +619,7 @@ static void do_extra_xstate_size_checks(void)
+ 
+ 
+ /*
+- * Get total size of enabled xstates in XCR0/xfeatures_mask.
++ * Get total size of enabled xstates in XCR0 | IA32_XSS.
+  *
+  * Note the SDM's wording here.  "sub-function 0" only enumerates
+  * the size of the *user* states.  If we use it to size a buffer
+@@ -699,7 +709,7 @@ static int __init init_xstate_size(void)
+  */
+ static void fpu__init_disable_system_xstate(void)
+ {
+-	xfeatures_mask = 0;
++	xfeatures_mask_all = 0;
+ 	cr4_clear_bits(X86_CR4_OSXSAVE);
+ 	setup_clear_cpu_cap(X86_FEATURE_XSAVE);
+ }
+@@ -734,16 +744,21 @@ void __init fpu__init_system_xstate(void)
+ 		return;
+ 	}
+ 
++	/*
++	 * Find user xstates supported by the processor.
++	 */
+ 	cpuid_count(XSTATE_CPUID, 0, &eax, &ebx, &ecx, &edx);
+-	xfeatures_mask = eax + ((u64)edx << 32);
++	xfeatures_mask_all = eax + ((u64)edx << 32);
+ 
+-	if ((xfeatures_mask & XFEATURE_MASK_FPSSE) != XFEATURE_MASK_FPSSE) {
++	/* Place supervisor features in xfeatures_mask_all here */
++	if ((xfeatures_mask_user() & XFEATURE_MASK_FPSSE) != XFEATURE_MASK_FPSSE) {
+ 		/*
+ 		 * This indicates that something really unexpected happened
+ 		 * with the enumeration.  Disable XSAVE and try to continue
+ 		 * booting without it.  This is too early to BUG().
+ 		 */
+-		pr_err("x86/fpu: FP/SSE not present amongst the CPU's xstate features: 0x%llx.\n", xfeatures_mask);
++		pr_err("x86/fpu: FP/SSE not present amongst the CPU's xstate features: 0x%llx.\n",
++		       xfeatures_mask_all);
+ 		goto out_disable;
+ 	}
+ 
+@@ -752,10 +767,10 @@ void __init fpu__init_system_xstate(void)
+ 	 */
+ 	for (i = 0; i < ARRAY_SIZE(xsave_cpuid_features); i++) {
+ 		if (!boot_cpu_has(xsave_cpuid_features[i]))
+-			xfeatures_mask &= ~BIT(i);
++			xfeatures_mask_all &= ~BIT_ULL(i);
+ 	}
+ 
+-	xfeatures_mask &= fpu__get_supported_xfeatures_mask();
++	xfeatures_mask_all &= fpu__get_supported_xfeatures_mask();
+ 
+ 	/* Enable xstate instructions to be able to continue with initialization: */
+ 	fpu__init_cpu_xstate();
+@@ -767,8 +782,7 @@ void __init fpu__init_system_xstate(void)
+ 	 * Update info used for ptrace frames; use standard-format size and no
+ 	 * supervisor xstates:
+ 	 */
+-	update_regset_xstate_info(fpu_user_xstate_size,
+-				  xfeatures_mask & XFEATURE_MASK_USER_SUPPORTED);
++	update_regset_xstate_info(fpu_user_xstate_size, xfeatures_mask_user());
+ 
+ 	fpu__init_prepare_fx_sw_frame();
+ 	setup_init_fpu_buf();
+@@ -776,7 +790,7 @@ void __init fpu__init_system_xstate(void)
+ 	print_xstate_offset_size();
+ 
+ 	pr_info("x86/fpu: Enabled xstate features 0x%llx, context size is %d bytes, using '%s' format.\n",
+-		xfeatures_mask,
++		xfeatures_mask_all,
+ 		fpu_kernel_xstate_size,
+ 		boot_cpu_has(X86_FEATURE_XSAVES) ? "compacted" : "standard");
+ 	return;
+@@ -795,7 +809,7 @@ void fpu__resume_cpu(void)
+ 	 * Restore XCR0 on xsave capable CPUs:
+ 	 */
+ 	if (boot_cpu_has(X86_FEATURE_XSAVE))
+-		xsetbv(XCR_XFEATURE_ENABLED_MASK, xfeatures_mask);
++		xsetbv(XCR_XFEATURE_ENABLED_MASK, xfeatures_mask_user());
+ }
+ 
+ /*
+@@ -840,10 +854,9 @@ void *get_xsave_addr(struct xregs_state *xsave, int xfeature_nr)
+ 
+ 	/*
+ 	 * We should not ever be requesting features that we
+-	 * have not enabled.  Remember that xfeatures_mask is
+-	 * what we write to the XCR0 register.
++	 * have not enabled.
+ 	 */
+-	WARN_ONCE(!(xfeatures_mask & BIT_ULL(xfeature_nr)),
++	WARN_ONCE(!(xfeatures_mask_all & BIT_ULL(xfeature_nr)),
+ 		  "get of unsupported state");
+ 	/*
+ 	 * This assumes the last 'xsave*' instruction to
+@@ -991,7 +1004,7 @@ int copy_xstate_to_kernel(void *kbuf, struct xregs_state *xsave, unsigned int of
+ 	 */
+ 	memset(&header, 0, sizeof(header));
+ 	header.xfeatures = xsave->header.xfeatures;
+-	header.xfeatures &= XFEATURE_MASK_USER_SUPPORTED;
++	header.xfeatures &= xfeatures_mask_user();
+ 
+ 	/*
+ 	 * Copy xregs_state->header:
+@@ -1075,7 +1088,7 @@ int copy_xstate_to_user(void __user *ubuf, struct xregs_state *xsave, unsigned i
+ 	 */
+ 	memset(&header, 0, sizeof(header));
+ 	header.xfeatures = xsave->header.xfeatures;
+-	header.xfeatures &= XFEATURE_MASK_USER_SUPPORTED;
++	header.xfeatures &= xfeatures_mask_user();
+ 
+ 	/*
+ 	 * Copy xregs_state->header:
+-- 
+2.26.0
+
diff --git a/0007-x86-fpu-xstate-Introduce-XSAVES-supervisor-states.patch b/0007-x86-fpu-xstate-Introduce-XSAVES-supervisor-states.patch
new file mode 100644
index 000000000..f0327bb39
--- /dev/null
+++ b/0007-x86-fpu-xstate-Introduce-XSAVES-supervisor-states.patch
@@ -0,0 +1,85 @@
+From 0c960bb7ebe9e1eae8e2119dfa9c06ba62e803d6 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Thu, 10 Nov 2016 10:13:56 -0800
+Subject: [PATCH 07/52] x86/fpu/xstate: Introduce XSAVES supervisor states
+
+Enable XSAVES supervisor states by setting MSR_IA32_XSS bits according to
+CPUID enumeration results.  Also revise comments at various places.
+
+v2:
+- Remove printing of supervisor xstates from fpu__init_system_xstate().
+
+Co-developed-by: Fenghua Yu <fenghua.yu@intel.com>
+Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
+Reviewed-by: Tony Luck <tony.luck@intel.com>
+---
+ arch/x86/kernel/fpu/xstate.c | 28 +++++++++++++++++++---------
+ 1 file changed, 19 insertions(+), 9 deletions(-)
+
+diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c
+index bb8b53223925..28e8229b23a7 100644
+--- a/arch/x86/kernel/fpu/xstate.c
++++ b/arch/x86/kernel/fpu/xstate.c
+@@ -228,13 +228,14 @@ void fpu__init_cpu_xstate(void)
+ 	 * states can be set here.
+ 	 */
+ 	xsetbv(XCR_XFEATURE_ENABLED_MASK, xfeatures_mask_user());
++
++	/*
++	 * MSR_IA32_XSS sets supervisor states managed by XSAVES.
++	 */
++	if (boot_cpu_has(X86_FEATURE_XSAVES))
++		wrmsrl(MSR_IA32_XSS, xfeatures_mask_supervisor());
+ }
+ 
+-/*
+- * Note that in the future we will likely need a pair of
+- * functions here: one for user xstates and the other for
+- * system xstates.  For now, they are the same.
+- */
+ static bool xfeature_enabled(enum xfeature xfeature)
+ {
+ 	return xfeatures_mask_all & BIT_ULL(xfeature);
+@@ -625,9 +626,6 @@ static void do_extra_xstate_size_checks(void)
+  * the size of the *user* states.  If we use it to size a buffer
+  * that we use 'XSAVES' on, we could potentially overflow the
+  * buffer because 'XSAVES' saves system states too.
+- *
+- * Note that we do not currently set any bits on IA32_XSS so
+- * 'XCR0 | IA32_XSS == XCR0' for now.
+  */
+ static unsigned int __init get_xsaves_size(void)
+ {
+@@ -750,7 +748,12 @@ void __init fpu__init_system_xstate(void)
+ 	cpuid_count(XSTATE_CPUID, 0, &eax, &ebx, &ecx, &edx);
+ 	xfeatures_mask_all = eax + ((u64)edx << 32);
+ 
+-	/* Place supervisor features in xfeatures_mask_all here */
++	/*
++	 * Find supervisor xstates supported by the processor.
++	 */
++	cpuid_count(XSTATE_CPUID, 1, &eax, &ebx, &ecx, &edx);
++	xfeatures_mask_all |= ecx + ((u64)edx << 32);
++
+ 	if ((xfeatures_mask_user() & XFEATURE_MASK_FPSSE) != XFEATURE_MASK_FPSSE) {
+ 		/*
+ 		 * This indicates that something really unexpected happened
+@@ -810,6 +813,13 @@ void fpu__resume_cpu(void)
+ 	 */
+ 	if (boot_cpu_has(X86_FEATURE_XSAVE))
+ 		xsetbv(XCR_XFEATURE_ENABLED_MASK, xfeatures_mask_user());
++
++	/*
++	 * Restore IA32_XSS. The same CPUID bit enumerates support
++	 * of XSAVES and MSR_IA32_XSS.
++	 */
++	if (boot_cpu_has(X86_FEATURE_XSAVES))
++		wrmsrl(MSR_IA32_XSS, xfeatures_mask_supervisor());
+ }
+ 
+ /*
+-- 
+2.26.0
+
diff --git a/0008-x86-fpu-xstate-Define-new-functions-for-clearing-fpr.patch b/0008-x86-fpu-xstate-Define-new-functions-for-clearing-fpr.patch
new file mode 100644
index 000000000..7ca51d6ec
--- /dev/null
+++ b/0008-x86-fpu-xstate-Define-new-functions-for-clearing-fpr.patch
@@ -0,0 +1,182 @@
+From 5bb1b172b2d9d01f78733be9e864471d958f4b61 Mon Sep 17 00:00:00 2001
+From: Fenghua Yu <fenghua.yu@intel.com>
+Date: Thu, 10 Nov 2016 10:13:56 -0800
+Subject: [PATCH 08/52] x86/fpu/xstate: Define new functions for clearing
+ fpregs and xstates
+
+Currently, fpu__clear() clears all fpregs and xstates.  Once XSAVES
+supervisor states are introduced, supervisor settings (e.g. CET xstates)
+must remain active for signals; It is necessary to have separate functions:
+
+- Create fpu__clear_user_states(): clear only user settings for signals;
+- Create fpu__clear_all(): clear both user and supervisor settings in
+   flush_thread().
+
+Also modify copy_init_fpstate_to_fpregs() to take a mask from above two
+functions.
+
+v3:
+- Put common code into a static function fpu__clear(), with a parameter
+  clear_user_only.
+
+v2:
+- Fixed an issue where fpu__clear_user_states() drops supervisor xstates.
+- Revise commit log.
+
+Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
+Co-developed-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
+Reviewed-by: Tony Luck <tony.luck@intel.com>
+---
+ arch/x86/include/asm/fpu/internal.h |  3 +-
+ arch/x86/kernel/fpu/core.c          | 49 +++++++++++++++++++----------
+ arch/x86/kernel/fpu/signal.c        |  4 +--
+ arch/x86/kernel/process.c           |  2 +-
+ arch/x86/kernel/signal.c            |  2 +-
+ 5 files changed, 39 insertions(+), 21 deletions(-)
+
+diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
+index ccb1bb32ad7d..a42fcb4b690d 100644
+--- a/arch/x86/include/asm/fpu/internal.h
++++ b/arch/x86/include/asm/fpu/internal.h
+@@ -31,7 +31,8 @@ extern void fpu__save(struct fpu *fpu);
+ extern int  fpu__restore_sig(void __user *buf, int ia32_frame);
+ extern void fpu__drop(struct fpu *fpu);
+ extern int  fpu__copy(struct task_struct *dst, struct task_struct *src);
+-extern void fpu__clear(struct fpu *fpu);
++extern void fpu__clear_user_states(struct fpu *fpu);
++extern void fpu__clear_all(struct fpu *fpu);
+ extern int  fpu__exception_code(struct fpu *fpu, int trap_nr);
+ extern int  dump_fpu(struct pt_regs *ptregs, struct user_i387_struct *fpstate);
+ 
+diff --git a/arch/x86/kernel/fpu/core.c b/arch/x86/kernel/fpu/core.c
+index 12c70840980e..6ba3a8b78bf9 100644
+--- a/arch/x86/kernel/fpu/core.c
++++ b/arch/x86/kernel/fpu/core.c
+@@ -294,12 +294,10 @@ void fpu__drop(struct fpu *fpu)
+  * Clear FPU registers by setting them up from
+  * the init fpstate:
+  */
+-static inline void copy_init_fpstate_to_fpregs(void)
++static inline void copy_init_fpstate_to_fpregs(u64 features_mask)
+ {
+-	fpregs_lock();
+-
+ 	if (use_xsave())
+-		copy_kernel_to_xregs(&init_fpstate.xsave, -1);
++		copy_kernel_to_xregs(&init_fpstate.xsave, features_mask);
+ 	else if (static_cpu_has(X86_FEATURE_FXSR))
+ 		copy_kernel_to_fxregs(&init_fpstate.fxsave);
+ 	else
+@@ -307,9 +305,6 @@ static inline void copy_init_fpstate_to_fpregs(void)
+ 
+ 	if (boot_cpu_has(X86_FEATURE_OSPKE))
+ 		copy_init_pkru_to_fpregs();
+-
+-	fpregs_mark_activate();
+-	fpregs_unlock();
+ }
+ 
+ /*
+@@ -318,18 +313,40 @@ static inline void copy_init_fpstate_to_fpregs(void)
+  * Called by sys_execve(), by the signal handler code and by various
+  * error paths.
+  */
+-void fpu__clear(struct fpu *fpu)
++static void fpu__clear(struct fpu *fpu, int clear_user_only)
+ {
+-	WARN_ON_FPU(fpu != &current->thread.fpu); /* Almost certainly an anomaly */
++	if (static_cpu_has(X86_FEATURE_FPU)) {
++		fpregs_lock();
++
++		if (clear_user_only) {
++			if (!fpregs_state_valid(fpu, smp_processor_id()) &&
++			    xfeatures_mask_supervisor())
++				copy_kernel_to_xregs(&fpu->state.xsave,
++						     xfeatures_mask_supervisor());
++			copy_init_fpstate_to_fpregs(xfeatures_mask_user());
++		} else {
++			copy_init_fpstate_to_fpregs(xfeatures_mask_all);
++		}
++
++		fpregs_mark_activate();
++		fpregs_unlock();
++		return;
++	} else {
++		fpu__drop(fpu);
++		fpu__initialize(fpu);
++	}
++}
+ 
+-	fpu__drop(fpu);
++void fpu__clear_user_states(struct fpu *fpu)
++{
++	WARN_ON_FPU(fpu != &current->thread.fpu);
++	fpu__clear(fpu, 1);
++}
+ 
+-	/*
+-	 * Make sure fpstate is cleared and initialized.
+-	 */
+-	fpu__initialize(fpu);
+-	if (static_cpu_has(X86_FEATURE_FPU))
+-		copy_init_fpstate_to_fpregs();
++void fpu__clear_all(struct fpu *fpu)
++{
++	WARN_ON_FPU(fpu != &current->thread.fpu);
++	fpu__clear(fpu, 0);
+ }
+ 
+ /*
+diff --git a/arch/x86/kernel/fpu/signal.c b/arch/x86/kernel/fpu/signal.c
+index 3df0cfae535f..cd6eafba12da 100644
+--- a/arch/x86/kernel/fpu/signal.c
++++ b/arch/x86/kernel/fpu/signal.c
+@@ -289,7 +289,7 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 			 IS_ENABLED(CONFIG_IA32_EMULATION));
+ 
+ 	if (!buf) {
+-		fpu__clear(fpu);
++		fpu__clear_user_states(fpu);
+ 		return 0;
+ 	}
+ 
+@@ -416,7 +416,7 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 
+ err_out:
+ 	if (ret)
+-		fpu__clear(fpu);
++		fpu__clear_user_states(fpu);
+ 	return ret;
+ }
+ 
+diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
+index 3053c85e0e42..87de18c64cf5 100644
+--- a/arch/x86/kernel/process.c
++++ b/arch/x86/kernel/process.c
+@@ -192,7 +192,7 @@ void flush_thread(void)
+ 	flush_ptrace_hw_breakpoint(tsk);
+ 	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));
+ 
+-	fpu__clear(&tsk->thread.fpu);
++	fpu__clear_all(&tsk->thread.fpu);
+ }
+ 
+ void disable_TSC(void)
+diff --git a/arch/x86/kernel/signal.c b/arch/x86/kernel/signal.c
+index 8a29573851a3..35f878e9f91d 100644
+--- a/arch/x86/kernel/signal.c
++++ b/arch/x86/kernel/signal.c
+@@ -761,7 +761,7 @@ handle_signal(struct ksignal *ksig, struct pt_regs *regs)
+ 		/*
+ 		 * Ensure the signal handler starts with the new fpu state.
+ 		 */
+-		fpu__clear(fpu);
++		fpu__clear_user_states(fpu);
+ 	}
+ 	signal_setup_done(failed, ksig, stepping);
+ }
+-- 
+2.26.0
+
diff --git a/0009-x86-fpu-xstate-Update-sanitize_restored_xstate-for-s.patch b/0009-x86-fpu-xstate-Update-sanitize_restored_xstate-for-s.patch
new file mode 100644
index 000000000..e75ab5eb8
--- /dev/null
+++ b/0009-x86-fpu-xstate-Update-sanitize_restored_xstate-for-s.patch
@@ -0,0 +1,137 @@
+From 1a1db260b1c60635465a939f94ccd6860d73d732 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 12 Nov 2019 09:09:10 -0800
+Subject: [PATCH 09/52] x86/fpu/xstate: Update sanitize_restored_xstate() for
+ supervisor xstates
+
+The function sanitize_restored_xstate() sanitizes user xstates of an XSAVE
+buffer by setting the buffer's header->xfeatures to the input 'xfeatures',
+effectively resetting features not in 'xfeatures' back to the init state.
+
+When supervisor xstates are introduced, it is necessary to make sure only
+user xstates are sanitized.  Ensure supervisor bits in header->xfeatures
+stay set and supervisor states are not modified.
+
+To make names clear, also:
+
+- Rename the function to sanitize_restored_user_xstate().
+- Rename input parameter 'xfeatures' to 'xfeatures_from_user'.
+- In __fpu__restore_sig(), rename 'xfeatures' to 'user_xfeatures'.
+
+v3:
+- Change xfeatures_user to user_xfeatures.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
+---
+ arch/x86/kernel/fpu/signal.c | 37 +++++++++++++++++++++++-------------
+ 1 file changed, 24 insertions(+), 13 deletions(-)
+
+diff --git a/arch/x86/kernel/fpu/signal.c b/arch/x86/kernel/fpu/signal.c
+index cd6eafba12da..d09d72334a12 100644
+--- a/arch/x86/kernel/fpu/signal.c
++++ b/arch/x86/kernel/fpu/signal.c
+@@ -211,9 +211,9 @@ int copy_fpstate_to_sigframe(void __user *buf, void __user *buf_fx, int size)
+ }
+ 
+ static inline void
+-sanitize_restored_xstate(union fpregs_state *state,
+-			 struct user_i387_ia32_struct *ia32_env,
+-			 u64 xfeatures, int fx_only)
++sanitize_restored_user_xstate(union fpregs_state *state,
++			      struct user_i387_ia32_struct *ia32_env,
++			      u64 xfeatures_from_user, int fx_only)
+ {
+ 	struct xregs_state *xsave = &state->xsave;
+ 	struct xstate_header *header = &xsave->header;
+@@ -226,13 +226,22 @@ sanitize_restored_xstate(union fpregs_state *state,
+ 		 */
+ 
+ 		/*
+-		 * Init the state that is not present in the memory
+-		 * layout and not enabled by the OS.
++		 * 'xfeatures_from_user' might have bits clear which are
++		 * set in header->xfeatures. This represents features that
++		 * were in init state prior to a signal delivery, and need
++		 * to be reset back to the init state.  Clear any user
++		 * feature bits which are set in the kernel buffer to get
++		 * them back to the init state.
++		 *
++		 * Supervisor state is unchanged by input from userspace.
++		 * Ensure supervisor state bits stay set and supervisor
++		 * state is not modified.
+ 		 */
+ 		if (fx_only)
+ 			header->xfeatures = XFEATURE_MASK_FPSSE;
+ 		else
+-			header->xfeatures &= xfeatures;
++			header->xfeatures &= xfeatures_from_user |
++					     xfeatures_mask_supervisor();
+ 	}
+ 
+ 	if (use_fxsr()) {
+@@ -281,7 +290,7 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 	struct task_struct *tsk = current;
+ 	struct fpu *fpu = &tsk->thread.fpu;
+ 	struct user_i387_ia32_struct env;
+-	u64 xfeatures = 0;
++	u64 user_xfeatures = 0;
+ 	int fx_only = 0;
+ 	int ret = 0;
+ 
+@@ -314,7 +323,7 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 			trace_x86_fpu_xstate_check_failed(fpu);
+ 		} else {
+ 			state_size = fx_sw_user.xstate_size;
+-			xfeatures = fx_sw_user.xfeatures;
++			user_xfeatures = fx_sw_user.xfeatures;
+ 		}
+ 	}
+ 
+@@ -349,7 +358,7 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 		 */
+ 		fpregs_lock();
+ 		pagefault_disable();
+-		ret = copy_user_to_fpregs_zeroing(buf_fx, xfeatures, fx_only);
++		ret = copy_user_to_fpregs_zeroing(buf_fx, user_xfeatures, fx_only);
+ 		pagefault_enable();
+ 		if (!ret) {
+ 			fpregs_mark_activate();
+@@ -362,7 +371,7 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 
+ 
+ 	if (use_xsave() && !fx_only) {
+-		u64 init_bv = xfeatures_mask_user() & ~xfeatures;
++		u64 init_bv = xfeatures_mask_user() & ~user_xfeatures;
+ 
+ 		if (using_compacted_format()) {
+ 			ret = copy_user_to_xstate(&fpu->state.xsave, buf_fx);
+@@ -375,12 +384,13 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 		if (ret)
+ 			goto err_out;
+ 
+-		sanitize_restored_xstate(&fpu->state, envp, xfeatures, fx_only);
++		sanitize_restored_user_xstate(&fpu->state, envp, user_xfeatures,
++					      fx_only);
+ 
+ 		fpregs_lock();
+ 		if (unlikely(init_bv))
+ 			copy_kernel_to_xregs(&init_fpstate.xsave, init_bv);
+-		ret = copy_kernel_to_xregs_err(&fpu->state.xsave, xfeatures);
++		ret = copy_kernel_to_xregs_err(&fpu->state.xsave, user_xfeatures);
+ 
+ 	} else if (use_fxsr()) {
+ 		ret = __copy_from_user(&fpu->state.fxsave, buf_fx, state_size);
+@@ -389,7 +399,8 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 			goto err_out;
+ 		}
+ 
+-		sanitize_restored_xstate(&fpu->state, envp, xfeatures, fx_only);
++		sanitize_restored_user_xstate(&fpu->state, envp,
++					      user_xfeatures, fx_only);
+ 
+ 		fpregs_lock();
+ 		if (use_xsave()) {
+-- 
+2.26.0
+
diff --git a/0010-x86-fpu-xstate-Update-copy_kernel_to_xregs_err-for-X.patch b/0010-x86-fpu-xstate-Update-copy_kernel_to_xregs_err-for-X.patch
new file mode 100644
index 000000000..c9055d5e0
--- /dev/null
+++ b/0010-x86-fpu-xstate-Update-copy_kernel_to_xregs_err-for-X.patch
@@ -0,0 +1,47 @@
+From 126b6b0861f2b862e50878c4a8de228358aba52b Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 29 Oct 2019 12:42:06 -0700
+Subject: [PATCH 10/52] x86/fpu/xstate: Update copy_kernel_to_xregs_err() for
+ XSAVES supervisor states
+
+The function copy_kernel_to_xregs_err() uses XRSTOR, which can work with
+standard or compacted format without supervisor xstates.  However, when
+supervisor xstates are present, XRSTORS must be used.  Fix it by using
+XRSTORS when XSAVES is enabled.
+
+I also considered if there were additional cases where XRSTOR might be
+mistakenly called instead of XRSTORS.  There are only three XRSTOR sites
+in kernel:
+
+1. copy_kernel_to_xregs_booting(), already switches between XRSTOR and
+   XRSTORS based on X86_FEATURE_XSAVES.
+2. copy_user_to_xregs(), which *needs* XRSTOR because it is copying from
+   userspace and must never copy supervisor state with XRSTORS.
+3. copy_kernel_to_xregs_err() mistakenly used XRSTOR only.  Fixed in
+   this patch.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
+---
+ arch/x86/include/asm/fpu/internal.h | 5 ++++-
+ 1 file changed, 4 insertions(+), 1 deletion(-)
+
+diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
+index a42fcb4b690d..42159f45bf9c 100644
+--- a/arch/x86/include/asm/fpu/internal.h
++++ b/arch/x86/include/asm/fpu/internal.h
+@@ -400,7 +400,10 @@ static inline int copy_kernel_to_xregs_err(struct xregs_state *xstate, u64 mask)
+ 	u32 hmask = mask >> 32;
+ 	int err;
+ 
+-	XSTATE_OP(XRSTOR, xstate, lmask, hmask, err);
++	if (static_cpu_has(X86_FEATURE_XSAVES))
++		XSTATE_OP(XRSTORS, xstate, lmask, hmask, err);
++	else
++		XSTATE_OP(XRSTOR, xstate, lmask, hmask, err);
+ 
+ 	return err;
+ }
+-- 
+2.26.0
+
diff --git a/0011-x86-fpu-Introduce-copy_supervisor_to_kernel.patch b/0011-x86-fpu-Introduce-copy_supervisor_to_kernel.patch
new file mode 100644
index 000000000..95558814b
--- /dev/null
+++ b/0011-x86-fpu-Introduce-copy_supervisor_to_kernel.patch
@@ -0,0 +1,148 @@
+From 02c0896151cd6a9074cdbd30d26911d1de53f690 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 16 Mar 2020 10:29:12 -0700
+Subject: [PATCH 11/52] x86/fpu: Introduce copy_supervisor_to_kernel()
+
+XSAVES takes mask and saves only features specified in that mask.  The
+kernel normally saves all features.  If only supervisor states are saved,
+the resulting buffer ends up with a different layout.
+
+Introduce copy_supervisor_to_kernel(), which saves only supervisor states
+and then converts the buffer format to what the rest of the kernel uses.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/include/asm/fpu/xstate.h |  1 +
+ arch/x86/kernel/fpu/xstate.c      | 83 +++++++++++++++++++++++++++++++
+ 2 files changed, 84 insertions(+)
+
+diff --git a/arch/x86/include/asm/fpu/xstate.h b/arch/x86/include/asm/fpu/xstate.h
+index 92104b298d77..422d8369012a 100644
+--- a/arch/x86/include/asm/fpu/xstate.h
++++ b/arch/x86/include/asm/fpu/xstate.h
+@@ -75,6 +75,7 @@ int copy_xstate_to_kernel(void *kbuf, struct xregs_state *xsave, unsigned int of
+ int copy_xstate_to_user(void __user *ubuf, struct xregs_state *xsave, unsigned int offset, unsigned int size);
+ int copy_kernel_to_xstate(struct xregs_state *xsave, const void *kbuf);
+ int copy_user_to_xstate(struct xregs_state *xsave, const void __user *ubuf);
++void copy_supervisor_to_kernel(struct xregs_state *xsave);
+ 
+ /* Validate an xstate header supplied by userspace (ptrace or sigreturn) */
+ int validate_user_xstate_header(const struct xstate_header *hdr);
+diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c
+index 28e8229b23a7..a0929c947d90 100644
+--- a/arch/x86/kernel/fpu/xstate.c
++++ b/arch/x86/kernel/fpu/xstate.c
+@@ -62,6 +62,7 @@ u64 xfeatures_mask_all __read_mostly;
+ static unsigned int xstate_offsets[XFEATURE_MAX] = { [ 0 ... XFEATURE_MAX - 1] = -1};
+ static unsigned int xstate_sizes[XFEATURE_MAX]   = { [ 0 ... XFEATURE_MAX - 1] = -1};
+ static unsigned int xstate_comp_offsets[XFEATURE_MAX] = { [ 0 ... XFEATURE_MAX - 1] = -1};
++static unsigned int xstate_supervisor_only_offsets[XFEATURE_MAX] = { [ 0 ... XFEATURE_MAX - 1] = -1};
+ 
+ /*
+  * The XSAVE area of kernel can be in standard or compacted format;
+@@ -392,6 +393,33 @@ static void __init setup_xstate_comp_offsets(void)
+ 	}
+ }
+ 
++/*
++ * Setup offsets of a supervisor-only XSAVES buffer:
++ *
++ * The offsets stored in xstate_comp_offsets[] only work for one specific
++ * value of the Requested Feature BitMap (RFBM).  In cases where a different
++ * RFBM value is used, a different set of offsets is required.  This set of
++ * offsets is for when RFBM=xfeatures_mask_supervisor().
++ */
++static void __init setup_supervisor_only_offsets(void)
++{
++	unsigned int next_offset;
++	int i;
++
++	next_offset = FXSAVE_SIZE + XSAVE_HDR_SIZE;
++
++	for (i = FIRST_EXTENDED_XFEATURE; i < XFEATURE_MAX; i++) {
++		if (!xfeature_enabled(i) || !xfeature_is_supervisor(i))
++			continue;
++
++		if (xfeature_is_aligned(i))
++			next_offset = ALIGN(next_offset, 64);
++
++		xstate_supervisor_only_offsets[i] = next_offset;
++		next_offset += xstate_sizes[i];
++	}
++}
++
+ /*
+  * Print out xstate component offsets and sizes
+  */
+@@ -790,6 +818,7 @@ void __init fpu__init_system_xstate(void)
+ 	fpu__init_prepare_fx_sw_frame();
+ 	setup_init_fpu_buf();
+ 	setup_xstate_comp_offsets();
++	setup_supervisor_only_offsets();
+ 	print_xstate_offset_size();
+ 
+ 	pr_info("x86/fpu: Enabled xstate features 0x%llx, context size is %d bytes, using '%s' format.\n",
+@@ -1257,6 +1286,60 @@ int copy_user_to_xstate(struct xregs_state *xsave, const void __user *ubuf)
+ 	return 0;
+ }
+ 
++/*
++ * Save only supervisor states to the kernel buffer.  This blows away all
++ * old states, and intended to be used only in __fpu__restore_sig(), where
++ * user states are restored from user buffer.
++ */
++void copy_supervisor_to_kernel(struct xregs_state *xstate)
++{
++	struct xstate_header *header;
++	u64 max_bit, min_bit;
++	u32 lmask, hmask;
++	int err, i;
++
++	if (WARN_ON(!boot_cpu_has(X86_FEATURE_XSAVES)))
++		return;
++
++	if (!xfeatures_mask_supervisor())
++		return;
++
++	max_bit = __fls(xfeatures_mask_supervisor());
++	min_bit = __ffs(xfeatures_mask_supervisor());
++
++	lmask = xfeatures_mask_supervisor();
++	hmask = xfeatures_mask_supervisor() >> 32;
++	XSTATE_OP(XSAVES, xstate, lmask, hmask, err);
++
++	/* We should never fault when copying to a kernel buffer: */
++	if (WARN_ON_FPU(err))
++		return;
++
++	/*
++	 * At this point, the buffer has only supervisor states and must be
++	 * converted back to normal kernel format.
++	 */
++	header = &xstate->header;
++	header->xcomp_bv |= xfeatures_mask_all;
++
++	/*
++	 * Each xstate[i] starts after the end of xstate[i - 1].
++	 * The memmove() below relies on this fact and walks backwards through
++	 * the states to preserve the content.
++	 */
++	for (i = max_bit; i >= min_bit; i--) {
++		u8 *xbuf = (u8 *)xstate;
++
++		if (!((header->xfeatures >> i) & 1))
++			continue;
++
++		/* Move xfeature 'i' into its normal location */
++		memmove(xbuf + xstate_comp_offsets[i],
++			xbuf + xstate_supervisor_only_offsets[i],
++			xstate_sizes[i]);
++	}
++}
++
+ #ifdef CONFIG_PROC_PID_ARCH_STATUS
+ /*
+  * Report the amount of time elapsed in millisecond since last AVX512
+-- 
+2.26.0
+
diff --git a/0012-x86-fpu-xstate-Restore-supervisor-xstates-for-__fpu_.patch b/0012-x86-fpu-xstate-Restore-supervisor-xstates-for-__fpu_.patch
new file mode 100644
index 000000000..557f00fd3
--- /dev/null
+++ b/0012-x86-fpu-xstate-Restore-supervisor-xstates-for-__fpu_.patch
@@ -0,0 +1,100 @@
+From ab525b5c6d21580d68a34238f281706d2d3d623a Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 29 Oct 2019 13:03:10 -0700
+Subject: [PATCH 12/52] x86/fpu/xstate: Restore supervisor xstates for
+ __fpu__restore_sig()
+
+The signal return code is responsible for taking an XSAVE buffer present
+in user memory and loading it into the hardware registers.  This
+operation only affects user XSAVE state and never affects supervisor state.
+
+The fast path through this code simply points XRSTOR directly at the
+user buffer.  However, due to page faults, this XRSTOR can fail.  If it
+fails, the signal return code falls back to a slow path which can
+tolerate page faults.
+
+That slow path copies the xfeatures one by one out of the user buffer
+into the task's fpu state area.  However, by being in a context where it
+can handle page faults, the code can also schedule.  That exposes us to
+the whims of the lazy-fpu-load code.  That code currently can not
+comprehend valid fpregs (the supervisor state) mixed with the *not*
+valid user fpregs.  To do that, we would need to track whether
+individual XSAVE components had valid fpregs or fpstate.
+
+If we left the current code in place, the context-switch code would
+think it has an up-to-date fpstate and would fail to save the supervisor
+state when scheduling the task out.  When scheduling back in, it would
+likely restore stale supervisor state.
+
+To fix that, we save the supervisor state earlier in the signal return
+code.  That way, the supervisor state is always up-to-date and the task
+can survive being scheduled.
+
+v3:
+- Change copy_xregs_to_kernel() to copy_supervisor_to_kernel(), which is
+  introduced in the previous patch.
+- Revise commit log.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
+---
+ arch/x86/kernel/fpu/signal.c | 25 ++++++++++++++++++++-----
+ 1 file changed, 20 insertions(+), 5 deletions(-)
+
+diff --git a/arch/x86/kernel/fpu/signal.c b/arch/x86/kernel/fpu/signal.c
+index d09d72334a12..15ddf174940b 100644
+--- a/arch/x86/kernel/fpu/signal.c
++++ b/arch/x86/kernel/fpu/signal.c
+@@ -328,14 +328,21 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 	}
+ 
+ 	/*
+-	 * The current state of the FPU registers does not matter. By setting
+-	 * TIF_NEED_FPU_LOAD unconditionally it is ensured that the our xstate
+-	 * is not modified on context switch and that the xstate is considered
++	 * Supervisor states are not modified by user space input.  Save
++	 * current supervisor states first.
++	 * By setting TIF_NEED_FPU_LOAD it is ensured that our xstate is
++	 * not modified on context switch and that the xstate is considered
+ 	 * to be loaded again on return to userland (overriding last_cpu avoids
+ 	 * the optimisation).
+ 	 */
+-	set_thread_flag(TIF_NEED_FPU_LOAD);
++	fpregs_lock();
++	if (!test_thread_flag(TIF_NEED_FPU_LOAD)) {
++		if (xfeatures_mask_supervisor())
++			copy_supervisor_to_kernel(&fpu->state.xsave);
++		set_thread_flag(TIF_NEED_FPU_LOAD);
++	}
+ 	__fpu_invalidate_fpregs_state(fpu);
++	fpregs_unlock();
+ 
+ 	if ((unsigned long)buf_fx % 64)
+ 		fx_only = 1;
+@@ -361,6 +368,9 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 		ret = copy_user_to_fpregs_zeroing(buf_fx, user_xfeatures, fx_only);
+ 		pagefault_enable();
+ 		if (!ret) {
++			if (xfeatures_mask_supervisor())
++				copy_kernel_to_xregs(&fpu->state.xsave,
++						     xfeatures_mask_supervisor());
+ 			fpregs_mark_activate();
+ 			fpregs_unlock();
+ 			return 0;
+@@ -390,7 +400,12 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 		fpregs_lock();
+ 		if (unlikely(init_bv))
+ 			copy_kernel_to_xregs(&init_fpstate.xsave, init_bv);
+-		ret = copy_kernel_to_xregs_err(&fpu->state.xsave, user_xfeatures);
++		/*
++		 * Restore previously saved supervisor xstates along with
++		 * copied-in user xstates.
++		 */
++		ret = copy_kernel_to_xregs_err(&fpu->state.xsave,
++					       user_xfeatures | xfeatures_mask_supervisor());
+ 
+ 	} else if (use_fxsr()) {
+ 		ret = __copy_from_user(&fpu->state.fxsave, buf_fx, state_size);
+-- 
+2.26.0
+
diff --git a/0013-Documentation-x86-Add-CET-description.patch b/0013-Documentation-x86-Add-CET-description.patch
new file mode 100644
index 000000000..3261f4404
--- /dev/null
+++ b/0013-Documentation-x86-Add-CET-description.patch
@@ -0,0 +1,195 @@
+From bb98f65a8f905835619a2eb4e3c59a2f8833f3ea Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Sun, 17 Dec 2017 09:09:23 -0800
+Subject: [PATCH 13/52] Documentation/x86: Add CET description
+
+Explain no_user_shstk/no_user_ibt kernel parameters, and introduce a new
+document on Control-flow Enforcement Technology (CET).
+
+v10:
+- In anticipation of kernel-mode CET, change no_cet_shstk, no_cet_ibt to
+  no_user_shstk, no_user_ibt
+- Remove the opcode section, as it is already in the Intel SDM.
+- Remove sections related to GLIBC implementation.
+- Remove shadow stack memory management section, as it is already in the
+  code comments.
+- Remove legacy bitmap related information, as it is not supported now.
+- Fix arch_ioctl() related text.
+- Change SHSTK, IBT to plain English.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+---
+ .../admin-guide/kernel-parameters.txt         |   6 +
+ Documentation/x86/index.rst                   |   1 +
+ Documentation/x86/intel_cet.rst               | 129 ++++++++++++++++++
+ 3 files changed, 136 insertions(+)
+ create mode 100644 Documentation/x86/intel_cet.rst
+
+diff --git a/Documentation/admin-guide/kernel-parameters.txt b/Documentation/admin-guide/kernel-parameters.txt
+index c07815d230bc..45653222a1d9 100644
+--- a/Documentation/admin-guide/kernel-parameters.txt
++++ b/Documentation/admin-guide/kernel-parameters.txt
+@@ -3050,6 +3050,12 @@
+ 			noexec=on: enable non-executable mappings (default)
+ 			noexec=off: disable non-executable mappings
+ 
++	no_user_shstk	[X86-64] Disable Shadow Stack for user-mode
++			applications
++
++	no_user_ibt	[X86-64] Disable Indirect Branch Tracking for user-mode
++			applications
++
+ 	nosmap		[X86,PPC]
+ 			Disable SMAP (Supervisor Mode Access Prevention)
+ 			even if it is supported by processor.
+diff --git a/Documentation/x86/index.rst b/Documentation/x86/index.rst
+index 265d9e9a093b..2aef972a868d 100644
+--- a/Documentation/x86/index.rst
++++ b/Documentation/x86/index.rst
+@@ -19,6 +19,7 @@ x86-specific Documentation
+    tlb
+    mtrr
+    pat
++   intel_cet
+    intel-iommu
+    intel_txt
+    amd-memory-encryption
+diff --git a/Documentation/x86/intel_cet.rst b/Documentation/x86/intel_cet.rst
+new file mode 100644
+index 000000000000..46c0b18d5d15
+--- /dev/null
++++ b/Documentation/x86/intel_cet.rst
+@@ -0,0 +1,129 @@
++.. SPDX-License-Identifier: GPL-2.0
++
++=========================================
++Control-flow Enforcement Technology (CET)
++=========================================
++
++[1] Overview
++============
++
++Control-flow Enforcement Technology (CET) is an Intel processor feature
++that provides protection against return/jump-oriented programming (ROP)
++attacks.  It can be set up to protect both applications and the kernel.
++Only user-mode protection is implemented in the 64-bit kernel, including
++support for running legacy 32-bit applications.
++
++CET introduces Shadow Stack and Indirect Branch Tracking.  Shadow stack is
++a secondary stack allocated from memory and cannot be directly modified by
++applications.  When executing a CALL, the processor pushes the return
++address to both the normal stack and the shadow stack.  Upon function
++return, the processor pops the shadow stack copy and compares it to the
++normal stack copy.  If the two differ, the processor raises a control-
++protection fault.  Indirect branch tracking verifies indirect CALL/JMP
++targets are intended as marked by the compiler with 'ENDBR' opcodes.
++
++There are two kernel configuration options:
++
++    X86_INTEL_SHADOW_STACK_USER, and
++    X86_INTEL_BRANCH_TRACKING_USER.
++
++These need to be enabled to build a CET-enabled kernel, and Binutils v2.31
++and GCC v8.1 or later are required to build a CET kernel.  To build a CET-
++enabled application, GLIBC v2.28 or later is also required.
++
++There are two command-line options for disabling CET features::
++
++    no_user_shstk - disables user shadow stack, and
++    no_user_ibt   - disables user indirect branch tracking.
++
++At run time, /proc/cpuinfo shows CET features if both the kernel and the
++processor support it.
++
++[2] Application Enabling
++========================
++
++An application's CET capability is marked in its ELF header and can be
++verified from the following command output, in the NT_GNU_PROPERTY_TYPE_0
++field:
++
++    readelf -n <application>
++
++If an application supports CET and is statically linked, it will run with
++CET protection.  If the application needs any shared libraries, the loader
++checks all dependencies and enables CET only when all requirements are met.
++
++[3] CET arch_prctl()'s
++======================
++
++Several arch_prctl()'s have been added for CET:
++
++arch_prctl(ARCH_X86_CET_STATUS, u64 *addr)
++    Return CET feature status.
++
++    The parameter 'addr' is a pointer to a user buffer.
++    On returning to the caller, the kernel fills the following
++    information::
++
++        *addr       = shadow stack/indirect branch tracking status
++        *(addr + 1) = shadow stack base address
++        *(addr + 2) = shadow stack size
++
++arch_prctl(ARCH_X86_CET_DISABLE, u64 features)
++    Disable shadow stack and/or indirect branch tracking as specified in
++    'features'.  Return -EPERM if CET is locked.
++
++arch_prctl(ARCH_X86_CET_LOCK)
++    Lock in all CET features.  They cannot be turned off afterwards.
++
++arch_prctl(ARCH_X86_CET_ALLOC_SHSTK, u64 *addr)
++    Allocate a new shadow stack and put a restore token at top.
++
++    The parameter 'addr' is a pointer to a user buffer and indicates the
++    desired shadow stack size to allocate.  On returning to the caller, the
++    kernel fills '*addr' with the base address of the new shadow stack.
++
++    User-level threads that need a new stack are expected to allocate a new
++    shadow stack.
++
++Note:
++  There is no CET-enabling arch_prctl function.  By design, CET is enabled
++  automatically if the binary and the system can support it.
++
++[4] The implementation of the Shadow Stack
++==========================================
++
++Shadow Stack size
++-----------------
++
++A task's shadow stack is allocated from memory to a fixed size of
++MIN(RLIMIT_STACK, 4 GB).  In other words, the shadow stack is allocated to
++the maximum size of the normal stack, but capped to 4 GB.  However, because
++a compat-mode application's address space is smaller, each of its thread's
++shadow stack size is 1/4 of RLIMIT_STACK.
++
++Signal
++------
++
++The main program and its signal handlers use the same shadow stack.
++Because the shadow stack stores only return addresses, a large shadow stack
++will cover the condition that both the program stack and the signal
++alternate stack run out.
++
++The kernel creates a restore token at the shadow stack restoring address and
++verifies that token when restoring from the signal handler.
++
++Fork
++----
++
++The shadow stack's vma has VM_SHSTK flag set; its PTEs are required to be
++read-only and dirty.  When a shadow PTE is not present, RO, and dirty, a
++shadow access triggers a page fault with an additional shadow stack bit set
++in the page fault error code.
++
++When a task forks a child, its shadow stack PTEs are copied and both the
++parent's and the child's shadow stack PTEs are cleared of the dirty bit.
++Upon the next shadow stack access, the resulting shadow stack page fault is
++handled by page copy/re-use.
++
++When a pthread child is created, the kernel allocates a new shadow stack for
++the new thread.
+-- 
+2.26.0
+
diff --git a/0014-x86-cpufeatures-Add-CET-CPU-feature-flags-for-Contro.patch b/0014-x86-cpufeatures-Add-CET-CPU-feature-flags-for-Contro.patch
new file mode 100644
index 000000000..52addb197
--- /dev/null
+++ b/0014-x86-cpufeatures-Add-CET-CPU-feature-flags-for-Contro.patch
@@ -0,0 +1,55 @@
+From 93e9e59f979f57076d6633c5b1108fdd2138cc9c Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Wed, 9 Nov 2016 16:26:37 -0800
+Subject: [PATCH 14/52] x86/cpufeatures: Add CET CPU feature flags for
+ Control-flow Enforcement Technology (CET)
+
+Add CPU feature flags for Control-flow Enforcement Technology (CET).
+
+CPUID.(EAX=7,ECX=0):ECX[bit 7] Shadow stack
+CPUID.(EAX=7,ECX=0):EDX[bit 20] Indirect Branch Tracking
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Borislav Petkov <bp@suse.de>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+---
+ arch/x86/include/asm/cpufeatures.h | 2 ++
+ arch/x86/kernel/cpu/cpuid-deps.c   | 2 ++
+ 2 files changed, 4 insertions(+)
+
+diff --git a/arch/x86/include/asm/cpufeatures.h b/arch/x86/include/asm/cpufeatures.h
+index f3327cb56edf..b970d0e4506b 100644
+--- a/arch/x86/include/asm/cpufeatures.h
++++ b/arch/x86/include/asm/cpufeatures.h
+@@ -337,6 +337,7 @@
+ #define X86_FEATURE_OSPKE		(16*32+ 4) /* OS Protection Keys Enable */
+ #define X86_FEATURE_WAITPKG		(16*32+ 5) /* UMONITOR/UMWAIT/TPAUSE Instructions */
+ #define X86_FEATURE_AVX512_VBMI2	(16*32+ 6) /* Additional AVX512 Vector Bit Manipulation Instructions */
++#define X86_FEATURE_SHSTK		(16*32+ 7) /* Shadow Stack */
+ #define X86_FEATURE_GFNI		(16*32+ 8) /* Galois Field New Instructions */
+ #define X86_FEATURE_VAES		(16*32+ 9) /* Vector AES */
+ #define X86_FEATURE_VPCLMULQDQ		(16*32+10) /* Carry-Less Multiplication Double Quadword */
+@@ -363,6 +364,7 @@
+ #define X86_FEATURE_MD_CLEAR		(18*32+10) /* VERW clears CPU buffers */
+ #define X86_FEATURE_TSX_FORCE_ABORT	(18*32+13) /* "" TSX_FORCE_ABORT */
+ #define X86_FEATURE_PCONFIG		(18*32+18) /* Intel PCONFIG */
++#define X86_FEATURE_IBT			(18*32+20) /* Indirect Branch Tracking */
+ #define X86_FEATURE_SPEC_CTRL		(18*32+26) /* "" Speculation Control (IBRS + IBPB) */
+ #define X86_FEATURE_INTEL_STIBP		(18*32+27) /* "" Single Thread Indirect Branch Predictors */
+ #define X86_FEATURE_FLUSH_L1D		(18*32+28) /* Flush L1D cache */
+diff --git a/arch/x86/kernel/cpu/cpuid-deps.c b/arch/x86/kernel/cpu/cpuid-deps.c
+index 3cbe24ca80ab..fec83cc74b9e 100644
+--- a/arch/x86/kernel/cpu/cpuid-deps.c
++++ b/arch/x86/kernel/cpu/cpuid-deps.c
+@@ -69,6 +69,8 @@ static const struct cpuid_dep cpuid_deps[] = {
+ 	{ X86_FEATURE_CQM_MBM_TOTAL,		X86_FEATURE_CQM_LLC   },
+ 	{ X86_FEATURE_CQM_MBM_LOCAL,		X86_FEATURE_CQM_LLC   },
+ 	{ X86_FEATURE_AVX512_BF16,		X86_FEATURE_AVX512VL  },
++	{ X86_FEATURE_SHSTK,			X86_FEATURE_XSAVES    },
++	{ X86_FEATURE_IBT,			X86_FEATURE_XSAVES    },
+ 	{}
+ };
+ 
+-- 
+2.26.0
+
diff --git a/0015-x86-fpu-xstate-Introduce-CET-MSR-XSAVES-supervisor-s.patch b/0015-x86-fpu-xstate-Introduce-CET-MSR-XSAVES-supervisor-s.patch
new file mode 100644
index 000000000..6da925109
--- /dev/null
+++ b/0015-x86-fpu-xstate-Introduce-CET-MSR-XSAVES-supervisor-s.patch
@@ -0,0 +1,203 @@
+From 8c991b34caa8bd62ecd8056bf4dabe303e68524d Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Thu, 10 Nov 2016 10:13:56 -0800
+Subject: [PATCH 15/52] x86/fpu/xstate: Introduce CET MSR XSAVES supervisor
+ states
+
+Control-flow Enforcement Technology (CET) adds five MSRs.  Introduce them
+and their XSAVES supervisor states:
+
+    MSR_IA32_U_CET (user-mode CET settings),
+    MSR_IA32_PL3_SSP (user-mode Shadow Stack pointer),
+    MSR_IA32_PL0_SSP (kernel-mode Shadow Stack pointer),
+    MSR_IA32_PL1_SSP (Privilege Level 1 Shadow Stack pointer),
+    MSR_IA32_PL2_SSP (Privilege Level 2 Shadow Stack pointer).
+
+v6:
+- Remove __packed from struct cet_user_state, struct cet_kernel_state.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+---
+ arch/x86/include/asm/fpu/types.h            | 22 ++++++++++++++++++
+ arch/x86/include/asm/fpu/xstate.h           |  5 +++--
+ arch/x86/include/asm/msr-index.h            | 18 +++++++++++++++
+ arch/x86/include/uapi/asm/processor-flags.h |  2 ++
+ arch/x86/kernel/fpu/xstate.c                | 25 +++++++++++++++++++--
+ 5 files changed, 68 insertions(+), 4 deletions(-)
+
+diff --git a/arch/x86/include/asm/fpu/types.h b/arch/x86/include/asm/fpu/types.h
+index f098f6cab94b..d7ef4d9c7ad5 100644
+--- a/arch/x86/include/asm/fpu/types.h
++++ b/arch/x86/include/asm/fpu/types.h
+@@ -114,6 +114,9 @@ enum xfeature {
+ 	XFEATURE_Hi16_ZMM,
+ 	XFEATURE_PT_UNIMPLEMENTED_SO_FAR,
+ 	XFEATURE_PKRU,
++	XFEATURE_RESERVED,
++	XFEATURE_CET_USER,
++	XFEATURE_CET_KERNEL,
+ 
+ 	XFEATURE_MAX,
+ };
+@@ -128,6 +131,8 @@ enum xfeature {
+ #define XFEATURE_MASK_Hi16_ZMM		(1 << XFEATURE_Hi16_ZMM)
+ #define XFEATURE_MASK_PT		(1 << XFEATURE_PT_UNIMPLEMENTED_SO_FAR)
+ #define XFEATURE_MASK_PKRU		(1 << XFEATURE_PKRU)
++#define XFEATURE_MASK_CET_USER		(1 << XFEATURE_CET_USER)
++#define XFEATURE_MASK_CET_KERNEL	(1 << XFEATURE_CET_KERNEL)
+ 
+ #define XFEATURE_MASK_FPSSE		(XFEATURE_MASK_FP | XFEATURE_MASK_SSE)
+ #define XFEATURE_MASK_AVX512		(XFEATURE_MASK_OPMASK \
+@@ -229,6 +234,23 @@ struct pkru_state {
+ 	u32				pad;
+ } __packed;
+ 
++/*
++ * State component 11 is Control-flow Enforcement user states
++ */
++struct cet_user_state {
++	u64 user_cet;			/* user control-flow settings */
++	u64 user_ssp;			/* user shadow stack pointer */
++};
++
++/*
++ * State component 12 is Control-flow Enforcement kernel states
++ */
++struct cet_kernel_state {
++	u64 kernel_ssp;			/* kernel shadow stack */
++	u64 pl1_ssp;			/* privilege level 1 shadow stack */
++	u64 pl2_ssp;			/* privilege level 2 shadow stack */
++};
++
+ struct xstate_header {
+ 	u64				xfeatures;
+ 	u64				xcomp_bv;
+diff --git a/arch/x86/include/asm/fpu/xstate.h b/arch/x86/include/asm/fpu/xstate.h
+index 422d8369012a..db89d796b22e 100644
+--- a/arch/x86/include/asm/fpu/xstate.h
++++ b/arch/x86/include/asm/fpu/xstate.h
+@@ -33,13 +33,14 @@
+ 				      XFEATURE_MASK_BNDCSR)
+ 
+ /* All currently supported supervisor features */
+-#define XFEATURE_MASK_SUPERVISOR_SUPPORTED (0)
++#define XFEATURE_MASK_SUPERVISOR_SUPPORTED (XFEATURE_MASK_CET_USER)
+ 
+ /*
+  * Unsupported supervisor features. When a supervisor feature in this mask is
+  * supported in the future, move it to the supported supervisor feature mask.
+  */
+-#define XFEATURE_MASK_SUPERVISOR_UNSUPPORTED (XFEATURE_MASK_PT)
++#define XFEATURE_MASK_SUPERVISOR_UNSUPPORTED (XFEATURE_MASK_PT | \
++					      XFEATURE_MASK_CET_KERNEL)
+ 
+ /* All supervisor states including supported and unsupported states. */
+ #define XFEATURE_MASK_SUPERVISOR_ALL (XFEATURE_MASK_SUPERVISOR_SUPPORTED | \
+diff --git a/arch/x86/include/asm/msr-index.h b/arch/x86/include/asm/msr-index.h
+index d5e517d1c3dd..411d732b395e 100644
+--- a/arch/x86/include/asm/msr-index.h
++++ b/arch/x86/include/asm/msr-index.h
+@@ -876,4 +876,22 @@
+ #define MSR_VM_IGNNE                    0xc0010115
+ #define MSR_VM_HSAVE_PA                 0xc0010117
+ 
++/* Control-flow Enforcement Technology MSRs */
++#define MSR_IA32_U_CET		0x6a0 /* user mode cet setting */
++#define MSR_IA32_S_CET		0x6a2 /* kernel mode cet setting */
++#define MSR_IA32_PL0_SSP	0x6a4 /* kernel shstk pointer */
++#define MSR_IA32_PL1_SSP	0x6a5 /* ring-1 shstk pointer */
++#define MSR_IA32_PL2_SSP	0x6a6 /* ring-2 shstk pointer */
++#define MSR_IA32_PL3_SSP	0x6a7 /* user shstk pointer */
++#define MSR_IA32_INT_SSP_TAB	0x6a8 /* exception shstk table */
++
++/* MSR_IA32_U_CET and MSR_IA32_S_CET bits */
++#define MSR_IA32_CET_SHSTK_EN		0x0000000000000001ULL
++#define MSR_IA32_CET_WRSS_EN		0x0000000000000002ULL
++#define MSR_IA32_CET_ENDBR_EN		0x0000000000000004ULL
++#define MSR_IA32_CET_LEG_IW_EN		0x0000000000000008ULL
++#define MSR_IA32_CET_NO_TRACK_EN	0x0000000000000010ULL
++#define MSR_IA32_CET_WAIT_ENDBR	0x00000000000000800UL
++#define MSR_IA32_CET_BITMAP_MASK	0xfffffffffffff000ULL
++
+ #endif /* _ASM_X86_MSR_INDEX_H */
+diff --git a/arch/x86/include/uapi/asm/processor-flags.h b/arch/x86/include/uapi/asm/processor-flags.h
+index bcba3c643e63..a8df907e8017 100644
+--- a/arch/x86/include/uapi/asm/processor-flags.h
++++ b/arch/x86/include/uapi/asm/processor-flags.h
+@@ -130,6 +130,8 @@
+ #define X86_CR4_SMAP		_BITUL(X86_CR4_SMAP_BIT)
+ #define X86_CR4_PKE_BIT		22 /* enable Protection Keys support */
+ #define X86_CR4_PKE		_BITUL(X86_CR4_PKE_BIT)
++#define X86_CR4_CET_BIT		23 /* enable Control-flow Enforcement */
++#define X86_CR4_CET		_BITUL(X86_CR4_CET_BIT)
+ 
+ /*
+  * x86-64 Task Priority Register, CR8
+diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c
+index a0929c947d90..9062ee19ce6d 100644
+--- a/arch/x86/kernel/fpu/xstate.c
++++ b/arch/x86/kernel/fpu/xstate.c
+@@ -38,6 +38,9 @@ static const char *xfeature_names[] =
+ 	"Processor Trace (unused)"	,
+ 	"Protection Keys User registers",
+ 	"unknown xstate feature"	,
++	"Control-flow User registers"	,
++	"Control-flow Kernel registers"	,
++	"unknown xstate feature"	,
+ };
+ 
+ static short xsave_cpuid_features[] __initdata = {
+@@ -51,6 +54,9 @@ static short xsave_cpuid_features[] __initdata = {
+ 	X86_FEATURE_AVX512F,
+ 	X86_FEATURE_INTEL_PT,
+ 	X86_FEATURE_PKU,
++	-1,		   /* Unused */
++	X86_FEATURE_SHSTK, /* XFEATURE_CET_USER */
++	X86_FEATURE_SHSTK, /* XFEATURE_CET_KERNEL */
+ };
+ 
+ /*
+@@ -316,6 +322,8 @@ static void __init print_xstate_features(void)
+ 	print_xstate_feature(XFEATURE_MASK_ZMM_Hi256);
+ 	print_xstate_feature(XFEATURE_MASK_Hi16_ZMM);
+ 	print_xstate_feature(XFEATURE_MASK_PKRU);
++	print_xstate_feature(XFEATURE_MASK_CET_USER);
++	print_xstate_feature(XFEATURE_MASK_CET_KERNEL);
+ }
+ 
+ /*
+@@ -590,6 +598,8 @@ static void check_xstate_against_struct(int nr)
+ 	XCHECK_SZ(sz, nr, XFEATURE_ZMM_Hi256, struct avx_512_zmm_uppers_state);
+ 	XCHECK_SZ(sz, nr, XFEATURE_Hi16_ZMM,  struct avx_512_hi16_state);
+ 	XCHECK_SZ(sz, nr, XFEATURE_PKRU,      struct pkru_state);
++	XCHECK_SZ(sz, nr, XFEATURE_CET_USER,   struct cet_user_state);
++	XCHECK_SZ(sz, nr, XFEATURE_CET_KERNEL, struct cet_kernel_state);
+ 
+ 	/*
+ 	 * Make *SURE* to add any feature numbers in below if
+@@ -797,8 +807,19 @@ void __init fpu__init_system_xstate(void)
+ 	 * Clear XSAVE features that are disabled in the normal CPUID.
+ 	 */
+ 	for (i = 0; i < ARRAY_SIZE(xsave_cpuid_features); i++) {
+-		if (!boot_cpu_has(xsave_cpuid_features[i]))
+-			xfeatures_mask_all &= ~BIT_ULL(i);
++		if (xsave_cpuid_features[i] == X86_FEATURE_SHSTK) {
++			/*
++			 * X86_FEATURE_SHSTK and X86_FEATURE_IBT share
++			 * same states, but can be enabled separately.
++			 */
++			if (!boot_cpu_has(X86_FEATURE_SHSTK) &&
++			    !boot_cpu_has(X86_FEATURE_IBT))
++				xfeatures_mask_all &= ~BIT_ULL(i);
++		} else {
++			if ((xsave_cpuid_features[i] == -1) ||
++			    !boot_cpu_has(xsave_cpuid_features[i]))
++				xfeatures_mask_all &= ~BIT_ULL(i);
++		}
+ 	}
+ 
+ 	xfeatures_mask_all &= fpu__get_supported_xfeatures_mask();
+-- 
+2.26.0
+
diff --git a/0016-x86-cet-Add-control-protection-fault-handler.patch b/0016-x86-cet-Add-control-protection-fault-handler.patch
new file mode 100644
index 000000000..83b4b85e9
--- /dev/null
+++ b/0016-x86-cet-Add-control-protection-fault-handler.patch
@@ -0,0 +1,188 @@
+From 984ca8fdd5152d919288314b54dbfffafa53f6de Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Thu, 10 Nov 2016 13:17:46 -0800
+Subject: [PATCH 16/52] x86/cet: Add control-protection fault handler
+
+A control-protection fault is triggered when a control-flow transfer
+attempt violates Shadow Stack or Indirect Branch Tracking constraints.
+For example, the return address for a RET instruction differs from the copy
+on the Shadow Stack; or an indirect JMP instruction, without the NOTRACK
+prefix, arrives at a non-ENDBR opcode.
+
+The control-protection fault handler works in a similar way as the general
+protection fault handler.  It provides the si_code SEGV_CPERR to the signal
+handler.
+
+v10:
+- Change CONFIG_X86_64 to CONFIG_X86_INTEL_CET in idt.c
+
+v9:
+- Add Shadow Stack pointer to the fault printout.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+---
+ arch/x86/entry/entry_64.S          |  2 +-
+ arch/x86/include/asm/traps.h       |  3 ++
+ arch/x86/kernel/idt.c              |  4 ++
+ arch/x86/kernel/signal_compat.c    |  2 +-
+ arch/x86/kernel/traps.c            | 59 ++++++++++++++++++++++++++++++
+ include/uapi/asm-generic/siginfo.h |  3 +-
+ 6 files changed, 70 insertions(+), 3 deletions(-)
+
+diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S
+index f2bb91e87877..31e1f590eb0b 100644
+--- a/arch/x86/entry/entry_64.S
++++ b/arch/x86/entry/entry_64.S
+@@ -1034,7 +1034,7 @@ idtentry spurious_interrupt_bug		do_spurious_interrupt_bug	has_error_code=0
+ idtentry coprocessor_error		do_coprocessor_error		has_error_code=0
+ idtentry alignment_check		do_alignment_check		has_error_code=1
+ idtentry simd_coprocessor_error		do_simd_coprocessor_error	has_error_code=0
+-
++idtentry control_protection		do_control_protection		has_error_code=1
+ 
+ 	/*
+ 	 * Reload gs selector with exception handling
+diff --git a/arch/x86/include/asm/traps.h b/arch/x86/include/asm/traps.h
+index ffa0dc8a535e..7ac26bbd0bef 100644
+--- a/arch/x86/include/asm/traps.h
++++ b/arch/x86/include/asm/traps.h
+@@ -26,6 +26,7 @@ asmlinkage void invalid_TSS(void);
+ asmlinkage void segment_not_present(void);
+ asmlinkage void stack_segment(void);
+ asmlinkage void general_protection(void);
++asmlinkage void control_protection(void);
+ asmlinkage void page_fault(void);
+ asmlinkage void async_page_fault(void);
+ asmlinkage void spurious_interrupt_bug(void);
+@@ -84,6 +85,7 @@ struct bad_iret_stack *fixup_bad_iret(struct bad_iret_stack *s);
+ void __init trap_init(void);
+ #endif
+ dotraplinkage void do_general_protection(struct pt_regs *regs, long error_code);
++dotraplinkage void do_control_protection(struct pt_regs *regs, long error_code);
+ dotraplinkage void do_page_fault(struct pt_regs *regs, unsigned long error_code, unsigned long address);
+ dotraplinkage void do_spurious_interrupt_bug(struct pt_regs *regs, long error_code);
+ dotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code);
+@@ -154,6 +156,7 @@ enum {
+ 	X86_TRAP_AC,		/* 17, Alignment Check */
+ 	X86_TRAP_MC,		/* 18, Machine Check */
+ 	X86_TRAP_XF,		/* 19, SIMD Floating-Point Exception */
++	X86_TRAP_CP = 21,	/* 21 Control Protection Fault */
+ 	X86_TRAP_IRET = 32,	/* 32, IRET Exception */
+ };
+ 
+diff --git a/arch/x86/kernel/idt.c b/arch/x86/kernel/idt.c
+index 87ef69a72c52..19160c8d734f 100644
+--- a/arch/x86/kernel/idt.c
++++ b/arch/x86/kernel/idt.c
+@@ -102,6 +102,10 @@ static const __initconst struct idt_data def_idts[] = {
+ #elif defined(CONFIG_X86_32)
+ 	SYSG(IA32_SYSCALL_VECTOR,	entry_INT80_32),
+ #endif
++
++#ifdef CONFIG_X86_INTEL_CET
++	INTG(X86_TRAP_CP,		control_protection),
++#endif
+ };
+ 
+ /*
+diff --git a/arch/x86/kernel/signal_compat.c b/arch/x86/kernel/signal_compat.c
+index 9ccbf0576cd0..c572a3de1037 100644
+--- a/arch/x86/kernel/signal_compat.c
++++ b/arch/x86/kernel/signal_compat.c
+@@ -27,7 +27,7 @@ static inline void signal_compat_build_tests(void)
+ 	 */
+ 	BUILD_BUG_ON(NSIGILL  != 11);
+ 	BUILD_BUG_ON(NSIGFPE  != 15);
+-	BUILD_BUG_ON(NSIGSEGV != 7);
++	BUILD_BUG_ON(NSIGSEGV != 8);
+ 	BUILD_BUG_ON(NSIGBUS  != 5);
+ 	BUILD_BUG_ON(NSIGTRAP != 5);
+ 	BUILD_BUG_ON(NSIGCHLD != 6);
+diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
+index 6ef00eb6fbb9..b8d386e16dc9 100644
+--- a/arch/x86/kernel/traps.c
++++ b/arch/x86/kernel/traps.c
+@@ -566,6 +566,65 @@ dotraplinkage void do_general_protection(struct pt_regs *regs, long error_code)
+ }
+ NOKPROBE_SYMBOL(do_general_protection);
+ 
++static const char * const control_protection_err[] = {
++	"unknown",
++	"near-ret",
++	"far-ret/iret",
++	"endbranch",
++	"rstorssp",
++	"setssbsy",
++};
++
++/*
++ * When a control protection exception occurs, send a signal
++ * to the responsible application.  Currently, control
++ * protection is only enabled for the user mode.  This
++ * exception should not come from the kernel mode.
++ */
++dotraplinkage void
++do_control_protection(struct pt_regs *regs, long error_code)
++{
++	struct task_struct *tsk;
++
++	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
++	if (notify_die(DIE_TRAP, "control protection fault", regs,
++		       error_code, X86_TRAP_CP, SIGSEGV) == NOTIFY_STOP)
++		return;
++	cond_local_irq_enable(regs);
++
++	if (!user_mode(regs))
++		die("kernel control protection fault", regs, error_code);
++
++	if (!static_cpu_has(X86_FEATURE_SHSTK) &&
++	    !static_cpu_has(X86_FEATURE_IBT))
++		WARN_ONCE(1, "CET is disabled but got control protection fault\n");
++
++	tsk = current;
++	tsk->thread.error_code = error_code;
++	tsk->thread.trap_nr = X86_TRAP_CP;
++
++	if (show_unhandled_signals && unhandled_signal(tsk, SIGSEGV) &&
++	    printk_ratelimit()) {
++		unsigned int max_err;
++		unsigned long ssp;
++
++		max_err = ARRAY_SIZE(control_protection_err) - 1;
++		if ((error_code < 0) || (error_code > max_err))
++			error_code = 0;
++		rdmsrl(MSR_IA32_PL3_SSP, ssp);
++		pr_info("%s[%d] control protection ip:%lx sp:%lx ssp:%lx error:%lx(%s)",
++			tsk->comm, task_pid_nr(tsk),
++			regs->ip, regs->sp, ssp, error_code,
++			control_protection_err[error_code]);
++		print_vma_addr(KERN_CONT " in ", regs->ip);
++		pr_cont("\n");
++	}
++
++	force_sig_fault(SIGSEGV, SEGV_CPERR,
++			(void __user *)uprobe_get_trap_addr(regs));
++}
++NOKPROBE_SYMBOL(do_control_protection);
++
+ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
+ {
+ 	if (poke_int3_handler(regs))
+diff --git a/include/uapi/asm-generic/siginfo.h b/include/uapi/asm-generic/siginfo.h
+index cb3d6c267181..693071dbe641 100644
+--- a/include/uapi/asm-generic/siginfo.h
++++ b/include/uapi/asm-generic/siginfo.h
+@@ -229,7 +229,8 @@ typedef struct siginfo {
+ #define SEGV_ACCADI	5	/* ADI not enabled for mapped object */
+ #define SEGV_ADIDERR	6	/* Disrupting MCD error */
+ #define SEGV_ADIPERR	7	/* Precise MCD exception */
+-#define NSIGSEGV	7
++#define SEGV_CPERR	8
++#define NSIGSEGV	8
+ 
+ /*
+  * SIGBUS si_codes
+-- 
+2.26.0
+
diff --git a/0017-x86-cet-shstk-Add-Kconfig-option-for-user-mode-Shado.patch b/0017-x86-cet-shstk-Add-Kconfig-option-for-user-mode-Shado.patch
new file mode 100644
index 000000000..95c6cdd49
--- /dev/null
+++ b/0017-x86-cet-shstk-Add-Kconfig-option-for-user-mode-Shado.patch
@@ -0,0 +1,83 @@
+From 94018efe47d030061a2ae9df34788c64c5f774b6 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 3 Oct 2017 12:55:03 -0700
+Subject: [PATCH 17/52] x86/cet/shstk: Add Kconfig option for user-mode Shadow
+ Stack protection
+
+Introduce Kconfig option: X86_INTEL_SHADOW_STACK_USER.
+
+Shadow Stack provides protection against function return address
+corruption.  It is active when the kernel has this feature enabled, and
+both the processor and the application support it.  When this feature is
+enabled, legacy non-shadow stack applications continue to work, but without
+protection.
+
+The user-mode shadow stack protection is only implemented for the 64-bit
+kernel.
+
+v10:
+- Change SHSTK to shadow stack in the help text.
+- Change build-time CET check to config depends on.
+- Revise help text.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/Kconfig                      | 30 +++++++++++++++++++++++++++
+ scripts/as-x86_64-has-shadow-stack.sh |  4 ++++
+ 2 files changed, 34 insertions(+)
+ create mode 100755 scripts/as-x86_64-has-shadow-stack.sh
+
+diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
+index beea77046f9b..8c9b2829c22b 100644
+--- a/arch/x86/Kconfig
++++ b/arch/x86/Kconfig
+@@ -1950,6 +1950,36 @@ config X86_INTEL_TSX_MODE_AUTO
+ 	  side channel attacks- equals the tsx=auto command line parameter.
+ endchoice
+ 
++config AS_HAS_SHADOW_STACK
++	def_bool $(success,$(srctree)/scripts/as-x86_64-has-shadow-stack.sh $(CC))
++	---help---
++	  Test the assembler for shadow stack instructions.
++
++config X86_INTEL_CET
++	def_bool n
++
++config ARCH_HAS_SHSTK
++	def_bool n
++
++config X86_INTEL_SHADOW_STACK_USER
++	prompt "Intel Shadow Stacks for user-mode"
++	def_bool n
++	depends on CPU_SUP_INTEL && X86_64
++	depends on AS_HAS_SHADOW_STACK
++	select ARCH_USES_HIGH_VMA_FLAGS
++	select X86_INTEL_CET
++	select ARCH_HAS_SHSTK
++	---help---
++	  Shadow Stacks provides protection against program stack
++	  corruption.  It's a hardware feature.  This only matters
++	  if you have the right hardware.  It's a security hardening
++	  feature and apps must be enabled to use it.  You get no
++	  protection "for free" on old userspace.  The hardware can
++	  support user and kernel, but this option is for user space
++	  only.
++
++	  If unsure, say y.
++
+ config EFI
+ 	bool "EFI runtime service support"
+ 	depends on ACPI
+diff --git a/scripts/as-x86_64-has-shadow-stack.sh b/scripts/as-x86_64-has-shadow-stack.sh
+new file mode 100755
+index 000000000000..fac1d363a1b8
+--- /dev/null
++++ b/scripts/as-x86_64-has-shadow-stack.sh
+@@ -0,0 +1,4 @@
++#!/bin/sh
++# SPDX-License-Identifier: GPL-2.0
++
++echo "wrussq %rax, (%rbx)" | $* -x assembler -c -
+-- 
+2.26.0
+
diff --git a/0018-mm-Introduce-VM_SHSTK-for-Shadow-Stack-memory.patch b/0018-mm-Introduce-VM_SHSTK-for-Shadow-Stack-memory.patch
new file mode 100644
index 000000000..0968384b7
--- /dev/null
+++ b/0018-mm-Introduce-VM_SHSTK-for-Shadow-Stack-memory.patch
@@ -0,0 +1,83 @@
+From 15b4c0e81929142d7e691175f6e7e9aec259265b Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Fri, 12 Jan 2018 15:04:54 -0800
+Subject: [PATCH 18/52] mm: Introduce VM_SHSTK for Shadow Stack memory
+
+A Shadow Stack (SHSTK) PTE must be read-only and have _PAGE_DIRTY set.
+However, read-only and Dirty PTEs also exist for copy-on-write (COW) pages.
+These two cases are handled differently for page faults and a new VM flag
+is necessary for tracking SHSTK VMAs.
+
+v9:
+- Add VM_SHSTK case to arch_vma_name().
+- Revise the commit log to explain why a new VM flag is needed.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+---
+ arch/x86/mm/mmap.c | 2 ++
+ fs/proc/task_mmu.c | 3 +++
+ include/linux/mm.h | 8 ++++++++
+ 3 files changed, 13 insertions(+)
+
+diff --git a/arch/x86/mm/mmap.c b/arch/x86/mm/mmap.c
+index cb91eccc4960..fe77fd6debf1 100644
+--- a/arch/x86/mm/mmap.c
++++ b/arch/x86/mm/mmap.c
+@@ -163,6 +163,8 @@ unsigned long get_mmap_base(int is_legacy)
+ 
+ const char *arch_vma_name(struct vm_area_struct *vma)
+ {
++	if (vma->vm_flags & VM_SHSTK)
++		return "[shadow stack]";
+ 	return NULL;
+ }
+ 
+diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c
+index 3ba9ae83bff5..1ed8575480be 100644
+--- a/fs/proc/task_mmu.c
++++ b/fs/proc/task_mmu.c
+@@ -687,6 +687,9 @@ static void show_smap_vma_flags(struct seq_file *m, struct vm_area_struct *vma)
+ 		[ilog2(VM_PKEY_BIT4)]	= "",
+ #endif
+ #endif /* CONFIG_ARCH_HAS_PKEYS */
++#ifdef CONFIG_X86_INTEL_SHADOW_STACK_USER
++		[ilog2(VM_SHSTK)]	= "ss",
++#endif
+ 	};
+ 	size_t i;
+ 
+diff --git a/include/linux/mm.h b/include/linux/mm.h
+index c54fb96cb1e6..e54f503a72f5 100644
+--- a/include/linux/mm.h
++++ b/include/linux/mm.h
+@@ -293,11 +293,13 @@ extern unsigned int kobjsize(const void *objp);
+ #define VM_HIGH_ARCH_BIT_2	34	/* bit only usable on 64-bit architectures */
+ #define VM_HIGH_ARCH_BIT_3	35	/* bit only usable on 64-bit architectures */
+ #define VM_HIGH_ARCH_BIT_4	36	/* bit only usable on 64-bit architectures */
++#define VM_HIGH_ARCH_BIT_5	37	/* bit only usable on 64-bit architectures */
+ #define VM_HIGH_ARCH_0	BIT(VM_HIGH_ARCH_BIT_0)
+ #define VM_HIGH_ARCH_1	BIT(VM_HIGH_ARCH_BIT_1)
+ #define VM_HIGH_ARCH_2	BIT(VM_HIGH_ARCH_BIT_2)
+ #define VM_HIGH_ARCH_3	BIT(VM_HIGH_ARCH_BIT_3)
+ #define VM_HIGH_ARCH_4	BIT(VM_HIGH_ARCH_BIT_4)
++#define VM_HIGH_ARCH_5	BIT(VM_HIGH_ARCH_BIT_5)
+ #endif /* CONFIG_ARCH_USES_HIGH_VMA_FLAGS */
+ 
+ #ifdef CONFIG_ARCH_HAS_PKEYS
+@@ -335,6 +337,12 @@ extern unsigned int kobjsize(const void *objp);
+ # define VM_MPX		VM_NONE
+ #endif
+ 
++#ifdef CONFIG_X86_INTEL_SHADOW_STACK_USER
++# define VM_SHSTK	VM_HIGH_ARCH_5
++#else
++# define VM_SHSTK	VM_NONE
++#endif
++
+ #ifndef VM_GROWSUP
+ # define VM_GROWSUP	VM_NONE
+ #endif
+-- 
+2.26.0
+
diff --git a/0019-Add-guard-pages-around-a-Shadow-Stack.patch b/0019-Add-guard-pages-around-a-Shadow-Stack.patch
new file mode 100644
index 000000000..25e5de87d
--- /dev/null
+++ b/0019-Add-guard-pages-around-a-Shadow-Stack.patch
@@ -0,0 +1,100 @@
+From 5bca064f76d2d0aca514d000bbfb35d36521480f Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Fri, 4 Oct 2019 14:00:58 -0700
+Subject: [PATCH 19/52] Add guard pages around a Shadow Stack.
+
+INCSSPQ/D increment shadow stack pointer and 'pop and discard' the first
+and the last elements in the range, effectively touch those memory areas.
+
+The maximum moving distance by INCSSPQ is 255 * 8 = 2040 bytes and
+255 * 4 = 1020 bytes for INCSSPD.  Both ranges are far from PAGE_SIZE.
+Putting a gap page on both ends of a shadow stack prevents INCSSP, CALL,
+and RET from going beyond.
+
+Define ARCH_SHADOW_STACK_GUARD_GAP and use that for shadow stack gap size.
+
+v10:
+- Define and use ARCH_SHADOW_STACK_GUARD_GAP.
+- Add comments.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/include/asm/processor.h | 10 ++++++++++
+ include/linux/mm.h               | 24 ++++++++++++++++++++----
+ 2 files changed, 30 insertions(+), 4 deletions(-)
+
+diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
+index 09705ccc393c..2db6242367a7 100644
+--- a/arch/x86/include/asm/processor.h
++++ b/arch/x86/include/asm/processor.h
+@@ -918,6 +918,16 @@ static inline void spin_lock_prefetch(const void *x)
+ #define STACK_TOP		TASK_SIZE_LOW
+ #define STACK_TOP_MAX		TASK_SIZE_MAX
+ 
++/*
++ * Shadow stack pointer is moved by CALL, JMP, and INCSSPQ/D.  INCSSPQ
++ * moves shadow stack pointer up to 255 * 8 = ~2 KB (~1KB for INCSSPD) and
++ * touches the first and the last element in the range, which triggers a
++ * page fault if the range is not in a shadow stack.  Because of this,
++ * creating 4-KB guard pages around a shadow stack prevents these
++ * instructions from going beyond.
++ */
++#define ARCH_SHADOW_STACK_GUARD_GAP PAGE_SIZE
++
+ #define INIT_THREAD  {						\
+ 	.addr_limit		= KERNEL_DS,			\
+ }
+diff --git a/include/linux/mm.h b/include/linux/mm.h
+index e54f503a72f5..88f4cf194db4 100644
+--- a/include/linux/mm.h
++++ b/include/linux/mm.h
+@@ -2428,6 +2428,10 @@ void page_cache_async_readahead(struct address_space *mapping,
+ 				pgoff_t offset,
+ 				unsigned long size);
+ 
++#ifndef ARCH_SHADOW_STACK_GUARD_GAP
++#define ARCH_SHADOW_STACK_GUARD_GAP 0
++#endif
++
+ extern unsigned long stack_guard_gap;
+ /* Generic expand stack which grows the stack according to GROWS{UP,DOWN} */
+ extern int expand_stack(struct vm_area_struct *vma, unsigned long address);
+@@ -2460,9 +2464,15 @@ static inline struct vm_area_struct * find_vma_intersection(struct mm_struct * m
+ static inline unsigned long vm_start_gap(struct vm_area_struct *vma)
+ {
+ 	unsigned long vm_start = vma->vm_start;
++	unsigned long gap = 0;
+ 
+-	if (vma->vm_flags & VM_GROWSDOWN) {
+-		vm_start -= stack_guard_gap;
++	if (vma->vm_flags & VM_GROWSDOWN)
++		gap = stack_guard_gap;
++	else if (vma->vm_flags & VM_SHSTK)
++		gap = ARCH_SHADOW_STACK_GUARD_GAP;
++
++	if (gap != 0) {
++		vm_start -= gap;
+ 		if (vm_start > vma->vm_start)
+ 			vm_start = 0;
+ 	}
+@@ -2472,9 +2482,15 @@ static inline unsigned long vm_start_gap(struct vm_area_struct *vma)
+ static inline unsigned long vm_end_gap(struct vm_area_struct *vma)
+ {
+ 	unsigned long vm_end = vma->vm_end;
++	unsigned long gap = 0;
++
++	if (vma->vm_flags & VM_GROWSUP)
++		gap = stack_guard_gap;
++	else if (vma->vm_flags & VM_SHSTK)
++		gap = ARCH_SHADOW_STACK_GUARD_GAP;
+ 
+-	if (vma->vm_flags & VM_GROWSUP) {
+-		vm_end += stack_guard_gap;
++	if (gap != 0) {
++		vm_end += gap;
+ 		if (vm_end < vma->vm_end)
+ 			vm_end = -PAGE_SIZE;
+ 	}
+-- 
+2.26.0
+
diff --git a/0020-x86-mm-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_HW.patch b/0020-x86-mm-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_HW.patch
new file mode 100644
index 000000000..9300ac178
--- /dev/null
+++ b/0020-x86-mm-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_HW.patch
@@ -0,0 +1,196 @@
+From 9a34db5533d543317c5307f41588350644d5d7a0 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Thu, 12 Apr 2018 09:32:59 -0700
+Subject: [PATCH 20/52] x86/mm: Change _PAGE_DIRTY to _PAGE_DIRTY_HW
+
+Before introducing _PAGE_DIRTY_SW for non-hardware memory management
+purposes in the next patch, rename _PAGE_DIRTY to _PAGE_DIRTY_HW and
+_PAGE_BIT_DIRTY to _PAGE_BIT_DIRTY_HW to make these PTE dirty bits
+more clear.  There are no functional changes from this patch.
+
+v9:
+- At some places _PAGE_DIRTY were not changed to _PAGE_DIRTY_HW, because
+  they will be changed again in the next patch to _PAGE_DIRTY_BITS.
+  However, this causes compile issues if the next patch is not yet applied.
+  Fix it by changing all _PAGE_DIRTY to _PAGE_DRITY_HW.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+---
+ arch/x86/include/asm/pgtable.h       | 18 +++++++++---------
+ arch/x86/include/asm/pgtable_types.h | 11 +++++------
+ arch/x86/kernel/relocate_kernel_64.S |  2 +-
+ arch/x86/kvm/vmx/vmx.c               |  2 +-
+ 4 files changed, 16 insertions(+), 17 deletions(-)
+
+diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
+index 7e118660bbd9..94fb2d05180f 100644
+--- a/arch/x86/include/asm/pgtable.h
++++ b/arch/x86/include/asm/pgtable.h
+@@ -123,7 +123,7 @@ extern pmdval_t early_pmd_flags;
+  */
+ static inline int pte_dirty(pte_t pte)
+ {
+-	return pte_flags(pte) & _PAGE_DIRTY;
++	return pte_flags(pte) & _PAGE_DIRTY_HW;
+ }
+ 
+ 
+@@ -162,7 +162,7 @@ static inline int pte_young(pte_t pte)
+ 
+ static inline int pmd_dirty(pmd_t pmd)
+ {
+-	return pmd_flags(pmd) & _PAGE_DIRTY;
++	return pmd_flags(pmd) & _PAGE_DIRTY_HW;
+ }
+ 
+ static inline int pmd_young(pmd_t pmd)
+@@ -172,7 +172,7 @@ static inline int pmd_young(pmd_t pmd)
+ 
+ static inline int pud_dirty(pud_t pud)
+ {
+-	return pud_flags(pud) & _PAGE_DIRTY;
++	return pud_flags(pud) & _PAGE_DIRTY_HW;
+ }
+ 
+ static inline int pud_young(pud_t pud)
+@@ -315,7 +315,7 @@ static inline pte_t pte_clear_flags(pte_t pte, pteval_t clear)
+ 
+ static inline pte_t pte_mkclean(pte_t pte)
+ {
+-	return pte_clear_flags(pte, _PAGE_DIRTY);
++	return pte_clear_flags(pte, _PAGE_DIRTY_HW);
+ }
+ 
+ static inline pte_t pte_mkold(pte_t pte)
+@@ -335,7 +335,7 @@ static inline pte_t pte_mkexec(pte_t pte)
+ 
+ static inline pte_t pte_mkdirty(pte_t pte)
+ {
+-	return pte_set_flags(pte, _PAGE_DIRTY | _PAGE_SOFT_DIRTY);
++	return pte_set_flags(pte, _PAGE_DIRTY_HW | _PAGE_SOFT_DIRTY);
+ }
+ 
+ static inline pte_t pte_mkyoung(pte_t pte)
+@@ -399,7 +399,7 @@ static inline pmd_t pmd_mkold(pmd_t pmd)
+ 
+ static inline pmd_t pmd_mkclean(pmd_t pmd)
+ {
+-	return pmd_clear_flags(pmd, _PAGE_DIRTY);
++	return pmd_clear_flags(pmd, _PAGE_DIRTY_HW);
+ }
+ 
+ static inline pmd_t pmd_wrprotect(pmd_t pmd)
+@@ -409,7 +409,7 @@ static inline pmd_t pmd_wrprotect(pmd_t pmd)
+ 
+ static inline pmd_t pmd_mkdirty(pmd_t pmd)
+ {
+-	return pmd_set_flags(pmd, _PAGE_DIRTY | _PAGE_SOFT_DIRTY);
++	return pmd_set_flags(pmd, _PAGE_DIRTY_HW | _PAGE_SOFT_DIRTY);
+ }
+ 
+ static inline pmd_t pmd_mkdevmap(pmd_t pmd)
+@@ -453,7 +453,7 @@ static inline pud_t pud_mkold(pud_t pud)
+ 
+ static inline pud_t pud_mkclean(pud_t pud)
+ {
+-	return pud_clear_flags(pud, _PAGE_DIRTY);
++	return pud_clear_flags(pud, _PAGE_DIRTY_HW);
+ }
+ 
+ static inline pud_t pud_wrprotect(pud_t pud)
+@@ -463,7 +463,7 @@ static inline pud_t pud_wrprotect(pud_t pud)
+ 
+ static inline pud_t pud_mkdirty(pud_t pud)
+ {
+-	return pud_set_flags(pud, _PAGE_DIRTY | _PAGE_SOFT_DIRTY);
++	return pud_set_flags(pud, _PAGE_DIRTY_HW | _PAGE_SOFT_DIRTY);
+ }
+ 
+ static inline pud_t pud_mkdevmap(pud_t pud)
+diff --git a/arch/x86/include/asm/pgtable_types.h b/arch/x86/include/asm/pgtable_types.h
+index 0239998d8cdc..36c03f4510c7 100644
+--- a/arch/x86/include/asm/pgtable_types.h
++++ b/arch/x86/include/asm/pgtable_types.h
+@@ -15,7 +15,7 @@
+ #define _PAGE_BIT_PWT		3	/* page write through */
+ #define _PAGE_BIT_PCD		4	/* page cache disabled */
+ #define _PAGE_BIT_ACCESSED	5	/* was accessed (raised by CPU) */
+-#define _PAGE_BIT_DIRTY		6	/* was written to (raised by CPU) */
++#define _PAGE_BIT_DIRTY_HW	6	/* was written to (raised by CPU) */
+ #define _PAGE_BIT_PSE		7	/* 4 MB (or 2MB) page */
+ #define _PAGE_BIT_PAT		7	/* on 4KB pages */
+ #define _PAGE_BIT_GLOBAL	8	/* Global TLB entry PPro+ */
+@@ -45,7 +45,7 @@
+ #define _PAGE_PWT	(_AT(pteval_t, 1) << _PAGE_BIT_PWT)
+ #define _PAGE_PCD	(_AT(pteval_t, 1) << _PAGE_BIT_PCD)
+ #define _PAGE_ACCESSED	(_AT(pteval_t, 1) << _PAGE_BIT_ACCESSED)
+-#define _PAGE_DIRTY	(_AT(pteval_t, 1) << _PAGE_BIT_DIRTY)
++#define _PAGE_DIRTY_HW	(_AT(pteval_t, 1) << _PAGE_BIT_DIRTY_HW)
+ #define _PAGE_PSE	(_AT(pteval_t, 1) << _PAGE_BIT_PSE)
+ #define _PAGE_GLOBAL	(_AT(pteval_t, 1) << _PAGE_BIT_GLOBAL)
+ #define _PAGE_SOFTW1	(_AT(pteval_t, 1) << _PAGE_BIT_SOFTW1)
+@@ -73,7 +73,7 @@
+ 			 _PAGE_PKEY_BIT3)
+ 
+ #if defined(CONFIG_X86_64) || defined(CONFIG_X86_PAE)
+-#define _PAGE_KNL_ERRATUM_MASK (_PAGE_DIRTY | _PAGE_ACCESSED)
++#define _PAGE_KNL_ERRATUM_MASK (_PAGE_DIRTY_HW | _PAGE_ACCESSED)
+ #else
+ #define _PAGE_KNL_ERRATUM_MASK 0
+ #endif
+@@ -117,7 +117,7 @@
+  * pte_modify() does modify it.
+  */
+ #define _PAGE_CHG_MASK	(PTE_PFN_MASK | _PAGE_PCD | _PAGE_PWT |		\
+-			 _PAGE_SPECIAL | _PAGE_ACCESSED | _PAGE_DIRTY |	\
++			 _PAGE_SPECIAL | _PAGE_ACCESSED | _PAGE_DIRTY_HW |	\
+ 			 _PAGE_SOFT_DIRTY | _PAGE_DEVMAP)
+ #define _HPAGE_CHG_MASK (_PAGE_CHG_MASK | _PAGE_PSE)
+ 
+@@ -153,7 +153,7 @@ enum page_cache_mode {
+ #define __RW _PAGE_RW
+ #define _USR _PAGE_USER
+ #define ___A _PAGE_ACCESSED
+-#define ___D _PAGE_DIRTY
++#define ___D _PAGE_DIRTY_HW
+ #define ___G _PAGE_GLOBAL
+ #define __NX _PAGE_NX
+ 
+@@ -195,7 +195,6 @@ enum page_cache_mode {
+ #define __PAGE_KERNEL_IO		__PAGE_KERNEL
+ #define __PAGE_KERNEL_IO_NOCACHE	__PAGE_KERNEL_NOCACHE
+ 
+-
+ #ifndef __ASSEMBLY__
+ 
+ #define __PAGE_KERNEL_ENC	(__PAGE_KERNEL    | _ENC)
+diff --git a/arch/x86/kernel/relocate_kernel_64.S b/arch/x86/kernel/relocate_kernel_64.S
+index ef3ba99068d3..3acd75f97b61 100644
+--- a/arch/x86/kernel/relocate_kernel_64.S
++++ b/arch/x86/kernel/relocate_kernel_64.S
+@@ -15,7 +15,7 @@
+  */
+ 
+ #define PTR(x) (x << 3)
+-#define PAGE_ATTR (_PAGE_PRESENT | _PAGE_RW | _PAGE_ACCESSED | _PAGE_DIRTY)
++#define PAGE_ATTR (_PAGE_PRESENT | _PAGE_RW | _PAGE_ACCESSED | _PAGE_DIRTY_HW)
+ 
+ /*
+  * control_page + KEXEC_CONTROL_CODE_MAX_SIZE
+diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
+index 079d9fbf278e..b409d25276fb 100644
+--- a/arch/x86/kvm/vmx/vmx.c
++++ b/arch/x86/kvm/vmx/vmx.c
+@@ -3538,7 +3538,7 @@ static int init_rmode_identity_map(struct kvm *kvm)
+ 	/* Set up identity-mapping pagetable for EPT in real mode */
+ 	for (i = 0; i < PT32_ENT_PER_PAGE; i++) {
+ 		tmp = (i << 22) + (_PAGE_PRESENT | _PAGE_RW | _PAGE_USER |
+-			_PAGE_ACCESSED | _PAGE_DIRTY | _PAGE_PSE);
++			_PAGE_ACCESSED | _PAGE_DIRTY_HW | _PAGE_PSE);
+ 		r = kvm_write_guest_page(kvm, identity_map_pfn,
+ 				&tmp, i * sizeof(tmp), sizeof(tmp));
+ 		if (r < 0)
+-- 
+2.26.0
+
diff --git a/0021-x86-mm-Introduce-_PAGE_DIRTY_SW.patch b/0021-x86-mm-Introduce-_PAGE_DIRTY_SW.patch
new file mode 100644
index 000000000..d0142284e
--- /dev/null
+++ b/0021-x86-mm-Introduce-_PAGE_DIRTY_SW.patch
@@ -0,0 +1,327 @@
+From 25f4bcec72e8e1667de6f779d5f44aaa986d5913 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Wed, 24 Jan 2018 10:27:13 -0800
+Subject: [PATCH 21/52] x86/mm: Introduce _PAGE_DIRTY_SW
+
+When Shadow Stack (SHSTK) is introduced, a R/O and Dirty PTE exists in the
+following cases:
+
+(a) A modified, copy-on-write (COW) page;
+(b) A R/O page that has been COW'ed;
+(c) A SHSTK page.
+
+To separate non-SHSTK memory from SHSTK, introduce a spare bit of the
+64-bit PTE as _PAGE_BIT_DIRTY_SW and use that for case (a) and (b).
+This results in the following possible settings:
+
+Modified PTE:         (R/W + DIRTY_HW)
+Modified and COW PTE: (R/O + DIRTY_SW)
+R/O PTE COW'ed:       (R/O + DIRTY_SW)
+SHSTK PTE:            (R/O + DIRTY_HW)
+SHSTK shared PTE[1]:  (R/O + DIRTY_SW)
+SHSTK PTE COW'ed:     (R/O + DIRTY_HW)
+
+[1] When a SHSTK page is being shared among threads, its PTE is cleared of
+    _PAGE_DIRTY_HW, so the next SHSTK access causes a fault, and the page
+    is duplicated and _PAGE_DIRTY_HW is set again.
+
+With this, in pte_wrprotect(), if SHSTK is active, use _PAGE_DIRTY_SW for
+the Dirty bit, and in pte_mkwrite() use _PAGE_DIRTY_HW.  The same changes
+apply to pmd and pud.
+
+When this patch is applied, there are six free bits left in the 64-bit PTE.
+There are no more free bits in the 32-bit PTE (except for PAE) and SHSTK is
+not implemented for the 32-bit kernel.
+
+v9:
+- Remove pte_move_flags() etc. and put the logic directly in
+  pte_wrprotect()/pte_mkwrite() etc.
+- Change compile-time conditionals to run-time checks.
+- Split out pte_modify()/pmd_modify() to a new patch.
+- Update comments.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+---
+ arch/x86/include/asm/pgtable.h       | 111 ++++++++++++++++++++++++---
+ arch/x86/include/asm/pgtable_types.h |  31 +++++++-
+ 2 files changed, 131 insertions(+), 11 deletions(-)
+
+diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
+index 94fb2d05180f..ba6325614690 100644
+--- a/arch/x86/include/asm/pgtable.h
++++ b/arch/x86/include/asm/pgtable.h
+@@ -121,9 +121,9 @@ extern pmdval_t early_pmd_flags;
+  * The following only work if pte_present() is true.
+  * Undefined behaviour if not..
+  */
+-static inline int pte_dirty(pte_t pte)
++static inline bool pte_dirty(pte_t pte)
+ {
+-	return pte_flags(pte) & _PAGE_DIRTY_HW;
++	return pte_flags(pte) & _PAGE_DIRTY_BITS;
+ }
+ 
+ 
+@@ -160,9 +160,9 @@ static inline int pte_young(pte_t pte)
+ 	return pte_flags(pte) & _PAGE_ACCESSED;
+ }
+ 
+-static inline int pmd_dirty(pmd_t pmd)
++static inline bool pmd_dirty(pmd_t pmd)
+ {
+-	return pmd_flags(pmd) & _PAGE_DIRTY_HW;
++	return pmd_flags(pmd) & _PAGE_DIRTY_BITS;
+ }
+ 
+ static inline int pmd_young(pmd_t pmd)
+@@ -170,9 +170,9 @@ static inline int pmd_young(pmd_t pmd)
+ 	return pmd_flags(pmd) & _PAGE_ACCESSED;
+ }
+ 
+-static inline int pud_dirty(pud_t pud)
++static inline bool pud_dirty(pud_t pud)
+ {
+-	return pud_flags(pud) & _PAGE_DIRTY_HW;
++	return pud_flags(pud) & _PAGE_DIRTY_BITS;
+ }
+ 
+ static inline int pud_young(pud_t pud)
+@@ -315,7 +315,7 @@ static inline pte_t pte_clear_flags(pte_t pte, pteval_t clear)
+ 
+ static inline pte_t pte_mkclean(pte_t pte)
+ {
+-	return pte_clear_flags(pte, _PAGE_DIRTY_HW);
++	return pte_clear_flags(pte, _PAGE_DIRTY_BITS);
+ }
+ 
+ static inline pte_t pte_mkold(pte_t pte)
+@@ -325,6 +325,17 @@ static inline pte_t pte_mkold(pte_t pte)
+ 
+ static inline pte_t pte_wrprotect(pte_t pte)
+ {
++	/*
++	 * Use _PAGE_DIRTY_SW on a R/O PTE to set it apart from
++	 * a Shadow Stack PTE, which is R/O + _PAGE_DIRTY_HW.
++	 */
++	if (static_cpu_has(X86_FEATURE_SHSTK)) {
++		if (pte_flags(pte) & _PAGE_DIRTY_HW) {
++			pte = pte_clear_flags(pte, _PAGE_DIRTY_HW);
++			pte = pte_set_flags(pte, _PAGE_DIRTY_SW);
++		}
++	}
++
+ 	return pte_clear_flags(pte, _PAGE_RW);
+ }
+ 
+@@ -335,9 +346,25 @@ static inline pte_t pte_mkexec(pte_t pte)
+ 
+ static inline pte_t pte_mkdirty(pte_t pte)
+ {
++	pteval_t dirty = _PAGE_DIRTY_HW;
++
++	if (static_cpu_has(X86_FEATURE_SHSTK) && !pte_write(pte))
++		dirty = _PAGE_DIRTY_SW;
++
++	return pte_set_flags(pte, dirty | _PAGE_SOFT_DIRTY);
++}
++
++static inline pte_t pte_mkdirty_shstk(pte_t pte)
++{
++	pte = pte_clear_flags(pte, _PAGE_DIRTY_SW);
+ 	return pte_set_flags(pte, _PAGE_DIRTY_HW | _PAGE_SOFT_DIRTY);
+ }
+ 
++static inline bool pte_dirty_hw(pte_t pte)
++{
++	return pte_flags(pte) & _PAGE_DIRTY_HW;
++}
++
+ static inline pte_t pte_mkyoung(pte_t pte)
+ {
+ 	return pte_set_flags(pte, _PAGE_ACCESSED);
+@@ -345,6 +372,13 @@ static inline pte_t pte_mkyoung(pte_t pte)
+ 
+ static inline pte_t pte_mkwrite(pte_t pte)
+ {
++	if (static_cpu_has(X86_FEATURE_SHSTK)) {
++		if (pte_flags(pte) & _PAGE_DIRTY_SW) {
++			pte = pte_clear_flags(pte, _PAGE_DIRTY_SW);
++			pte = pte_set_flags(pte, _PAGE_DIRTY_HW);
++		}
++	}
++
+ 	return pte_set_flags(pte, _PAGE_RW);
+ }
+ 
+@@ -399,19 +433,46 @@ static inline pmd_t pmd_mkold(pmd_t pmd)
+ 
+ static inline pmd_t pmd_mkclean(pmd_t pmd)
+ {
+-	return pmd_clear_flags(pmd, _PAGE_DIRTY_HW);
++	return pmd_clear_flags(pmd, _PAGE_DIRTY_BITS);
+ }
+ 
+ static inline pmd_t pmd_wrprotect(pmd_t pmd)
+ {
++	/*
++	 * Use _PAGE_DIRTY_SW on a R/O PMD to set it apart from
++	 * a Shadow Stack PTE, which is R/O + _PAGE_DIRTY_HW.
++	 */
++	if (static_cpu_has(X86_FEATURE_SHSTK)) {
++		if (pmd_flags(pmd) & _PAGE_DIRTY_HW) {
++			pmd = pmd_clear_flags(pmd, _PAGE_DIRTY_HW);
++			pmd = pmd_set_flags(pmd, _PAGE_DIRTY_SW);
++		}
++	}
++
+ 	return pmd_clear_flags(pmd, _PAGE_RW);
+ }
+ 
+ static inline pmd_t pmd_mkdirty(pmd_t pmd)
+ {
++	pmdval_t dirty = _PAGE_DIRTY_HW;
++
++	if (static_cpu_has(X86_FEATURE_SHSTK) && !(pmd_flags(pmd) & _PAGE_RW))
++		dirty = _PAGE_DIRTY_SW;
++
++	return pmd_set_flags(pmd, dirty | _PAGE_SOFT_DIRTY);
++}
++
++static inline pmd_t pmd_mkdirty_shstk(pmd_t pmd)
++{
++	pmd = pmd_clear_flags(pmd, _PAGE_DIRTY_SW);
+ 	return pmd_set_flags(pmd, _PAGE_DIRTY_HW | _PAGE_SOFT_DIRTY);
+ }
+ 
++static inline bool pmd_dirty_hw(pmd_t pmd)
++{
++	return  pmd_flags(pmd) & _PAGE_DIRTY_HW;
++}
++
+ static inline pmd_t pmd_mkdevmap(pmd_t pmd)
+ {
+ 	return pmd_set_flags(pmd, _PAGE_DEVMAP);
+@@ -429,6 +490,13 @@ static inline pmd_t pmd_mkyoung(pmd_t pmd)
+ 
+ static inline pmd_t pmd_mkwrite(pmd_t pmd)
+ {
++	if (static_cpu_has(X86_FEATURE_SHSTK)) {
++		if (pmd_flags(pmd) & _PAGE_DIRTY_SW) {
++			pmd = pmd_clear_flags(pmd, _PAGE_DIRTY_SW);
++			pmd = pmd_set_flags(pmd, _PAGE_DIRTY_HW);
++		}
++	}
++
+ 	return pmd_set_flags(pmd, _PAGE_RW);
+ }
+ 
+@@ -453,17 +521,33 @@ static inline pud_t pud_mkold(pud_t pud)
+ 
+ static inline pud_t pud_mkclean(pud_t pud)
+ {
+-	return pud_clear_flags(pud, _PAGE_DIRTY_HW);
++	return pud_clear_flags(pud, _PAGE_DIRTY_BITS);
+ }
+ 
+ static inline pud_t pud_wrprotect(pud_t pud)
+ {
++	/*
++	 * Use _PAGE_DIRTY_SW on a R/O PUD to set it apart from
++	 * a Shadow Stack PTE, which is R/O + _PAGE_DIRTY_HW.
++	 */
++	if (static_cpu_has(X86_FEATURE_SHSTK)) {
++		if (pud_flags(pud) & _PAGE_DIRTY_HW) {
++			pud = pud_clear_flags(pud, _PAGE_DIRTY_HW);
++			pud = pud_set_flags(pud, _PAGE_DIRTY_SW);
++		}
++	}
++
+ 	return pud_clear_flags(pud, _PAGE_RW);
+ }
+ 
+ static inline pud_t pud_mkdirty(pud_t pud)
+ {
+-	return pud_set_flags(pud, _PAGE_DIRTY_HW | _PAGE_SOFT_DIRTY);
++	pudval_t dirty = _PAGE_DIRTY_HW;
++
++	if (static_cpu_has(X86_FEATURE_SHSTK) && !(pud_flags(pud) & _PAGE_RW))
++		dirty = _PAGE_DIRTY_SW;
++
++	return pud_set_flags(pud, dirty | _PAGE_SOFT_DIRTY);
+ }
+ 
+ static inline pud_t pud_mkdevmap(pud_t pud)
+@@ -483,6 +567,13 @@ static inline pud_t pud_mkyoung(pud_t pud)
+ 
+ static inline pud_t pud_mkwrite(pud_t pud)
+ {
++	if (static_cpu_has(X86_FEATURE_SHSTK)) {
++		if (pud_flags(pud) & _PAGE_DIRTY_SW) {
++			pud = pud_clear_flags(pud, _PAGE_DIRTY_SW);
++			pud = pud_set_flags(pud, _PAGE_DIRTY_HW);
++		}
++	}
++
+ 	return pud_set_flags(pud, _PAGE_RW);
+ }
+ 
+diff --git a/arch/x86/include/asm/pgtable_types.h b/arch/x86/include/asm/pgtable_types.h
+index 36c03f4510c7..17735fa3ece8 100644
+--- a/arch/x86/include/asm/pgtable_types.h
++++ b/arch/x86/include/asm/pgtable_types.h
+@@ -23,7 +23,8 @@
+ #define _PAGE_BIT_SOFTW2	10	/* " */
+ #define _PAGE_BIT_SOFTW3	11	/* " */
+ #define _PAGE_BIT_PAT_LARGE	12	/* On 2MB or 1GB pages */
+-#define _PAGE_BIT_SOFTW4	58	/* available for programmer */
++#define _PAGE_BIT_SOFTW4	57	/* available for programmer */
++#define _PAGE_BIT_SOFTW5	58	/* available for programmer */
+ #define _PAGE_BIT_PKEY_BIT0	59	/* Protection Keys, bit 1/4 */
+ #define _PAGE_BIT_PKEY_BIT1	60	/* Protection Keys, bit 2/4 */
+ #define _PAGE_BIT_PKEY_BIT2	61	/* Protection Keys, bit 3/4 */
+@@ -35,6 +36,12 @@
+ #define _PAGE_BIT_SOFT_DIRTY	_PAGE_BIT_SOFTW3 /* software dirty tracking */
+ #define _PAGE_BIT_DEVMAP	_PAGE_BIT_SOFTW4
+ 
++/*
++ * This bit indicates a copy-on-write page, and is different from
++ * _PAGE_BIT_SOFT_DIRTY, which tracks which pages a task writes to.
++ */
++#define _PAGE_BIT_DIRTY_SW	_PAGE_BIT_SOFTW5 /* was written to */
++
+ /* If _PAGE_BIT_PRESENT is clear, we use these: */
+ /* - if the user mapped it with PROT_NONE; pte_present gives true */
+ #define _PAGE_BIT_PROTNONE	_PAGE_BIT_GLOBAL
+@@ -108,6 +115,28 @@
+ #define _PAGE_DEVMAP	(_AT(pteval_t, 0))
+ #endif
+ 
++/* A R/O and dirty PTE exists in the following cases:
++ *	(a) A modified, copy-on-write (COW) page;
++ *	(b) A R/O page that has been COW'ed;
++ *	(c) A SHSTK page.
++ * _PAGE_DIRTY_SW is used to separate case (c) from others.
++ * This results in the following settings:
++ *
++ *	Modified PTE:         (R/W + DIRTY_HW)
++ *	Modified and COW PTE: (R/O + DIRTY_SW)
++ *	R/O PTE COW'ed:       (R/O + DIRTY_SW)
++ *	SHSTK PTE:            (R/O + DIRTY_HW)
++ *	SHSTK PTE COW'ed:     (R/O + DIRTY_HW)
++ *	SHSTK PTE being shared among threads: (R/O + DIRTY_SW)
++ */
++#ifdef CONFIG_X86_INTEL_SHADOW_STACK_USER
++#define _PAGE_DIRTY_SW	(_AT(pteval_t, 1) << _PAGE_BIT_DIRTY_SW)
++#else
++#define _PAGE_DIRTY_SW	(_AT(pteval_t, 0))
++#endif
++
++#define _PAGE_DIRTY_BITS (_PAGE_DIRTY_HW | _PAGE_DIRTY_SW)
++
+ #define _PAGE_PROTNONE	(_AT(pteval_t, 1) << _PAGE_BIT_PROTNONE)
+ 
+ /*
+-- 
+2.26.0
+
diff --git a/0022-x86-mm-Update-pte_modify-pmd_modify-and-_PAGE_CHG_MA.patch b/0022-x86-mm-Update-pte_modify-pmd_modify-and-_PAGE_CHG_MA.patch
new file mode 100644
index 000000000..fd95dfbcd
--- /dev/null
+++ b/0022-x86-mm-Update-pte_modify-pmd_modify-and-_PAGE_CHG_MA.patch
@@ -0,0 +1,71 @@
+From a75599ccc6a83d68389038d374d0690495cd8d7d Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Thu, 29 Aug 2019 09:24:13 -0700
+Subject: [PATCH 22/52] x86/mm: Update pte_modify, pmd_modify, and
+ _PAGE_CHG_MASK for _PAGE_DIRTY_SW
+
+After the introduction of _PAGE_DIRTY_SW, pte_modify and pmd_modify need to
+set the Dirty bit accordingly: if Shadow Stack is enabled and _PAGE_RW is
+cleared, use _PAGE_DIRTY_SW; otherwise _PAGE_DIRTY_HW.
+
+Since the Dirty bit is modify by pte_modify(), remove _PAGE_DIRTY_HW from
+PAGE_CHG_MASK.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/include/asm/pgtable.h       | 16 ++++++++++++++++
+ arch/x86/include/asm/pgtable_types.h |  4 ++--
+ 2 files changed, 18 insertions(+), 2 deletions(-)
+
+diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
+index ba6325614690..11e64cfc8738 100644
+--- a/arch/x86/include/asm/pgtable.h
++++ b/arch/x86/include/asm/pgtable.h
+@@ -705,6 +705,14 @@ static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
+ 	val &= _PAGE_CHG_MASK;
+ 	val |= check_pgprot(newprot) & ~_PAGE_CHG_MASK;
+ 	val = flip_protnone_guard(oldval, val, PTE_PFN_MASK);
++
++	if (pte_dirty(pte)) {
++		if (static_cpu_has(X86_FEATURE_SHSTK) && !(val & _PAGE_RW))
++			val |= _PAGE_DIRTY_SW;
++		else
++			val |= _PAGE_DIRTY_HW;
++	}
++
+ 	return __pte(val);
+ }
+ 
+@@ -715,6 +723,14 @@ static inline pmd_t pmd_modify(pmd_t pmd, pgprot_t newprot)
+ 	val &= _HPAGE_CHG_MASK;
+ 	val |= check_pgprot(newprot) & ~_HPAGE_CHG_MASK;
+ 	val = flip_protnone_guard(oldval, val, PHYSICAL_PMD_PAGE_MASK);
++
++	if (pmd_dirty(pmd)) {
++		if (static_cpu_has(X86_FEATURE_SHSTK) && !(val & _PAGE_RW))
++			val |= _PAGE_DIRTY_SW;
++		else
++			val |= _PAGE_DIRTY_HW;
++	}
++
+ 	return __pmd(val);
+ }
+ 
+diff --git a/arch/x86/include/asm/pgtable_types.h b/arch/x86/include/asm/pgtable_types.h
+index 17735fa3ece8..cba3c9612e8b 100644
+--- a/arch/x86/include/asm/pgtable_types.h
++++ b/arch/x86/include/asm/pgtable_types.h
+@@ -145,8 +145,8 @@
+  * instance, and is *not* included in this mask since
+  * pte_modify() does modify it.
+  */
+-#define _PAGE_CHG_MASK	(PTE_PFN_MASK | _PAGE_PCD | _PAGE_PWT |		\
+-			 _PAGE_SPECIAL | _PAGE_ACCESSED | _PAGE_DIRTY_HW |	\
++#define _PAGE_CHG_MASK	(PTE_PFN_MASK | _PAGE_PCD | _PAGE_PWT |	\
++			 _PAGE_SPECIAL | _PAGE_ACCESSED |	\
+ 			 _PAGE_SOFT_DIRTY | _PAGE_DEVMAP)
+ #define _HPAGE_CHG_MASK (_PAGE_CHG_MASK | _PAGE_PSE)
+ 
+-- 
+2.26.0
+
diff --git a/0023-drm-i915-gvt-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_BITS.patch b/0023-drm-i915-gvt-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_BITS.patch
new file mode 100644
index 000000000..2aa71248a
--- /dev/null
+++ b/0023-drm-i915-gvt-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_BITS.patch
@@ -0,0 +1,30 @@
+From cad254f4a4a16e81dc0687185d71bb54be26b250 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 28 Aug 2018 13:01:49 -0700
+Subject: [PATCH 23/52] drm/i915/gvt: Change _PAGE_DIRTY to _PAGE_DIRTY_BITS
+
+After the introduction of _PAGE_DIRTY_SW, a dirty PTE can have either
+_PAGE_DIRTY_HW or _PAGE_DIRTY_SW.  Change _PAGE_DIRTY to _PAGE_DIRTY_BITS.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+---
+ drivers/gpu/drm/i915/gvt/gtt.c | 2 +-
+ 1 file changed, 1 insertion(+), 1 deletion(-)
+
+diff --git a/drivers/gpu/drm/i915/gvt/gtt.c b/drivers/gpu/drm/i915/gvt/gtt.c
+index 4a4828074cb7..854ba5c42d88 100644
+--- a/drivers/gpu/drm/i915/gvt/gtt.c
++++ b/drivers/gpu/drm/i915/gvt/gtt.c
+@@ -1201,7 +1201,7 @@ static int split_2MB_gtt_entry(struct intel_vgpu *vgpu,
+ 	}
+ 
+ 	/* Clear dirty field. */
+-	se->val64 &= ~_PAGE_DIRTY;
++	se->val64 &= ~_PAGE_DIRTY_BITS;
+ 
+ 	ops->clear_pse(se);
+ 	ops->clear_ips(se);
+-- 
+2.26.0
+
diff --git a/0024-x86-mm-Modify-ptep_set_wrprotect-and-pmdp_set_wrprot.patch b/0024-x86-mm-Modify-ptep_set_wrprotect-and-pmdp_set_wrprot.patch
new file mode 100644
index 000000000..4f227fb76
--- /dev/null
+++ b/0024-x86-mm-Modify-ptep_set_wrprotect-and-pmdp_set_wrprot.patch
@@ -0,0 +1,126 @@
+From af7e389de4381423b4b48b57a7b6e2a96e993720 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Fri, 15 Jun 2018 09:33:40 -0700
+Subject: [PATCH 24/52] x86/mm: Modify ptep_set_wrprotect and
+ pmdp_set_wrprotect for _PAGE_DIRTY_SW
+
+When Shadow Stack (SHSTK) is enabled, the [R/O + PAGE_DIRTY_HW] setting is
+reserved only for SHSTK.  Non-Shadow Stack R/O PTEs are
+[R/O + PAGE_DIRTY_SW].
+
+When a PTE goes from [R/W + PAGE_DIRTY_HW] to [R/O + PAGE_DIRTY_SW], it
+could become a transient SHSTK PTE in two cases.
+
+The first case is that some processors can start a write but end up seeing
+a read-only PTE by the time they get to the Dirty bit, creating a transient
+SHSTK PTE.  However, this will not occur on processors supporting SHSTK
+therefore we don't need a TLB flush here.
+
+The second case is that when the software, without atomic, tests & replaces
+PAGE_DIRTY_HW with PAGE_DIRTY_SW, a transient SHSTK PTE can exist.  This is
+prevented with cmpxchg.
+
+Dave Hansen, Jann Horn, Andy Lutomirski, and Peter Zijlstra provided many
+insights to the issue.  Jann Horn provided the cmpxchg solution.
+
+v9:
+- Change compile-time conditionals to runtime checks.
+- Fix parameters of try_cmpxchg(): change pte_t/pmd_t to
+  pte_t.pte/pmd_t.pmd.
+
+v4:
+- Implement try_cmpxchg().
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+---
+ arch/x86/include/asm/pgtable.h | 66 ++++++++++++++++++++++++++++++++++
+ 1 file changed, 66 insertions(+)
+
+diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
+index 11e64cfc8738..ad9de3046271 100644
+--- a/arch/x86/include/asm/pgtable.h
++++ b/arch/x86/include/asm/pgtable.h
+@@ -1258,6 +1258,39 @@ static inline pte_t ptep_get_and_clear_full(struct mm_struct *mm,
+ static inline void ptep_set_wrprotect(struct mm_struct *mm,
+ 				      unsigned long addr, pte_t *ptep)
+ {
++	/*
++	 * Some processors can start a write, but end up seeing a read-only
++	 * PTE by the time they get to the Dirty bit.  In this case, they
++	 * will set the Dirty bit, leaving a read-only, Dirty PTE which
++	 * looks like a Shadow Stack PTE.
++	 *
++	 * However, this behavior has been improved and will not occur on
++	 * processors supporting Shadow Stack.  Without this guarantee, a
++	 * transition to a non-present PTE and flush the TLB would be
++	 * needed.
++	 *
++	 * When changing a writable PTE to read-only and if the PTE has
++	 * _PAGE_DIRTY_HW set, we move that bit to _PAGE_DIRTY_SW so that
++	 * the PTE is not a valid Shadow Stack PTE.
++	 */
++#ifdef CONFIG_X86_64
++	if (static_cpu_has(X86_FEATURE_SHSTK)) {
++		pte_t new_pte, pte = READ_ONCE(*ptep);
++
++		do {
++			/*
++			 * This is the same as moving _PAGE_DIRTY_HW
++			 * to _PAGE_DIRTY_SW.
++			 */
++			new_pte = pte_wrprotect(pte);
++			new_pte.pte |= (new_pte.pte & _PAGE_DIRTY_HW) >>
++					_PAGE_BIT_DIRTY_HW << _PAGE_BIT_DIRTY_SW;
++			new_pte.pte &= ~_PAGE_DIRTY_HW;
++		} while (!try_cmpxchg(&ptep->pte, &pte.pte, new_pte.pte));
++
++		return;
++	}
++#endif
+ 	clear_bit(_PAGE_BIT_RW, (unsigned long *)&ptep->pte);
+ }
+ 
+@@ -1308,6 +1341,39 @@ static inline pud_t pudp_huge_get_and_clear(struct mm_struct *mm,
+ static inline void pmdp_set_wrprotect(struct mm_struct *mm,
+ 				      unsigned long addr, pmd_t *pmdp)
+ {
++	/*
++	 * Some processors can start a write, but end up seeing a read-only
++	 * PMD by the time they get to the Dirty bit.  In this case, they
++	 * will set the Dirty bit, leaving a read-only, Dirty PMD which
++	 * looks like a Shadow Stack PMD.
++	 *
++	 * However, this behavior has been improved and will not occur on
++	 * processors supporting Shadow Stack.  Without this guarantee, a
++	 * transition to a non-present PMD and flush the TLB would be
++	 * needed.
++	 *
++	 * When changing a writable PMD to read-only and if the PMD has
++	 * _PAGE_DIRTY_HW set, we move that bit to _PAGE_DIRTY_SW so that
++	 * the PMD is not a valid Shadow Stack PMD.
++	 */
++#ifdef CONFIG_X86_64
++	if (static_cpu_has(X86_FEATURE_SHSTK)) {
++		pmd_t new_pmd, pmd = READ_ONCE(*pmdp);
++
++		do {
++			/*
++			 * This is the same as moving _PAGE_DIRTY_HW
++			 * to _PAGE_DIRTY_SW.
++			 */
++			new_pmd = pmd_wrprotect(pmd);
++			new_pmd.pmd |= (new_pmd.pmd & _PAGE_DIRTY_HW) >>
++					_PAGE_BIT_DIRTY_HW << _PAGE_BIT_DIRTY_SW;
++			new_pmd.pmd &= ~_PAGE_DIRTY_HW;
++		} while (!try_cmpxchg(&pmdp->pmd, &pmd.pmd, new_pmd.pmd));
++
++		return;
++	}
++#endif
+ 	clear_bit(_PAGE_BIT_RW, (unsigned long *)pmdp);
+ }
+ 
+-- 
+2.26.0
+
diff --git a/0025-x86-mm-Shadow-Stack-page-fault-error-checking.patch b/0025-x86-mm-Shadow-Stack-page-fault-error-checking.patch
new file mode 100644
index 000000000..0f176a0cb
--- /dev/null
+++ b/0025-x86-mm-Shadow-Stack-page-fault-error-checking.patch
@@ -0,0 +1,79 @@
+From 1aa70cb6d8254afe46cd4afe59933606c1f5f562 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 28 Nov 2017 13:01:18 -0800
+Subject: [PATCH 25/52] x86/mm: Shadow Stack page fault error checking
+
+If a page fault is triggered by a Shadow Stack (SHSTK) access
+(e.g. CALL/RET) or SHSTK management instructions (e.g. WRUSSQ), then bit[6]
+of the page fault error code is set.
+
+In access_error(), verify a SHSTK page fault is within a SHSTK memory area.
+It is always an error otherwise.
+
+For a valid SHSTK access, set FAULT_FLAG_WRITE to effect copy-on-write.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+---
+ arch/x86/include/asm/traps.h |  2 ++
+ arch/x86/mm/fault.c          | 18 ++++++++++++++++++
+ 2 files changed, 20 insertions(+)
+
+diff --git a/arch/x86/include/asm/traps.h b/arch/x86/include/asm/traps.h
+index 7ac26bbd0bef..8023d177fcd8 100644
+--- a/arch/x86/include/asm/traps.h
++++ b/arch/x86/include/asm/traps.h
+@@ -169,6 +169,7 @@ enum {
+  *   bit 3 ==				1: use of reserved bit detected
+  *   bit 4 ==				1: fault was an instruction fetch
+  *   bit 5 ==				1: protection keys block access
++ *   bit 6 ==				1: shadow stack access fault
+  */
+ enum x86_pf_error_code {
+ 	X86_PF_PROT	=		1 << 0,
+@@ -177,5 +178,6 @@ enum x86_pf_error_code {
+ 	X86_PF_RSVD	=		1 << 3,
+ 	X86_PF_INSTR	=		1 << 4,
+ 	X86_PF_PK	=		1 << 5,
++	X86_PF_SHSTK	=		1 << 6,
+ };
+ #endif /* _ASM_X86_TRAPS_H */
+diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
+index 629fdf13f846..d38bf584367a 100644
+--- a/arch/x86/mm/fault.c
++++ b/arch/x86/mm/fault.c
+@@ -1210,6 +1210,17 @@ access_error(unsigned long error_code, struct vm_area_struct *vma)
+ 				       (error_code & X86_PF_INSTR), foreign))
+ 		return 1;
+ 
++	/*
++	 * Verify X86_PF_SHSTK is within a Shadow Stack VMA.
++	 * It is always an error if there is a Shadow Stack
++	 * fault outside a Shadow Stack VMA.
++	 */
++	if (error_code & X86_PF_SHSTK) {
++		if (!(vma->vm_flags & VM_SHSTK))
++			return 1;
++		return 0;
++	}
++
+ 	if (error_code & X86_PF_WRITE) {
+ 		/* write, present and write, not present: */
+ 		if (unlikely(!(vma->vm_flags & VM_WRITE)))
+@@ -1367,6 +1378,13 @@ void do_user_addr_fault(struct pt_regs *regs,
+ 
+ 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
+ 
++	/*
++	 * If the fault is caused by a Shadow Stack access,
++	 * i.e. CALL/RET/SAVEPREVSSP/RSTORSSP, then set
++	 * FAULT_FLAG_WRITE to effect copy-on-write.
++	 */
++	if (hw_error_code & X86_PF_SHSTK)
++		flags |= FAULT_FLAG_WRITE;
+ 	if (hw_error_code & X86_PF_WRITE)
+ 		flags |= FAULT_FLAG_WRITE;
+ 	if (hw_error_code & X86_PF_INSTR)
+-- 
+2.26.0
+
diff --git a/0026-mm-Handle-Shadow-Stack-page-fault.patch b/0026-mm-Handle-Shadow-Stack-page-fault.patch
new file mode 100644
index 000000000..b66e72014
--- /dev/null
+++ b/0026-mm-Handle-Shadow-Stack-page-fault.patch
@@ -0,0 +1,121 @@
+From 179b1a91385947f8fce765fb4d264a4d006571ac Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 28 Nov 2017 13:49:07 -0800
+Subject: [PATCH 26/52] mm: Handle Shadow Stack page fault
+
+When a task does fork(), its Shadow Stack (SHSTK) must be duplicated for
+the child.  This patch implements a flow similar to copy-on-write of an
+anonymous page, but for SHSTK.
+
+A SHSTK PTE must be RO and Dirty.  This Dirty bit requirement is used to
+effect the copying.  In copy_one_pte(), clear the Dirty bit from a SHSTK
+PTE to cause a page fault upon the next SHSTK access.  At that time, fix
+the PTE and copy/re-use the page.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/mm/pgtable.c         | 15 +++++++++++++++
+ include/asm-generic/pgtable.h | 17 +++++++++++++++++
+ mm/memory.c                   |  7 ++++++-
+ 3 files changed, 38 insertions(+), 1 deletion(-)
+
+diff --git a/arch/x86/mm/pgtable.c b/arch/x86/mm/pgtable.c
+index 7bd2c3a52297..2eb33794c08d 100644
+--- a/arch/x86/mm/pgtable.c
++++ b/arch/x86/mm/pgtable.c
+@@ -872,3 +872,18 @@ int pmd_free_pte_page(pmd_t *pmd, unsigned long addr)
+ 
+ #endif /* CONFIG_X86_64 */
+ #endif	/* CONFIG_HAVE_ARCH_HUGE_VMAP */
++
++#ifdef CONFIG_X86_INTEL_SHADOW_STACK_USER
++inline bool arch_copy_pte_mapping(vm_flags_t vm_flags)
++{
++	return (vm_flags & VM_SHSTK);
++}
++
++inline pte_t pte_set_vma_features(pte_t pte, struct vm_area_struct *vma)
++{
++	if (vma->vm_flags & VM_SHSTK)
++		return pte_mkdirty_shstk(pte);
++	else
++		return pte;
++}
++#endif /* CONFIG_X86_INTEL_SHADOW_STACK_USER */
+diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h
+index e2e2bef07dd2..c429580fba3c 100644
+--- a/include/asm-generic/pgtable.h
++++ b/include/asm-generic/pgtable.h
+@@ -1190,6 +1190,23 @@ static inline bool arch_has_pfn_modify_check(void)
+ }
+ #endif /* !_HAVE_ARCH_PFN_MODIFY_ALLOWED */
+ 
++#ifdef CONFIG_MMU
++#ifndef CONFIG_ARCH_HAS_SHSTK
++static inline bool arch_copy_pte_mapping(vm_flags_t vm_flags)
++{
++	return false;
++}
++
++static inline pte_t pte_set_vma_features(pte_t pte, struct vm_area_struct *vma)
++{
++	return pte;
++}
++#else
++bool arch_copy_pte_mapping(vm_flags_t vm_flags);
++pte_t pte_set_vma_features(pte_t pte, struct vm_area_struct *vma);
++#endif
++#endif /* CONFIG_MMU */
++
+ /*
+  * Architecture PAGE_KERNEL_* fallbacks
+  *
+diff --git a/mm/memory.c b/mm/memory.c
+index e8bfdf0d9d1d..b4234d2bca60 100644
+--- a/mm/memory.c
++++ b/mm/memory.c
+@@ -772,7 +772,8 @@ copy_one_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,
+ 	 * If it's a COW mapping, write protect it both
+ 	 * in the parent and the child
+ 	 */
+-	if (is_cow_mapping(vm_flags) && pte_write(pte)) {
++	if ((is_cow_mapping(vm_flags) && pte_write(pte)) ||
++	    arch_copy_pte_mapping(vm_flags)) {
+ 		ptep_set_wrprotect(src_mm, addr, src_pte);
+ 		pte = pte_wrprotect(pte);
+ 	}
+@@ -2472,6 +2473,7 @@ static inline void wp_page_reuse(struct vm_fault *vmf)
+ 	flush_cache_page(vma, vmf->address, pte_pfn(vmf->orig_pte));
+ 	entry = pte_mkyoung(vmf->orig_pte);
+ 	entry = maybe_mkwrite(pte_mkdirty(entry), vma);
++	entry = pte_set_vma_features(entry, vma);
+ 	if (ptep_set_access_flags(vma, vmf->address, vmf->pte, entry, 1))
+ 		update_mmu_cache(vma, vmf->address, vmf->pte);
+ 	pte_unmap_unlock(vmf->pte, vmf->ptl);
+@@ -2559,6 +2561,7 @@ static vm_fault_t wp_page_copy(struct vm_fault *vmf)
+ 		flush_cache_page(vma, vmf->address, pte_pfn(vmf->orig_pte));
+ 		entry = mk_pte(new_page, vma->vm_page_prot);
+ 		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
++		entry = pte_set_vma_features(entry, vma);
+ 		/*
+ 		 * Clear the pte entry and flush it first, before updating the
+ 		 * pte with the new entry. This will avoid a race condition
+@@ -3078,6 +3081,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
+ 	pte = mk_pte(page, vma->vm_page_prot);
+ 	if ((vmf->flags & FAULT_FLAG_WRITE) && reuse_swap_page(page, NULL)) {
+ 		pte = maybe_mkwrite(pte_mkdirty(pte), vma);
++		pte = pte_set_vma_features(pte, vma);
+ 		vmf->flags &= ~FAULT_FLAG_WRITE;
+ 		ret |= VM_FAULT_WRITE;
+ 		exclusive = RMAP_EXCLUSIVE;
+@@ -3220,6 +3224,7 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
+ 	entry = mk_pte(page, vma->vm_page_prot);
+ 	if (vma->vm_flags & VM_WRITE)
+ 		entry = pte_mkwrite(pte_mkdirty(entry));
++	entry = pte_set_vma_features(entry, vma);
+ 
+ 	vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,
+ 			&vmf->ptl);
+-- 
+2.26.0
+
diff --git a/0027-mm-Handle-THP-HugeTLB-Shadow-Stack-page-fault.patch b/0027-mm-Handle-THP-HugeTLB-Shadow-Stack-page-fault.patch
new file mode 100644
index 000000000..80f6ce85a
--- /dev/null
+++ b/0027-mm-Handle-THP-HugeTLB-Shadow-Stack-page-fault.patch
@@ -0,0 +1,100 @@
+From a82c10b7bfab1ba06c0edccfc6539619bfb6f2d0 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Sat, 3 Feb 2018 21:12:05 -0800
+Subject: [PATCH 27/52] mm: Handle THP/HugeTLB Shadow Stack page fault
+
+This patch implements THP Shadow Stack (SHSTK) copying in the same way as
+in the previous patch for regular PTE.
+
+In copy_huge_pmd(), clear the dirty bit from the PMD to cause a page fault
+upon the next SHSTK access to the PMD.  At that time, fix the PMD and
+copy/re-use the page.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/mm/pgtable.c         |  8 ++++++++
+ include/asm-generic/pgtable.h | 11 +++++++++++
+ mm/huge_memory.c              |  4 ++++
+ 3 files changed, 23 insertions(+)
+
+diff --git a/arch/x86/mm/pgtable.c b/arch/x86/mm/pgtable.c
+index 2eb33794c08d..3340b1d4e9da 100644
+--- a/arch/x86/mm/pgtable.c
++++ b/arch/x86/mm/pgtable.c
+@@ -886,4 +886,12 @@ inline pte_t pte_set_vma_features(pte_t pte, struct vm_area_struct *vma)
+ 	else
+ 		return pte;
+ }
++
++inline pmd_t pmd_set_vma_features(pmd_t pmd, struct vm_area_struct *vma)
++{
++	if (vma->vm_flags & VM_SHSTK)
++		return pmd_mkdirty_shstk(pmd);
++	else
++		return pmd;
++}
+ #endif /* CONFIG_X86_INTEL_SHADOW_STACK_USER */
+diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h
+index c429580fba3c..23ee561cbb96 100644
+--- a/include/asm-generic/pgtable.h
++++ b/include/asm-generic/pgtable.h
+@@ -1201,9 +1201,20 @@ static inline pte_t pte_set_vma_features(pte_t pte, struct vm_area_struct *vma)
+ {
+ 	return pte;
+ }
++
++#ifdef CONFIG_TRANSPARENT_HUGEPAGE
++static inline pmd_t pmd_set_vma_features(pmd_t pmd, struct vm_area_struct *vma)
++{
++	return pmd;
++}
++#endif
+ #else
+ bool arch_copy_pte_mapping(vm_flags_t vm_flags);
+ pte_t pte_set_vma_features(pte_t pte, struct vm_area_struct *vma);
++
++#ifdef CONFIG_TRANSPARENT_HUGEPAGE
++pmd_t pmd_set_vma_features(pmd_t pmd, struct vm_area_struct *vma);
++#endif
+ #endif
+ #endif /* CONFIG_MMU */
+ 
+diff --git a/mm/huge_memory.c b/mm/huge_memory.c
+index 24ad53b4dfc0..b5476bd35284 100644
+--- a/mm/huge_memory.c
++++ b/mm/huge_memory.c
+@@ -639,6 +639,7 @@ static vm_fault_t __do_huge_pmd_anonymous_page(struct vm_fault *vmf,
+ 
+ 		entry = mk_huge_pmd(page, vma->vm_page_prot);
+ 		entry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);
++		entry = pmd_set_vma_features(entry, vma);
+ 		page_add_new_anon_rmap(page, vma, haddr, true);
+ 		mem_cgroup_commit_charge(page, memcg, false, true);
+ 		lru_cache_add_active_or_unevictable(page, vma);
+@@ -1281,6 +1282,7 @@ static vm_fault_t do_huge_pmd_wp_page_fallback(struct vm_fault *vmf,
+ 		pte_t entry;
+ 		entry = mk_pte(pages[i], vma->vm_page_prot);
+ 		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
++		entry = pte_set_vma_features(entry, vma);
+ 		memcg = (void *)page_private(pages[i]);
+ 		set_page_private(pages[i], 0);
+ 		page_add_new_anon_rmap(pages[i], vmf->vma, haddr, false);
+@@ -1363,6 +1365,7 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd)
+ 		pmd_t entry;
+ 		entry = pmd_mkyoung(orig_pmd);
+ 		entry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);
++		entry = pmd_set_vma_features(entry, vma);
+ 		if (pmdp_set_access_flags(vma, haddr, vmf->pmd, entry,  1))
+ 			update_mmu_cache_pmd(vma, vmf->address, vmf->pmd);
+ 		ret |= VM_FAULT_WRITE;
+@@ -1435,6 +1438,7 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd)
+ 		pmd_t entry;
+ 		entry = mk_huge_pmd(new_page, vma->vm_page_prot);
+ 		entry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);
++		entry = pmd_set_vma_features(entry, vma);
+ 		pmdp_huge_clear_flush_notify(vma, haddr, vmf->pmd);
+ 		page_add_new_anon_rmap(new_page, vma, haddr, true);
+ 		mem_cgroup_commit_charge(new_page, memcg, false, true);
+-- 
+2.26.0
+
diff --git a/0028-mm-Update-can_follow_write_pte-for-Shadow-Stack.patch b/0028-mm-Update-can_follow_write_pte-for-Shadow-Stack.patch
new file mode 100644
index 000000000..bd6ef9ce7
--- /dev/null
+++ b/0028-mm-Update-can_follow_write_pte-for-Shadow-Stack.patch
@@ -0,0 +1,158 @@
+From aede40567e1b51fd32c1e56c2ae81bf85ebe1904 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 3 Jul 2018 13:07:12 -0700
+Subject: [PATCH 28/52] mm: Update can_follow_write_pte() for Shadow Stack
+
+Can_follow_write_pte() verifies that a read-only page is the task's own
+copy by ensuring the page has gone through faultin_page() and the PTE is
+Dirty.
+
+A Shadow Stack (SHSTK) PTE must be (read-only + _PAGE_DIRTY_HW).  When a
+task does fork(), its SHSTK PTEs become (read-only + _PAGE_DIRTY_SW).  This
+causes the next SHSTK access (i.e. CALL, RET, INCSSP) to trigger a fault;
+the page is then copied, and (read-only + _PAGE_DIRTY_HW) is restored.
+
+To update can_follow_write_pte() for SHSTK, introduce pte_exclusive().  It
+verifies a data PTE is Dirty and a SHSTK PTE has _PAGE_DIRTY_HW.
+
+Also rename can_follow_write_pte() to can_follow_write() to make its
+meaning clear; i.e. "Can we write to the page?", not "Is the PTE writable?"
+
+Also apply same changes to the huge memory case.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/mm/pgtable.c         | 18 ++++++++++++++++++
+ include/asm-generic/pgtable.h | 12 ++++++++++++
+ mm/gup.c                      |  8 +++++---
+ mm/huge_memory.c              |  8 +++++---
+ 4 files changed, 40 insertions(+), 6 deletions(-)
+
+diff --git a/arch/x86/mm/pgtable.c b/arch/x86/mm/pgtable.c
+index 3340b1d4e9da..fa8133f37918 100644
+--- a/arch/x86/mm/pgtable.c
++++ b/arch/x86/mm/pgtable.c
+@@ -887,6 +887,15 @@ inline pte_t pte_set_vma_features(pte_t pte, struct vm_area_struct *vma)
+ 		return pte;
+ }
+ 
++inline bool pte_exclusive(pte_t pte, struct vm_area_struct *vma)
++{
++	if (vma->vm_flags & VM_SHSTK)
++		return pte_dirty_hw(pte);
++	else
++		return pte_dirty(pte);
++}
++
++#ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ inline pmd_t pmd_set_vma_features(pmd_t pmd, struct vm_area_struct *vma)
+ {
+ 	if (vma->vm_flags & VM_SHSTK)
+@@ -894,4 +903,13 @@ inline pmd_t pmd_set_vma_features(pmd_t pmd, struct vm_area_struct *vma)
+ 	else
+ 		return pmd;
+ }
++
++inline bool pmd_exclusive(pmd_t pmd, struct vm_area_struct *vma)
++{
++	if (vma->vm_flags & VM_SHSTK)
++		return pmd_dirty_hw(pmd);
++	else
++		return pmd_dirty(pmd);
++}
++#endif /* CONFIG_TRANSPARENT_HUGEPAGE */
+ #endif /* CONFIG_X86_INTEL_SHADOW_STACK_USER */
+diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h
+index 23ee561cbb96..31a3c7dfae20 100644
+--- a/include/asm-generic/pgtable.h
++++ b/include/asm-generic/pgtable.h
+@@ -1202,18 +1202,30 @@ static inline pte_t pte_set_vma_features(pte_t pte, struct vm_area_struct *vma)
+ 	return pte;
+ }
+ 
++static inline bool pte_exclusive(pte_t pte, struct vm_area_struct *vma)
++{
++	return pte_dirty(pte);
++}
++
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ static inline pmd_t pmd_set_vma_features(pmd_t pmd, struct vm_area_struct *vma)
+ {
+ 	return pmd;
+ }
++
++static inline bool pmd_exclusive(pmd_t pmd, struct vm_area_struct *vma)
++{
++	return pmd_dirty(pmd);
++}
+ #endif
+ #else
+ bool arch_copy_pte_mapping(vm_flags_t vm_flags);
+ pte_t pte_set_vma_features(pte_t pte, struct vm_area_struct *vma);
++bool pte_exclusive(pte_t pte, struct vm_area_struct *vma);
+ 
+ #ifdef CONFIG_TRANSPARENT_HUGEPAGE
+ pmd_t pmd_set_vma_features(pmd_t pmd, struct vm_area_struct *vma);
++bool pmd_exclusive(pmd_t pmd, struct vm_area_struct *vma);
+ #endif
+ #endif
+ #endif /* CONFIG_MMU */
+diff --git a/mm/gup.c b/mm/gup.c
+index 1b521e0ac1de..863e9540b21c 100644
+--- a/mm/gup.c
++++ b/mm/gup.c
+@@ -179,10 +179,12 @@ static int follow_pfn_pte(struct vm_area_struct *vma, unsigned long address,
+  * FOLL_FORCE can write to even unwritable pte's, but only
+  * after we've gone through a COW cycle and they are dirty.
+  */
+-static inline bool can_follow_write_pte(pte_t pte, unsigned int flags)
++static inline bool can_follow_write(pte_t pte, unsigned int flags,
++				    struct vm_area_struct *vma)
+ {
+ 	return pte_write(pte) ||
+-		((flags & FOLL_FORCE) && (flags & FOLL_COW) && pte_dirty(pte));
++		((flags & FOLL_FORCE) && (flags & FOLL_COW) &&
++		 pte_exclusive(pte, vma));
+ }
+ 
+ static struct page *follow_page_pte(struct vm_area_struct *vma,
+@@ -224,7 +226,7 @@ static struct page *follow_page_pte(struct vm_area_struct *vma,
+ 	}
+ 	if ((flags & FOLL_NUMA) && pte_protnone(pte))
+ 		goto no_page;
+-	if ((flags & FOLL_WRITE) && !can_follow_write_pte(pte, flags)) {
++	if ((flags & FOLL_WRITE) && !can_follow_write(pte, flags, vma)) {
+ 		pte_unmap_unlock(ptep, ptl);
+ 		return NULL;
+ 	}
+diff --git a/mm/huge_memory.c b/mm/huge_memory.c
+index b5476bd35284..ae03a8e18fc6 100644
+--- a/mm/huge_memory.c
++++ b/mm/huge_memory.c
+@@ -1472,10 +1472,12 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd)
+  * FOLL_FORCE can write to even unwritable pmd's, but only
+  * after we've gone through a COW cycle and they are dirty.
+  */
+-static inline bool can_follow_write_pmd(pmd_t pmd, unsigned int flags)
++static inline bool can_follow_write(pmd_t pmd, unsigned int flags,
++				    struct vm_area_struct *vma)
+ {
+ 	return pmd_write(pmd) ||
+-	       ((flags & FOLL_FORCE) && (flags & FOLL_COW) && pmd_dirty(pmd));
++	       ((flags & FOLL_FORCE) && (flags & FOLL_COW) &&
++		pmd_exclusive(pmd, vma));
+ }
+ 
+ struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,
+@@ -1488,7 +1490,7 @@ struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,
+ 
+ 	assert_spin_locked(pmd_lockptr(mm, pmd));
+ 
+-	if (flags & FOLL_WRITE && !can_follow_write_pmd(*pmd, flags))
++	if (flags & FOLL_WRITE && !can_follow_write(*pmd, flags, vma))
+ 		goto out;
+ 
+ 	/* Avoid dumping huge zero page */
+-- 
+2.26.0
+
diff --git a/0029-x86-cet-shstk-User-mode-Shadow-Stack-support.patch b/0029-x86-cet-shstk-User-mode-Shadow-Stack-support.patch
new file mode 100644
index 000000000..462c05fde
--- /dev/null
+++ b/0029-x86-cet-shstk-User-mode-Shadow-Stack-support.patch
@@ -0,0 +1,356 @@
+From 46e70cc99e2b969a1bf18745e55da1fd38bc2e03 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Thu, 22 Aug 2019 10:06:11 -0700
+Subject: [PATCH 29/52] x86/cet/shstk: User-mode Shadow Stack support
+
+This patch adds basic shadow stack  enabling/disabling routines.  A task's
+shadow stack is allocated from memory with VM_SHSTK flag and read-only
+protection.  It has a fixed size of min(RLIMIT_STACK, 4GB).
+
+v10:
+- Change no_cet_shstk to no_user_shstk.
+- Limit shadow stack size to 4 GB.
+
+v9:
+- Change cpu_feature_enabled() to static_cpu_has().
+- Merge cet_disable_shstk to cet_disable_free_shstk.
+- Remove the empty slot at the top of the shadow stack, as it is not needed.
+- Move do_mmap_locked() to alloc_shstk(), which is a static function.
+
+v6:
+- Create a function do_mmap_locked() for shadow stack allocation.
+
+v2:
+- Change noshstk to no_cet_shstk.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/include/asm/cet.h                    |  31 +++++
+ arch/x86/include/asm/disabled-features.h      |   8 +-
+ arch/x86/include/asm/processor.h              |   5 +
+ arch/x86/kernel/Makefile                      |   2 +
+ arch/x86/kernel/cet.c                         | 121 ++++++++++++++++++
+ arch/x86/kernel/cpu/common.c                  |  25 ++++
+ arch/x86/kernel/process.c                     |   1 +
+ .../arch/x86/include/asm/disabled-features.h  |   8 +-
+ 8 files changed, 199 insertions(+), 2 deletions(-)
+ create mode 100644 arch/x86/include/asm/cet.h
+ create mode 100644 arch/x86/kernel/cet.c
+
+diff --git a/arch/x86/include/asm/cet.h b/arch/x86/include/asm/cet.h
+new file mode 100644
+index 000000000000..c44c991ca91f
+--- /dev/null
++++ b/arch/x86/include/asm/cet.h
+@@ -0,0 +1,31 @@
++/* SPDX-License-Identifier: GPL-2.0 */
++#ifndef _ASM_X86_CET_H
++#define _ASM_X86_CET_H
++
++#ifndef __ASSEMBLY__
++#include <linux/types.h>
++
++struct task_struct;
++/*
++ * Per-thread CET status
++ */
++struct cet_status {
++	unsigned long	shstk_base;
++	unsigned long	shstk_size;
++	unsigned int	shstk_enabled:1;
++};
++
++#ifdef CONFIG_X86_INTEL_CET
++int cet_setup_shstk(void);
++void cet_disable_free_shstk(struct task_struct *p);
++#else
++static inline void cet_disable_free_shstk(struct task_struct *p) {}
++#endif
++
++#define cpu_x86_cet_enabled() \
++	(static_cpu_has(X86_FEATURE_SHSTK) || \
++	 static_cpu_has(X86_FEATURE_IBT))
++
++#endif /* __ASSEMBLY__ */
++
++#endif /* _ASM_X86_CET_H */
+diff --git a/arch/x86/include/asm/disabled-features.h b/arch/x86/include/asm/disabled-features.h
+index 4ea8584682f9..a0e1b24cfa02 100644
+--- a/arch/x86/include/asm/disabled-features.h
++++ b/arch/x86/include/asm/disabled-features.h
+@@ -56,6 +56,12 @@
+ # define DISABLE_PTI		(1 << (X86_FEATURE_PTI & 31))
+ #endif
+ 
++#ifdef CONFIG_X86_INTEL_SHADOW_STACK_USER
++#define DISABLE_SHSTK	0
++#else
++#define DISABLE_SHSTK	(1<<(X86_FEATURE_SHSTK & 31))
++#endif
++
+ /*
+  * Make sure to add features to the correct mask
+  */
+@@ -75,7 +81,7 @@
+ #define DISABLED_MASK13	0
+ #define DISABLED_MASK14	0
+ #define DISABLED_MASK15	0
+-#define DISABLED_MASK16	(DISABLE_PKU|DISABLE_OSPKE|DISABLE_LA57|DISABLE_UMIP)
++#define DISABLED_MASK16	(DISABLE_PKU|DISABLE_OSPKE|DISABLE_LA57|DISABLE_UMIP|DISABLE_SHSTK)
+ #define DISABLED_MASK17	0
+ #define DISABLED_MASK18	0
+ #define DISABLED_MASK_CHECK BUILD_BUG_ON_ZERO(NCAPINTS != 19)
+diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
+index 2db6242367a7..e4e7eaddd12d 100644
+--- a/arch/x86/include/asm/processor.h
++++ b/arch/x86/include/asm/processor.h
+@@ -26,6 +26,7 @@ struct vm86;
+ #include <asm/fpu/types.h>
+ #include <asm/unwind_hints.h>
+ #include <asm/vmxfeatures.h>
++#include <asm/cet.h>
+ 
+ #include <linux/personality.h>
+ #include <linux/cache.h>
+@@ -543,6 +544,10 @@ struct thread_struct {
+ 	unsigned int		sig_on_uaccess_err:1;
+ 	unsigned int		uaccess_err:1;	/* uaccess failed */
+ 
++#ifdef CONFIG_X86_INTEL_CET
++	struct cet_status	cet;
++#endif
++
+ 	/* Floating point and extended processor state */
+ 	struct fpu		fpu;
+ 	/*
+diff --git a/arch/x86/kernel/Makefile b/arch/x86/kernel/Makefile
+index 9b294c13809a..b83601e6b9ed 100644
+--- a/arch/x86/kernel/Makefile
++++ b/arch/x86/kernel/Makefile
+@@ -143,6 +143,8 @@ obj-$(CONFIG_UNWINDER_ORC)		+= unwind_orc.o
+ obj-$(CONFIG_UNWINDER_FRAME_POINTER)	+= unwind_frame.o
+ obj-$(CONFIG_UNWINDER_GUESS)		+= unwind_guess.o
+ 
++obj-$(CONFIG_X86_INTEL_CET)		+= cet.o
++
+ ###
+ # 64 bit specific files
+ ifeq ($(CONFIG_X86_64),y)
+diff --git a/arch/x86/kernel/cet.c b/arch/x86/kernel/cet.c
+new file mode 100644
+index 000000000000..edf386e45c6d
+--- /dev/null
++++ b/arch/x86/kernel/cet.c
+@@ -0,0 +1,121 @@
++/* SPDX-License-Identifier: GPL-2.0 */
++/*
++ * cet.c - Control-flow Enforcement (CET)
++ *
++ * Copyright (c) 2019, Intel Corporation.
++ * Yu-cheng Yu <yu-cheng.yu@intel.com>
++ */
++
++#include <linux/types.h>
++#include <linux/mm.h>
++#include <linux/mman.h>
++#include <linux/slab.h>
++#include <linux/uaccess.h>
++#include <linux/sched/signal.h>
++#include <linux/compat.h>
++#include <asm/msr.h>
++#include <asm/user.h>
++#include <asm/fpu/internal.h>
++#include <asm/fpu/xstate.h>
++#include <asm/fpu/types.h>
++#include <asm/cet.h>
++
++static void start_update_msrs(void)
++{
++	fpregs_lock();
++	if (test_thread_flag(TIF_NEED_FPU_LOAD))
++		__fpregs_load_activate();
++}
++
++static void end_update_msrs(void)
++{
++	fpregs_unlock();
++}
++
++static unsigned long cet_get_shstk_addr(void)
++{
++	struct fpu *fpu = &current->thread.fpu;
++	unsigned long ssp = 0;
++
++	fpregs_lock();
++
++	if (fpregs_state_valid(fpu, smp_processor_id())) {
++		rdmsrl(MSR_IA32_PL3_SSP, ssp);
++	} else {
++		struct cet_user_state *p;
++
++		p = get_xsave_addr(&fpu->state.xsave, XFEATURE_CET_USER);
++		if (p)
++			ssp = p->user_ssp;
++	}
++
++	fpregs_unlock();
++	return ssp;
++}
++
++static unsigned long alloc_shstk(unsigned long size)
++{
++	struct mm_struct *mm = current->mm;
++	unsigned long addr, populate;
++
++	down_write(&mm->mmap_sem);
++	addr = do_mmap(NULL, 0, size, PROT_READ, MAP_ANONYMOUS | MAP_PRIVATE,
++		       VM_SHSTK, 0, &populate, NULL);
++	up_write(&mm->mmap_sem);
++
++	if (populate)
++		mm_populate(addr, populate);
++
++	return addr;
++}
++
++int cet_setup_shstk(void)
++{
++	unsigned long addr, size;
++	struct cet_status *cet = &current->thread.cet;
++
++	if (!static_cpu_has(X86_FEATURE_SHSTK))
++		return -EOPNOTSUPP;
++
++	size = min(rlimit(RLIMIT_STACK), 1UL << 32);
++	addr = alloc_shstk(size);
++
++	if (IS_ERR((void *)addr))
++		return PTR_ERR((void *)addr);
++
++	cet->shstk_base = addr;
++	cet->shstk_size = size;
++	cet->shstk_enabled = 1;
++
++	start_update_msrs();
++	wrmsrl(MSR_IA32_PL3_SSP, addr + size);
++	wrmsrl(MSR_IA32_U_CET, MSR_IA32_CET_SHSTK_EN);
++	end_update_msrs();
++	return 0;
++}
++
++void cet_disable_free_shstk(struct task_struct *tsk)
++{
++	struct cet_status *cet = &tsk->thread.cet;
++
++	if (!static_cpu_has(X86_FEATURE_SHSTK) ||
++	    !cet->shstk_enabled || !cet->shstk_base)
++		return;
++
++	if (!tsk->mm || (tsk->mm != current->mm))
++		return;
++
++	if (tsk == current) {
++		u64 msr_val;
++
++		start_update_msrs();
++		rdmsrl(MSR_IA32_U_CET, msr_val);
++		wrmsrl(MSR_IA32_U_CET, msr_val & ~MSR_IA32_CET_SHSTK_EN);
++		end_update_msrs();
++	}
++
++	vm_munmap(cet->shstk_base, cet->shstk_size);
++	cet->shstk_base = 0;
++	cet->shstk_size = 0;
++	cet->shstk_enabled = 0;
++}
+diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
+index 4cdb123ff66a..355eb6d1a6c0 100644
+--- a/arch/x86/kernel/cpu/common.c
++++ b/arch/x86/kernel/cpu/common.c
+@@ -55,6 +55,7 @@
+ #include <asm/microcode_intel.h>
+ #include <asm/intel-family.h>
+ #include <asm/cpu_device_id.h>
++#include <asm/cet.h>
+ #include <asm/uv/uv.h>
+ 
+ #include "cpu.h"
+@@ -469,6 +470,29 @@ static __init int setup_disable_pku(char *arg)
+ __setup("nopku", setup_disable_pku);
+ #endif /* CONFIG_X86_64 */
+ 
++static __always_inline void setup_cet(struct cpuinfo_x86 *c)
++{
++	if (cpu_x86_cet_enabled())
++		cr4_set_bits(X86_CR4_CET);
++}
++
++#ifdef CONFIG_X86_INTEL_SHADOW_STACK_USER
++static __init int setup_disable_shstk(char *s)
++{
++	/* require an exact match without trailing characters */
++	if (s[0] != '\0')
++		return 0;
++
++	if (!boot_cpu_has(X86_FEATURE_SHSTK))
++		return 1;
++
++	setup_clear_cpu_cap(X86_FEATURE_SHSTK);
++	pr_info("x86: 'no_user_shstk' specified, disabling user Shadow Stack\n");
++	return 1;
++}
++__setup("no_user_shstk", setup_disable_shstk);
++#endif
++
+ /*
+  * Some CPU features depend on higher CPUID levels, which may not always
+  * be available due to CPUID level capping or broken virtualization
+@@ -1503,6 +1527,7 @@ static void identify_cpu(struct cpuinfo_x86 *c)
+ 	x86_init_rdrand(c);
+ 	x86_init_cache_qos(c);
+ 	setup_pku(c);
++	setup_cet(c);
+ 
+ 	/*
+ 	 * Clear/Set all flags overridden by options, need do it
+diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
+index 87de18c64cf5..e478a0e00aaa 100644
+--- a/arch/x86/kernel/process.c
++++ b/arch/x86/kernel/process.c
+@@ -43,6 +43,7 @@
+ #include <asm/spec-ctrl.h>
+ #include <asm/io_bitmap.h>
+ #include <asm/proto.h>
++#include <asm/cet.h>
+ 
+ #include "process.h"
+ 
+diff --git a/tools/arch/x86/include/asm/disabled-features.h b/tools/arch/x86/include/asm/disabled-features.h
+index 4ea8584682f9..a0e1b24cfa02 100644
+--- a/tools/arch/x86/include/asm/disabled-features.h
++++ b/tools/arch/x86/include/asm/disabled-features.h
+@@ -56,6 +56,12 @@
+ # define DISABLE_PTI		(1 << (X86_FEATURE_PTI & 31))
+ #endif
+ 
++#ifdef CONFIG_X86_INTEL_SHADOW_STACK_USER
++#define DISABLE_SHSTK	0
++#else
++#define DISABLE_SHSTK	(1<<(X86_FEATURE_SHSTK & 31))
++#endif
++
+ /*
+  * Make sure to add features to the correct mask
+  */
+@@ -75,7 +81,7 @@
+ #define DISABLED_MASK13	0
+ #define DISABLED_MASK14	0
+ #define DISABLED_MASK15	0
+-#define DISABLED_MASK16	(DISABLE_PKU|DISABLE_OSPKE|DISABLE_LA57|DISABLE_UMIP)
++#define DISABLED_MASK16	(DISABLE_PKU|DISABLE_OSPKE|DISABLE_LA57|DISABLE_UMIP|DISABLE_SHSTK)
+ #define DISABLED_MASK17	0
+ #define DISABLED_MASK18	0
+ #define DISABLED_MASK_CHECK BUILD_BUG_ON_ZERO(NCAPINTS != 19)
+-- 
+2.26.0
+
diff --git a/0030-x86-cet-shstk-Introduce-WRUSS-instruction.patch b/0030-x86-cet-shstk-Introduce-WRUSS-instruction.patch
new file mode 100644
index 000000000..723affae1
--- /dev/null
+++ b/0030-x86-cet-shstk-Introduce-WRUSS-instruction.patch
@@ -0,0 +1,66 @@
+From 742aa44809ec79885bb5549dade592340a90e4a9 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Thu, 22 Mar 2018 10:06:58 -0700
+Subject: [PATCH 30/52] x86/cet/shstk: Introduce WRUSS instruction
+
+WRUSS is a new kernel-mode instruction but writes directly to user Shadow
+Stack (SHSTK) memory.  This is used to construct a return address on SHSTK
+for the signal handler.
+
+This instruction can fault if the user SHSTK is not valid SHSTK memory.
+In that case, the kernel does a fixup.
+
+v4:
+- Change to asm goto.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/include/asm/special_insns.h | 32 ++++++++++++++++++++++++++++
+ 1 file changed, 32 insertions(+)
+
+diff --git a/arch/x86/include/asm/special_insns.h b/arch/x86/include/asm/special_insns.h
+index 6d37b8fcfc77..1b9b2e79c353 100644
+--- a/arch/x86/include/asm/special_insns.h
++++ b/arch/x86/include/asm/special_insns.h
+@@ -222,6 +222,38 @@ static inline void clwb(volatile void *__p)
+ 		: [pax] "a" (p));
+ }
+ 
++#ifdef CONFIG_X86_INTEL_CET
++#if defined(CONFIG_IA32_EMULATION) || defined(CONFIG_X86_X32)
++static inline int write_user_shstk_32(unsigned long addr, unsigned int val)
++{
++	asm_volatile_goto("1: wrussd %1, (%0)\n"
++			  _ASM_EXTABLE(1b, %l[fail])
++			  :: "r" (addr), "r" (val)
++			  :: fail);
++	return 0;
++fail:
++	return -EPERM;
++}
++#else
++static inline int write_user_shstk_32(unsigned long addr, unsigned int val)
++{
++	WARN_ONCE(1, "%s used but not supported.\n", __func__);
++	return -EFAULT;
++}
++#endif
++
++static inline int write_user_shstk_64(unsigned long addr, unsigned long val)
++{
++	asm_volatile_goto("1: wrussq %1, (%0)\n"
++			  _ASM_EXTABLE(1b, %l[fail])
++			  :: "r" (addr), "r" (val)
++			  :: fail);
++	return 0;
++fail:
++	return -EPERM;
++}
++#endif /* CONFIG_X86_INTEL_CET */
++
+ #define nop() asm volatile ("nop")
+ 
+ 
+-- 
+2.26.0
+
diff --git a/0031-x86-cet-shstk-Handle-signals-for-Shadow-Stack.patch b/0031-x86-cet-shstk-Handle-signals-for-Shadow-Stack.patch
new file mode 100644
index 000000000..4b01f9ad2
--- /dev/null
+++ b/0031-x86-cet-shstk-Handle-signals-for-Shadow-Stack.patch
@@ -0,0 +1,496 @@
+From ed60cbd7557251e6dd09cad552a4ea118f80c2b5 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Thu, 5 Jan 2017 13:48:31 -0800
+Subject: [PATCH 31/52] x86/cet/shstk: Handle signals for Shadow Stack
+
+To deliver a signal, create a Shadow Stack (SHSTK) restore token and put
+the token and the signal restorer address on the SHSTK.  For sigreturn,
+verify the token and restore the SHSTK pointer.
+
+Introduce a signal context extension struct 'sc_ext', which is used to save
+SHSTK restore token address and WAIT_ENDBR status.  WAIT_ENDBR will be
+introduced later in the Indirect Branch Tracking (IBT) series, but add that
+into sc_ext now to keep the struct stable in case the IBT series is applied
+later.
+
+v10:
+- Simply #ifdef.
+
+v9:
+- Update CET MSR access according to XSAVES supervisor state changes.
+- Add 'wait_endbr' to struct 'sc_ext'.
+- Update and simplify signal frame allocation, setup, and restoration.
+- Update commit log text.
+
+v2:
+- Move CET status from sigcontext to a separate struct sc_ext, which is
+  located above the fpstate on the signal frame.
+- Add a restore token for sigreturn address.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/ia32/ia32_signal.c            |  17 +++
+ arch/x86/include/asm/cet.h             |   7 ++
+ arch/x86/include/asm/fpu/internal.h    |  10 ++
+ arch/x86/include/uapi/asm/sigcontext.h |   9 ++
+ arch/x86/kernel/cet.c                  | 149 +++++++++++++++++++++++++
+ arch/x86/kernel/fpu/signal.c           |  97 ++++++++++++++++
+ arch/x86/kernel/signal.c               |  10 ++
+ 7 files changed, 299 insertions(+)
+
+diff --git a/arch/x86/ia32/ia32_signal.c b/arch/x86/ia32/ia32_signal.c
+index a3aefe9b9401..c307efe65555 100644
+--- a/arch/x86/ia32/ia32_signal.c
++++ b/arch/x86/ia32/ia32_signal.c
+@@ -35,6 +35,7 @@
+ #include <asm/sigframe.h>
+ #include <asm/sighandling.h>
+ #include <asm/smap.h>
++#include <asm/cet.h>
+ 
+ /*
+  * Do a signal return; undo the signal stack.
+@@ -221,6 +222,7 @@ static void __user *get_sigframe(struct ksignal *ksig, struct pt_regs *regs,
+ 				 void __user **fpstate)
+ {
+ 	unsigned long sp, fx_aligned, math_size;
++	void __user *restorer = NULL;
+ 
+ 	/* Default to using normal stack */
+ 	sp = regs->sp;
+@@ -234,8 +236,23 @@ static void __user *get_sigframe(struct ksignal *ksig, struct pt_regs *regs,
+ 		 ksig->ka.sa.sa_restorer)
+ 		sp = (unsigned long) ksig->ka.sa.sa_restorer;
+ 
++	if (ksig->ka.sa.sa_flags & SA_RESTORER) {
++		restorer = ksig->ka.sa.sa_restorer;
++	} else if (current->mm->context.vdso) {
++		if (ksig->ka.sa.sa_flags & SA_SIGINFO)
++			restorer = current->mm->context.vdso +
++				vdso_image_32.sym___kernel_rt_sigreturn;
++		else
++			restorer = current->mm->context.vdso +
++				vdso_image_32.sym___kernel_sigreturn;
++	}
++
+ 	sp = fpu__alloc_mathframe(sp, 1, &fx_aligned, &math_size);
+ 	*fpstate = (struct _fpstate_32 __user *) sp;
++
++	if (save_cet_to_sigframe(*fpstate, (unsigned long)restorer, 1))
++		return (void __user *) -1L;
++
+ 	if (copy_fpstate_to_sigframe(*fpstate, (void __user *)fx_aligned,
+ 				     math_size) < 0)
+ 		return (void __user *) -1L;
+diff --git a/arch/x86/include/asm/cet.h b/arch/x86/include/asm/cet.h
+index c44c991ca91f..409d4f91a0dc 100644
+--- a/arch/x86/include/asm/cet.h
++++ b/arch/x86/include/asm/cet.h
+@@ -6,6 +6,8 @@
+ #include <linux/types.h>
+ 
+ struct task_struct;
++struct sc_ext;
++
+ /*
+  * Per-thread CET status
+  */
+@@ -18,8 +20,13 @@ struct cet_status {
+ #ifdef CONFIG_X86_INTEL_CET
+ int cet_setup_shstk(void);
+ void cet_disable_free_shstk(struct task_struct *p);
++int cet_restore_signal(bool ia32, struct sc_ext *sc);
++int cet_setup_signal(bool ia32, unsigned long rstor, struct sc_ext *sc);
+ #else
+ static inline void cet_disable_free_shstk(struct task_struct *p) {}
++static inline int cet_restore_signal(bool ia32, struct sc_ext *sc) { return -EINVAL; }
++static inline int cet_setup_signal(bool ia32, unsigned long rstor,
++				   struct sc_ext *sc) { return -EINVAL; }
+ #endif
+ 
+ #define cpu_x86_cet_enabled() \
+diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
+index 42159f45bf9c..ed5a76f1f608 100644
+--- a/arch/x86/include/asm/fpu/internal.h
++++ b/arch/x86/include/asm/fpu/internal.h
+@@ -476,6 +476,16 @@ static inline void copy_kernel_to_fpregs(union fpregs_state *fpstate)
+ 	__copy_kernel_to_fpregs(fpstate, -1);
+ }
+ 
++#ifdef CONFIG_X86_INTEL_CET
++extern int save_cet_to_sigframe(void __user *fp, unsigned long restorer,
++				int is_ia32);
++#else
++static inline int save_cet_to_sigframe(void __user *fp, unsigned long restorer,
++				int is_ia32)
++{
++	return 0;
++}
++#endif
+ extern int copy_fpstate_to_sigframe(void __user *buf, void __user *fp, int size);
+ 
+ /*
+diff --git a/arch/x86/include/uapi/asm/sigcontext.h b/arch/x86/include/uapi/asm/sigcontext.h
+index 844d60eb1882..cf2d55db3be4 100644
+--- a/arch/x86/include/uapi/asm/sigcontext.h
++++ b/arch/x86/include/uapi/asm/sigcontext.h
+@@ -196,6 +196,15 @@ struct _xstate {
+ 	/* New processor state extensions go here: */
+ };
+ 
++/*
++ * Located at the end of sigcontext->fpstate, aligned to 8.
++ */
++struct sc_ext {
++	unsigned long total_size;
++	unsigned long ssp;
++	unsigned long wait_endbr;
++};
++
+ /*
+  * The 32-bit signal frame:
+  */
+diff --git a/arch/x86/kernel/cet.c b/arch/x86/kernel/cet.c
+index edf386e45c6d..f95211b4940e 100644
+--- a/arch/x86/kernel/cet.c
++++ b/arch/x86/kernel/cet.c
+@@ -19,6 +19,8 @@
+ #include <asm/fpu/xstate.h>
+ #include <asm/fpu/types.h>
+ #include <asm/cet.h>
++#include <asm/special_insns.h>
++#include <uapi/asm/sigcontext.h>
+ 
+ static void start_update_msrs(void)
+ {
+@@ -69,6 +71,80 @@ static unsigned long alloc_shstk(unsigned long size)
+ 	return addr;
+ }
+ 
++#define TOKEN_MODE_MASK	3UL
++#define TOKEN_MODE_64	1UL
++#define IS_TOKEN_64(token) ((token & TOKEN_MODE_MASK) == TOKEN_MODE_64)
++#define IS_TOKEN_32(token) ((token & TOKEN_MODE_MASK) == 0)
++
++/*
++ * Verify the restore token at the address of 'ssp' is
++ * valid and then set shadow stack pointer according to the
++ * token.
++ */
++static int verify_rstor_token(bool ia32, unsigned long ssp,
++			      unsigned long *new_ssp)
++{
++	unsigned long token;
++
++	*new_ssp = 0;
++
++	if (!IS_ALIGNED(ssp, 8))
++		return -EINVAL;
++
++	if (get_user(token, (unsigned long __user *)ssp))
++		return -EFAULT;
++
++	/* Is 64-bit mode flag correct? */
++	if (!ia32 && !IS_TOKEN_64(token))
++		return -EINVAL;
++	else if (ia32 && !IS_TOKEN_32(token))
++		return -EINVAL;
++
++	token &= ~TOKEN_MODE_MASK;
++
++	/*
++	 * Restore address properly aligned?
++	 */
++	if ((!ia32 && !IS_ALIGNED(token, 8)) || !IS_ALIGNED(token, 4))
++		return -EINVAL;
++
++	/*
++	 * Token was placed properly?
++	 */
++	if ((ALIGN_DOWN(token, 8) - 8) != ssp)
++		return -EINVAL;
++
++	*new_ssp = token;
++	return 0;
++}
++
++/*
++ * Create a restore token on the shadow stack.
++ * A token is always 8-byte and aligned to 8.
++ */
++static int create_rstor_token(bool ia32, unsigned long ssp,
++			      unsigned long *new_ssp)
++{
++	unsigned long addr;
++
++	*new_ssp = 0;
++
++	if ((!ia32 && !IS_ALIGNED(ssp, 8)) || !IS_ALIGNED(ssp, 4))
++		return -EINVAL;
++
++	addr = ALIGN_DOWN(ssp, 8) - 8;
++
++	/* Is the token for 64-bit? */
++	if (!ia32)
++		ssp |= TOKEN_MODE_64;
++
++	if (write_user_shstk_64(addr, ssp))
++		return -EFAULT;
++
++	*new_ssp = addr;
++	return 0;
++}
++
+ int cet_setup_shstk(void)
+ {
+ 	unsigned long addr, size;
+@@ -119,3 +195,76 @@ void cet_disable_free_shstk(struct task_struct *tsk)
+ 	cet->shstk_size = 0;
+ 	cet->shstk_enabled = 0;
+ }
++
++/*
++ * Called from __fpu__restore_sig() and XSAVES buffer is protected by
++ * set_thread_flag(TIF_NEED_FPU_LOAD).
++ */
++int cet_restore_signal(bool ia32, struct sc_ext *sc_ext)
++{
++	struct cet_user_state *cet_user_state;
++	struct cet_status *cet = &current->thread.cet;
++	unsigned long new_ssp = 0;
++	u64 msr_val = 0;
++	int err;
++
++	cet_user_state = get_xsave_addr(&current->thread.fpu.state.xsave,
++					XFEATURE_CET_USER);
++	if (!cet_user_state)
++		return -1;
++
++	if (cet->shstk_enabled) {
++		err = verify_rstor_token(ia32, sc_ext->ssp, &new_ssp);
++		if (err)
++			return err;
++
++		cet_user_state->user_ssp = new_ssp;
++		msr_val |= MSR_IA32_CET_SHSTK_EN;
++	}
++
++	cet_user_state->user_cet = msr_val;
++	return 0;
++}
++
++/*
++ * Setup the shadow stack for the signal handler: first,
++ * create a restore token to keep track of the current ssp,
++ * and then the return address of the signal handler.
++ */
++int cet_setup_signal(bool ia32, unsigned long rstor_addr, struct sc_ext *sc_ext)
++{
++	struct cet_status *cet = &current->thread.cet;
++	unsigned long ssp = 0, new_ssp = 0;
++	int err;
++
++	if (cet->shstk_enabled) {
++		if (!rstor_addr)
++			return -EINVAL;
++
++		ssp = cet_get_shstk_addr();
++		err = create_rstor_token(ia32, ssp, &new_ssp);
++		if (err)
++			return err;
++
++		if (ia32) {
++			ssp = new_ssp - sizeof(u32);
++			err = write_user_shstk_32(ssp, (unsigned int)rstor_addr);
++		} else {
++			ssp = new_ssp - sizeof(u64);
++			err = write_user_shstk_64(ssp, rstor_addr);
++		}
++
++		if (err)
++			return err;
++
++		sc_ext->ssp = new_ssp;
++	}
++
++	if (ssp) {
++		start_update_msrs();
++		wrmsrl(MSR_IA32_PL3_SSP, ssp);
++		end_update_msrs();
++	}
++
++	return 0;
++}
+diff --git a/arch/x86/kernel/fpu/signal.c b/arch/x86/kernel/fpu/signal.c
+index 15ddf174940b..816cd7f69b2d 100644
+--- a/arch/x86/kernel/fpu/signal.c
++++ b/arch/x86/kernel/fpu/signal.c
+@@ -52,6 +52,72 @@ static inline int check_for_xstate(struct fxregs_state __user *buf,
+ 	return 0;
+ }
+ 
++#ifdef CONFIG_X86_INTEL_CET
++int save_cet_to_sigframe(void __user *fp, unsigned long restorer, int is_ia32)
++{
++	int err = 0;
++
++	if (!current->thread.cet.shstk_enabled)
++		return 0;
++
++	if (fp) {
++		struct sc_ext ext = {0, 0, 0};
++
++		err = cet_setup_signal(is_ia32, restorer, &ext);
++		if (!err) {
++			void __user *p = fp;
++
++			ext.total_size = sizeof(ext);
++
++			if (is_ia32)
++				p += sizeof(struct fregs_state);
++
++			p += fpu_user_xstate_size + FP_XSTATE_MAGIC2_SIZE;
++			p = (void __user *)ALIGN((unsigned long)p, 8);
++
++			if (copy_to_user(p, &ext, sizeof(ext)))
++				return -EFAULT;
++		}
++	}
++
++	return err;
++}
++
++static int restore_cet_from_sigframe(int is_ia32, void __user *fp)
++{
++	int err = 0;
++
++	if (!current->thread.cet.shstk_enabled)
++		return 0;
++
++	if (fp) {
++		struct sc_ext ext = {0, 0, 0};
++		void __user *p = fp;
++
++		if (is_ia32)
++			p += sizeof(struct fregs_state);
++
++		p += fpu_user_xstate_size + FP_XSTATE_MAGIC2_SIZE;
++		p = (void __user *)ALIGN((unsigned long)p, 8);
++
++		if (copy_from_user(&ext, p, sizeof(ext)))
++			return -EFAULT;
++
++		if (ext.total_size != sizeof(ext))
++			return -EFAULT;
++
++		err = cet_restore_signal(is_ia32, &ext);
++	}
++
++	return err;
++}
++#else
++static int restore_cet_from_sigframe(int is_ia32, void __user *fp)
++{
++	return 0;
++}
++#endif
++
+ /*
+  * Signal frame handlers.
+  */
+@@ -367,6 +433,10 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 		pagefault_disable();
+ 		ret = copy_user_to_fpregs_zeroing(buf_fx, user_xfeatures, fx_only);
+ 		pagefault_enable();
++
++		if (!ret)
++			ret = restore_cet_from_sigframe(0, buf);
++
+ 		if (!ret) {
+ 			if (xfeatures_mask_supervisor())
+ 				copy_kernel_to_xregs(&fpu->state.xsave,
+@@ -397,6 +467,10 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
+ 		sanitize_restored_user_xstate(&fpu->state, envp, user_xfeatures,
+ 					      fx_only);
+ 
++		ret = restore_cet_from_sigframe((int)ia32_fxstate, buf);
++		if (ret)
++			goto err_out;
++
+ 		fpregs_lock();
+ 		if (unlikely(init_bv))
+ 			copy_kernel_to_xregs(&init_fpstate.xsave, init_bv);
+@@ -468,12 +542,35 @@ int fpu__restore_sig(void __user *buf, int ia32_frame)
+ 	return __fpu__restore_sig(buf, buf_fx, size);
+ }
+ 
++#ifdef CONFIG_X86_INTEL_CET
++static unsigned long fpu__alloc_sigcontext_ext(unsigned long sp)
++{
++	struct cet_status *cet = &current->thread.cet;
++
++	/*
++	 * sigcontext_ext is at: fpu + fpu_user_xstate_size +
++	 * FP_XSTATE_MAGIC2_SIZE, then aligned to 8.
++	 */
++	if (cet->shstk_enabled)
++		sp -= (sizeof(struct sc_ext) + 8);
++
++	return sp;
++}
++#else
++static unsigned long fpu__alloc_sigcontext_ext(unsigned long sp)
++{
++	return sp;
++}
++#endif
++
+ unsigned long
+ fpu__alloc_mathframe(unsigned long sp, int ia32_frame,
+ 		     unsigned long *buf_fx, unsigned long *size)
+ {
+ 	unsigned long frame_size = xstate_sigframe_size();
+ 
++	sp = fpu__alloc_sigcontext_ext(sp);
++
+ 	*buf_fx = sp = round_down(sp - frame_size, 64);
+ 	if (ia32_frame && use_fxsr()) {
+ 		frame_size += sizeof(struct fregs_state);
+diff --git a/arch/x86/kernel/signal.c b/arch/x86/kernel/signal.c
+index 35f878e9f91d..0d4ac5007256 100644
+--- a/arch/x86/kernel/signal.c
++++ b/arch/x86/kernel/signal.c
+@@ -46,6 +46,7 @@
+ 
+ #include <asm/sigframe.h>
+ #include <asm/signal.h>
++#include <asm/cet.h>
+ 
+ #define COPY(x)			do {			\
+ 	get_user_ex(regs->x, &sc->x);			\
+@@ -244,6 +245,9 @@ get_sigframe(struct k_sigaction *ka, struct pt_regs *regs, size_t frame_size,
+ 	unsigned long buf_fx = 0;
+ 	int onsigstack = on_sig_stack(sp);
+ 	int ret;
++#ifdef CONFIG_X86_64
++	void __user *restorer = NULL;
++#endif
+ 
+ 	/* redzone */
+ 	if (IS_ENABLED(CONFIG_X86_64))
+@@ -275,6 +279,12 @@ get_sigframe(struct k_sigaction *ka, struct pt_regs *regs, size_t frame_size,
+ 	if (onsigstack && !likely(on_sig_stack(sp)))
+ 		return (void __user *)-1L;
+ 
++#ifdef CONFIG_X86_64
++	if (ka->sa.sa_flags & SA_RESTORER)
++		restorer = ka->sa.sa_restorer;
++	ret = save_cet_to_sigframe(*fpstate, (unsigned long)restorer, 0);
++#endif
++
+ 	/* save i387 and extended state */
+ 	ret = copy_fpstate_to_sigframe(*fpstate, (void __user *)buf_fx, math_size);
+ 	if (ret < 0)
+-- 
+2.26.0
+
diff --git a/0032-ELF-UAPI-and-Kconfig-additions-for-ELF-program-prope.patch b/0032-ELF-UAPI-and-Kconfig-additions-for-ELF-program-prope.patch
new file mode 100644
index 000000000..fa4e61cb0
--- /dev/null
+++ b/0032-ELF-UAPI-and-Kconfig-additions-for-ELF-program-prope.patch
@@ -0,0 +1,85 @@
+From e4943fd3fe7c8bb8ee87fc5049dbb5302950a20b Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Fri, 18 Oct 2019 18:25:34 +0100
+Subject: [PATCH 32/52] ELF: UAPI and Kconfig additions for ELF program
+ properties
+
+Introduce basic ELF definitions relating to the NT_GNU_PROPERTY_TYPE_0
+note.
+
+v10:
+- Merge GNU_PROPERTY_X86_FEATURE_1_* from a separate patch.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+---
+ fs/Kconfig.binfmt        | 3 +++
+ include/linux/elf.h      | 8 ++++++++
+ include/uapi/linux/elf.h | 8 ++++++++
+ 3 files changed, 19 insertions(+)
+
+diff --git a/fs/Kconfig.binfmt b/fs/Kconfig.binfmt
+index 62dc4f577ba1..d2cfe0729a73 100644
+--- a/fs/Kconfig.binfmt
++++ b/fs/Kconfig.binfmt
+@@ -36,6 +36,9 @@ config COMPAT_BINFMT_ELF
+ config ARCH_BINFMT_ELF_STATE
+ 	bool
+ 
++config ARCH_USE_GNU_PROPERTY
++	bool
++
+ config BINFMT_ELF_FDPIC
+ 	bool "Kernel support for FDPIC ELF binaries"
+ 	default y if !BINFMT_ELF
+diff --git a/include/linux/elf.h b/include/linux/elf.h
+index e3649b3e970e..459cddcceaac 100644
+--- a/include/linux/elf.h
++++ b/include/linux/elf.h
+@@ -2,6 +2,7 @@
+ #ifndef _LINUX_ELF_H
+ #define _LINUX_ELF_H
+ 
++#include <linux/types.h>
+ #include <asm/elf.h>
+ #include <uapi/linux/elf.h>
+ 
+@@ -56,4 +57,11 @@ static inline int elf_coredump_extra_notes_write(struct coredump_params *cprm) {
+ extern int elf_coredump_extra_notes_size(void);
+ extern int elf_coredump_extra_notes_write(struct coredump_params *cprm);
+ #endif
++
++/* NT_GNU_PROPERTY_TYPE_0 header */
++struct gnu_property {
++	u32 pr_type;
++	u32 pr_datasz;
++};
++
+ #endif /* _LINUX_ELF_H */
+diff --git a/include/uapi/linux/elf.h b/include/uapi/linux/elf.h
+index 34c02e4290fe..61251ecabdd7 100644
+--- a/include/uapi/linux/elf.h
++++ b/include/uapi/linux/elf.h
+@@ -36,6 +36,7 @@ typedef __s64	Elf64_Sxword;
+ #define PT_LOPROC  0x70000000
+ #define PT_HIPROC  0x7fffffff
+ #define PT_GNU_EH_FRAME		0x6474e550
++#define PT_GNU_PROPERTY		0x6474e553
+ 
+ #define PT_GNU_STACK	(PT_LOOS + 0x474e551)
+ 
+@@ -443,4 +444,11 @@ typedef struct elf64_note {
+   Elf64_Word n_type;	/* Content type */
+ } Elf64_Nhdr;
+ 
++/* .note.gnu.property types */
++#define GNU_PROPERTY_X86_FEATURE_1_AND		0xc0000002
++
++/* Bits of GNU_PROPERTY_X86_FEATURE_1_AND */
++#define GNU_PROPERTY_X86_FEATURE_1_IBT		0x00000001
++#define GNU_PROPERTY_X86_FEATURE_1_SHSTK	0x00000002
++
+ #endif /* _UAPI_LINUX_ELF_H */
+-- 
+2.26.0
+
diff --git a/0033-ELF-Add-ELF-program-property-parsing-support.patch b/0033-ELF-Add-ELF-program-property-parsing-support.patch
new file mode 100644
index 000000000..f2ce5c60d
--- /dev/null
+++ b/0033-ELF-Add-ELF-program-property-parsing-support.patch
@@ -0,0 +1,310 @@
+From 711d9b91b1de10ba0d79cedc2a9b6e901a90f5db Mon Sep 17 00:00:00 2001
+From: Dave Martin <Dave.Martin@arm.com>
+Date: Mon, 16 Mar 2020 16:50:44 +0000
+Subject: [PATCH 33/52] ELF: Add ELF program property parsing support
+
+ELF program properties will be needed for detecting whether to
+enable optional architecture or ABI features for a new ELF process.
+
+For now, there are no generic properties that we care about, so do
+nothing unless CONFIG_ARCH_USE_GNU_PROPERTY=y.
+
+Otherwise, the presence of properties using the PT_PROGRAM_PROPERTY
+phdrs entry (if any), and notify each property to the arch code.
+
+For now, the added code is not used.
+
+Signed-off-by: Dave Martin <Dave.Martin@arm.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+Signed-off-by: Mark Brown <broonie@kernel.org>
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ fs/binfmt_elf.c          | 127 +++++++++++++++++++++++++++++++++++++++
+ fs/compat_binfmt_elf.c   |   4 ++
+ include/linux/elf.h      |  19 ++++++
+ include/uapi/linux/elf.h |   4 ++
+ 4 files changed, 154 insertions(+)
+
+diff --git a/fs/binfmt_elf.c b/fs/binfmt_elf.c
+index f4713ea76e82..1fb67e506b68 100644
+--- a/fs/binfmt_elf.c
++++ b/fs/binfmt_elf.c
+@@ -39,12 +39,18 @@
+ #include <linux/sched/coredump.h>
+ #include <linux/sched/task_stack.h>
+ #include <linux/sched/cputime.h>
++#include <linux/sizes.h>
++#include <linux/types.h>
+ #include <linux/cred.h>
+ #include <linux/dax.h>
+ #include <linux/uaccess.h>
+ #include <asm/param.h>
+ #include <asm/page.h>
+ 
++#ifndef ELF_COMPAT
++#define ELF_COMPAT 0
++#endif
++
+ #ifndef user_long_t
+ #define user_long_t long
+ #endif
+@@ -681,6 +687,111 @@ static unsigned long load_elf_interp(struct elfhdr *interp_elf_ex,
+  * libraries.  There is no binary dependent code anywhere else.
+  */
+ 
++static int parse_elf_property(const char *data, size_t *off, size_t datasz,
++			      struct arch_elf_state *arch,
++			      bool have_prev_type, u32 *prev_type)
++{
++	size_t o, step;
++	const struct gnu_property *pr;
++	int ret;
++
++	if (*off == datasz)
++		return -ENOENT;
++
++	if (WARN_ON_ONCE(*off > datasz || *off % ELF_GNU_PROPERTY_ALIGN))
++		return -EIO;
++	o = *off;
++	datasz -= *off;
++
++	if (datasz < sizeof(*pr))
++		return -ENOEXEC;
++	pr = (const struct gnu_property *)(data + o);
++	o += sizeof(*pr);
++	datasz -= sizeof(*pr);
++
++	if (pr->pr_datasz > datasz)
++		return -ENOEXEC;
++
++	WARN_ON_ONCE(o % ELF_GNU_PROPERTY_ALIGN);
++	step = round_up(pr->pr_datasz, ELF_GNU_PROPERTY_ALIGN);
++	if (step > datasz)
++		return -ENOEXEC;
++
++	/* Properties are supposed to be unique and sorted on pr_type: */
++	if (have_prev_type && pr->pr_type <= *prev_type)
++		return -ENOEXEC;
++	*prev_type = pr->pr_type;
++
++	ret = arch_parse_elf_property(pr->pr_type, data + o,
++				      pr->pr_datasz, ELF_COMPAT, arch);
++	if (ret)
++		return ret;
++
++	*off = o + step;
++	return 0;
++}
++
++#define NOTE_DATA_SZ SZ_1K
++#define GNU_PROPERTY_TYPE_0_NAME "GNU"
++#define NOTE_NAME_SZ (sizeof(GNU_PROPERTY_TYPE_0_NAME))
++
++static int parse_elf_properties(struct file *f, const struct elf_phdr *phdr,
++				struct arch_elf_state *arch)
++{
++	union {
++		struct elf_note nhdr;
++		char data[NOTE_DATA_SZ];
++	} note;
++	loff_t pos;
++	ssize_t n;
++	size_t off, datasz;
++	int ret;
++	bool have_prev_type;
++	u32 prev_type;
++
++	if (!IS_ENABLED(CONFIG_ARCH_USE_GNU_PROPERTY) || !phdr)
++		return 0;
++
++	/* load_elf_binary() shouldn't call us unless this is true... */
++	if (WARN_ON_ONCE(phdr->p_type != PT_GNU_PROPERTY))
++		return -ENOEXEC;
++
++	/* If the properties are crazy large, that's too bad (for now): */
++	if (phdr->p_filesz > sizeof(note))
++		return -ENOEXEC;
++
++	pos = phdr->p_offset;
++	n = kernel_read(f, &note, phdr->p_filesz, &pos);
++
++	BUILD_BUG_ON(sizeof(note) < sizeof(note.nhdr) + NOTE_NAME_SZ);
++	if (n < 0 || n < sizeof(note.nhdr) + NOTE_NAME_SZ)
++		return -EIO;
++
++	if (note.nhdr.n_type != NT_GNU_PROPERTY_TYPE_0 ||
++	    note.nhdr.n_namesz != NOTE_NAME_SZ ||
++	    strncmp(note.data + sizeof(note.nhdr),
++		    GNU_PROPERTY_TYPE_0_NAME, n - sizeof(note.nhdr)))
++		return -ENOEXEC;
++
++	off = round_up(sizeof(note.nhdr) + NOTE_NAME_SZ,
++		       ELF_GNU_PROPERTY_ALIGN);
++	if (off > n)
++		return -ENOEXEC;
++
++	if (note.nhdr.n_descsz > n - off)
++		return -ENOEXEC;
++	datasz = off + note.nhdr.n_descsz;
++
++	have_prev_type = false;
++	do {
++		ret = parse_elf_property(note.data, &off, datasz, arch,
++					 have_prev_type, &prev_type);
++		have_prev_type = true;
++	} while (!ret);
++
++	return ret == -ENOENT ? 0 : ret;
++}
++
+ static int load_elf_binary(struct linux_binprm *bprm)
+ {
+ 	struct file *interpreter = NULL; /* to shut gcc up */
+@@ -688,6 +799,7 @@ static int load_elf_binary(struct linux_binprm *bprm)
+ 	int load_addr_set = 0;
+ 	unsigned long error;
+ 	struct elf_phdr *elf_ppnt, *elf_phdata, *interp_elf_phdata = NULL;
++	struct elf_phdr *elf_property_phdata = NULL;
+ 	unsigned long elf_bss, elf_brk;
+ 	int bss_prot = 0;
+ 	int retval, i;
+@@ -733,6 +845,11 @@ static int load_elf_binary(struct linux_binprm *bprm)
+ 	for (i = 0; i < elf_ex->e_phnum; i++, elf_ppnt++) {
+ 		char *elf_interpreter;
+ 
++		if (elf_ppnt->p_type == PT_GNU_PROPERTY) {
++			elf_property_phdata = elf_ppnt;
++			continue;
++		}
++
+ 		if (elf_ppnt->p_type != PT_INTERP)
+ 			continue;
+ 
+@@ -820,9 +937,14 @@ static int load_elf_binary(struct linux_binprm *bprm)
+ 			goto out_free_dentry;
+ 
+ 		/* Pass PT_LOPROC..PT_HIPROC headers to arch code */
++		elf_property_phdata = NULL;
+ 		elf_ppnt = interp_elf_phdata;
+ 		for (i = 0; i < loc->interp_elf_ex.e_phnum; i++, elf_ppnt++)
+ 			switch (elf_ppnt->p_type) {
++			case PT_GNU_PROPERTY:
++				elf_property_phdata = elf_ppnt;
++				break;
++
+ 			case PT_LOPROC ... PT_HIPROC:
+ 				retval = arch_elf_pt_proc(&loc->interp_elf_ex,
+ 							  elf_ppnt, interpreter,
+@@ -833,6 +955,11 @@ static int load_elf_binary(struct linux_binprm *bprm)
+ 			}
+ 	}
+ 
++	retval = parse_elf_properties(interpreter ?: bprm->file,
++				      elf_property_phdata, &arch_state);
++	if (retval)
++		goto out_free_dentry;
++
+ 	/*
+ 	 * Allow arch code to reject the ELF at this point, whilst it's
+ 	 * still possible to return an error to the code that invoked
+diff --git a/fs/compat_binfmt_elf.c b/fs/compat_binfmt_elf.c
+index aaad4ca1217e..13a087bc816b 100644
+--- a/fs/compat_binfmt_elf.c
++++ b/fs/compat_binfmt_elf.c
+@@ -17,6 +17,8 @@
+ #include <linux/elfcore-compat.h>
+ #include <linux/time.h>
+ 
++#define ELF_COMPAT	1
++
+ /*
+  * Rename the basic ELF layout types to refer to the 32-bit class of files.
+  */
+@@ -28,11 +30,13 @@
+ #undef	elf_shdr
+ #undef	elf_note
+ #undef	elf_addr_t
++#undef	ELF_GNU_PROPERTY_ALIGN
+ #define elfhdr		elf32_hdr
+ #define elf_phdr	elf32_phdr
+ #define elf_shdr	elf32_shdr
+ #define elf_note	elf32_note
+ #define elf_addr_t	Elf32_Addr
++#define ELF_GNU_PROPERTY_ALIGN	ELF32_GNU_PROPERTY_ALIGN
+ 
+ /*
+  * Some data types as stored in coredump.
+diff --git a/include/linux/elf.h b/include/linux/elf.h
+index 459cddcceaac..7bdc6da160c7 100644
+--- a/include/linux/elf.h
++++ b/include/linux/elf.h
+@@ -22,6 +22,9 @@
+ 	SET_PERSONALITY(ex)
+ #endif
+ 
++#define ELF32_GNU_PROPERTY_ALIGN	4
++#define ELF64_GNU_PROPERTY_ALIGN	8
++
+ #if ELF_CLASS == ELFCLASS32
+ 
+ extern Elf32_Dyn _DYNAMIC [];
+@@ -32,6 +35,7 @@ extern Elf32_Dyn _DYNAMIC [];
+ #define elf_addr_t	Elf32_Off
+ #define Elf_Half	Elf32_Half
+ #define Elf_Word	Elf32_Word
++#define ELF_GNU_PROPERTY_ALIGN	ELF32_GNU_PROPERTY_ALIGN
+ 
+ #else
+ 
+@@ -43,6 +47,7 @@ extern Elf64_Dyn _DYNAMIC [];
+ #define elf_addr_t	Elf64_Off
+ #define Elf_Half	Elf64_Half
+ #define Elf_Word	Elf64_Word
++#define ELF_GNU_PROPERTY_ALIGN	ELF64_GNU_PROPERTY_ALIGN
+ 
+ #endif
+ 
+@@ -64,4 +69,18 @@ struct gnu_property {
+ 	u32 pr_datasz;
+ };
+ 
++struct arch_elf_state;
++
++#ifndef CONFIG_ARCH_USE_GNU_PROPERTY
++static inline int arch_parse_elf_property(u32 type, const void *data,
++					  size_t datasz, bool compat,
++					  struct arch_elf_state *arch)
++{
++	return 0;
++}
++#else
++extern int arch_parse_elf_property(u32 type, const void *data, size_t datasz,
++				   bool compat, struct arch_elf_state *arch);
++#endif
++
+ #endif /* _LINUX_ELF_H */
+diff --git a/include/uapi/linux/elf.h b/include/uapi/linux/elf.h
+index 61251ecabdd7..518651708d8f 100644
+--- a/include/uapi/linux/elf.h
++++ b/include/uapi/linux/elf.h
+@@ -368,6 +368,7 @@ typedef struct elf64_shdr {
+  * Notes used in ET_CORE. Architectures export some of the arch register sets
+  * using the corresponding note types via the PTRACE_GETREGSET and
+  * PTRACE_SETREGSET requests.
++ * The note name for all these is "LINUX".
+  */
+ #define NT_PRSTATUS	1
+ #define NT_PRFPREG	2
+@@ -430,6 +431,9 @@ typedef struct elf64_shdr {
+ #define NT_MIPS_FP_MODE	0x801		/* MIPS floating-point mode */
+ #define NT_MIPS_MSA	0x802		/* MIPS SIMD registers */
+ 
++/* Note types with note name "GNU" */
++#define NT_GNU_PROPERTY_TYPE_0	5
++
+ /* Note header in a PT_NOTE section */
+ typedef struct elf32_note {
+   Elf32_Word	n_namesz;	/* Name size */
+-- 
+2.26.0
+
diff --git a/0034-ELF-Introduce-arch_setup_elf_property.patch b/0034-ELF-Introduce-arch_setup_elf_property.patch
new file mode 100644
index 000000000..1b8df6139
--- /dev/null
+++ b/0034-ELF-Introduce-arch_setup_elf_property.patch
@@ -0,0 +1,55 @@
+From ffdb29fc6882412d3e845dd42cc192e55136247a Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 20 Aug 2019 13:20:37 -0700
+Subject: [PATCH 34/52] ELF: Introduce arch_setup_elf_property()
+
+An ELF file's .note.gnu.property indicates architecture features of the
+file.  These features are extracted earlier and stored in the struct
+'arch_elf_state'.  Introduce arch_setup_elf_property() to setup and enable
+these features.  The first use-case of this function is Shadow Stack and
+Indirect Branch Tracking, which are introduced later.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ fs/binfmt_elf.c     | 4 ++++
+ include/linux/elf.h | 6 ++++++
+ 2 files changed, 10 insertions(+)
+
+diff --git a/fs/binfmt_elf.c b/fs/binfmt_elf.c
+index 1fb67e506b68..1de4fee18421 100644
+--- a/fs/binfmt_elf.c
++++ b/fs/binfmt_elf.c
+@@ -1215,6 +1215,10 @@ static int load_elf_binary(struct linux_binprm *bprm)
+ 
+ 	set_binfmt(&elf_format);
+ 
++	retval = arch_setup_elf_property(&arch_state);
++	if (retval < 0)
++		goto out;
++
+ #ifdef ARCH_HAS_SETUP_ADDITIONAL_PAGES
+ 	retval = arch_setup_additional_pages(bprm, !!interpreter);
+ 	if (retval < 0)
+diff --git a/include/linux/elf.h b/include/linux/elf.h
+index 7bdc6da160c7..81f2161fa4a8 100644
+--- a/include/linux/elf.h
++++ b/include/linux/elf.h
+@@ -78,9 +78,15 @@ static inline int arch_parse_elf_property(u32 type, const void *data,
+ {
+ 	return 0;
+ }
++
++static inline int arch_setup_elf_property(struct arch_elf_state *arch)
++{
++	return 0;
++}
+ #else
+ extern int arch_parse_elf_property(u32 type, const void *data, size_t datasz,
+ 				   bool compat, struct arch_elf_state *arch);
++extern int arch_setup_elf_property(struct arch_elf_state *arch);
+ #endif
+ 
+ #endif /* _LINUX_ELF_H */
+-- 
+2.26.0
+
diff --git a/0035-x86-cet-shstk-ELF-header-parsing-for-Shadow-Stack.patch b/0035-x86-cet-shstk-ELF-header-parsing-for-Shadow-Stack.patch
new file mode 100644
index 000000000..b3c0de528
--- /dev/null
+++ b/0035-x86-cet-shstk-ELF-header-parsing-for-Shadow-Stack.patch
@@ -0,0 +1,95 @@
+From d757aca00bb352f1661c02d68d6d14f414825c58 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 3 Oct 2017 16:07:12 -0700
+Subject: [PATCH 35/52] x86/cet/shstk: ELF header parsing for Shadow Stack
+
+Check an ELF file's .note.gnu.property, and setup Shadow Stack if the
+application supports it.
+
+v9:
+- Change cpu_feature_enabled() to static_cpu_has().
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/Kconfig             |  2 ++
+ arch/x86/include/asm/elf.h   | 13 +++++++++++++
+ arch/x86/kernel/process_64.c | 29 +++++++++++++++++++++++++++++
+ 3 files changed, 44 insertions(+)
+
+diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
+index 8c9b2829c22b..4042350acb1c 100644
+--- a/arch/x86/Kconfig
++++ b/arch/x86/Kconfig
+@@ -1969,6 +1969,8 @@ config X86_INTEL_SHADOW_STACK_USER
+ 	select ARCH_USES_HIGH_VMA_FLAGS
+ 	select X86_INTEL_CET
+ 	select ARCH_HAS_SHSTK
++	select ARCH_USE_GNU_PROPERTY
++	select ARCH_BINFMT_ELF_STATE
+ 	---help---
+ 	  Shadow Stacks provides protection against program stack
+ 	  corruption.  It's a hardware feature.  This only matters
+diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h
+index 69c0f892e310..fac79b621e0a 100644
+--- a/arch/x86/include/asm/elf.h
++++ b/arch/x86/include/asm/elf.h
+@@ -367,6 +367,19 @@ extern int compat_arch_setup_additional_pages(struct linux_binprm *bprm,
+ 					      int uses_interp);
+ #define compat_arch_setup_additional_pages compat_arch_setup_additional_pages
+ 
++#ifdef CONFIG_ARCH_BINFMT_ELF_STATE
++struct arch_elf_state {
++	unsigned int gnu_property;
++};
++
++#define INIT_ARCH_ELF_STATE {	\
++	.gnu_property = 0,	\
++}
++
++#define arch_elf_pt_proc(ehdr, phdr, elf, interp, state) (0)
++#define arch_check_elf(ehdr, interp, interp_ehdr, state) (0)
++#endif
++
+ /* Do not change the values. See get_align_mask() */
+ enum align_flags {
+ 	ALIGN_VA_32	= BIT(0),
+diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
+index ffd497804dbc..133a917c0d35 100644
+--- a/arch/x86/kernel/process_64.c
++++ b/arch/x86/kernel/process_64.c
+@@ -731,3 +731,32 @@ unsigned long KSTK_ESP(struct task_struct *task)
+ {
+ 	return task_pt_regs(task)->sp;
+ }
++
++#ifdef CONFIG_ARCH_USE_GNU_PROPERTY
++int arch_parse_elf_property(u32 type, const void *data, size_t datasz,
++			     bool compat, struct arch_elf_state *state)
++{
++	if (type != GNU_PROPERTY_X86_FEATURE_1_AND)
++		return 0;
++
++	if (datasz != sizeof(unsigned int))
++		return -ENOEXEC;
++
++	state->gnu_property = *(unsigned int *)data;
++	return 0;
++}
++
++int arch_setup_elf_property(struct arch_elf_state *state)
++{
++	int r = 0;
++
++	memset(&current->thread.cet, 0, sizeof(struct cet_status));
++
++	if (static_cpu_has(X86_FEATURE_SHSTK)) {
++		if (state->gnu_property & GNU_PROPERTY_X86_FEATURE_1_SHSTK)
++			r = cet_setup_shstk();
++	}
++
++	return r;
++}
++#endif
+-- 
+2.26.0
+
diff --git a/0036-x86-cet-shstk-Handle-thread-Shadow-Stack.patch b/0036-x86-cet-shstk-Handle-thread-Shadow-Stack.patch
new file mode 100644
index 000000000..51d99183e
--- /dev/null
+++ b/0036-x86-cet-shstk-Handle-thread-Shadow-Stack.patch
@@ -0,0 +1,161 @@
+From 8c83768388869257a2f6b7a61c72685ca384588d Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Thu, 3 May 2018 12:40:57 -0700
+Subject: [PATCH 36/52] x86/cet/shstk: Handle thread Shadow Stack
+
+The shadow stack for clone/fork is handled as the following:
+
+(1) If ((clone_flags & (CLONE_VFORK | CLONE_VM)) == CLONE_VM),
+    the kernel allocates (and frees on thread exit) a new shadow stack for
+    the child.
+
+    It is possible for the kernel to complete the clone syscall and set the
+    child's shadow stack pointer to NULL and let the child thread allocate
+    a shadow stack for itself.  There are two issues in this approach: It
+    is not compatible with existing code that does inline syscall and it
+    cannot handle signals before the child can successfully allocate a
+    shadow stack.
+
+(2) For (clone_flags & CLONE_VFORK), the child uses the existing shadow
+    stack.
+
+(3) For all other cases, the shadow stack is copied/reused whenever the
+    parent or the child does a call/ret.
+
+This patch handles cases (1) & (2).  Case (3) is handled in the shaodw
+stack page fault patches.
+
+A 64-bit shadow stack has a size of min(RLIMIT_STACK, 4GB).  A compat-mode
+thread shadow stack has a size of 1/4 min(RLIMIT_STACK, 4GB).  This allows
+more threads to share a 32-bit address space.
+
+v10:
+- Limit shadow stack size to 4GB.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/include/asm/cet.h         |  2 ++
+ arch/x86/include/asm/mmu_context.h |  3 +++
+ arch/x86/kernel/cet.c              | 42 ++++++++++++++++++++++++++++++
+ arch/x86/kernel/process.c          |  7 +++++
+ 4 files changed, 54 insertions(+)
+
+diff --git a/arch/x86/include/asm/cet.h b/arch/x86/include/asm/cet.h
+index 409d4f91a0dc..9a3e2da9c1c4 100644
+--- a/arch/x86/include/asm/cet.h
++++ b/arch/x86/include/asm/cet.h
+@@ -19,10 +19,12 @@ struct cet_status {
+ 
+ #ifdef CONFIG_X86_INTEL_CET
+ int cet_setup_shstk(void);
++int cet_setup_thread_shstk(struct task_struct *p);
+ void cet_disable_free_shstk(struct task_struct *p);
+ int cet_restore_signal(bool ia32, struct sc_ext *sc);
+ int cet_setup_signal(bool ia32, unsigned long rstor, struct sc_ext *sc);
+ #else
++static inline int cet_setup_thread_shstk(struct task_struct *p) { return 0; }
+ static inline void cet_disable_free_shstk(struct task_struct *p) {}
+ static inline int cet_restore_signal(bool ia32, struct sc_ext *sc) { return -EINVAL; }
+ static inline int cet_setup_signal(bool ia32, unsigned long rstor,
+diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h
+index b538d9ddee9c..c09f6ea2aa60 100644
+--- a/arch/x86/include/asm/mmu_context.h
++++ b/arch/x86/include/asm/mmu_context.h
+@@ -12,6 +12,7 @@
+ #include <asm/pgalloc.h>
+ #include <asm/tlbflush.h>
+ #include <asm/paravirt.h>
++#include <asm/cet.h>
+ #include <asm/debugreg.h>
+ 
+ extern atomic64_t last_mm_ctx_id;
+@@ -155,6 +156,8 @@ do {						\
+ #else
+ #define deactivate_mm(tsk, mm)			\
+ do {						\
++	if (!tsk->vfork_done)			\
++		cet_disable_free_shstk(tsk);	\
+ 	load_gs_index(0);			\
+ 	loadsegment(fs, 0);			\
+ } while (0)
+diff --git a/arch/x86/kernel/cet.c b/arch/x86/kernel/cet.c
+index f95211b4940e..24cefffa4b70 100644
+--- a/arch/x86/kernel/cet.c
++++ b/arch/x86/kernel/cet.c
+@@ -170,6 +170,48 @@ int cet_setup_shstk(void)
+ 	return 0;
+ }
+ 
++int cet_setup_thread_shstk(struct task_struct *tsk)
++{
++	unsigned long addr, size;
++	struct cet_user_state *state;
++	struct cet_status *cet = &tsk->thread.cet;
++
++	if (!cet->shstk_enabled)
++		return 0;
++
++	state = get_xsave_addr(&tsk->thread.fpu.state.xsave,
++			       XFEATURE_CET_USER);
++
++	if (!state)
++		return -EINVAL;
++
++	/* Cap shadow stack size to 4 GB */
++	size = min(rlimit(RLIMIT_STACK), 1UL << 32);
++
++	/*
++	 * Compat-mode pthreads share a limited address space.
++	 * If each function call takes an average of four slots
++	 * stack space, we need 1/4 of stack size for shadow stack.
++	 */
++	if (in_compat_syscall())
++		size /= 4;
++
++	addr = alloc_shstk(size);
++
++	if (IS_ERR((void *)addr)) {
++		cet->shstk_base = 0;
++		cet->shstk_size = 0;
++		cet->shstk_enabled = 0;
++		return PTR_ERR((void *)addr);
++	}
++
++	fpu__prepare_write(&tsk->thread.fpu);
++	state->user_ssp = (u64)(addr + size);
++	cet->shstk_base = addr;
++	cet->shstk_size = size;
++	return 0;
++}
++
+ void cet_disable_free_shstk(struct task_struct *tsk)
+ {
+ 	struct cet_status *cet = &tsk->thread.cet;
+diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
+index e478a0e00aaa..ca7e6ef8e77e 100644
+--- a/arch/x86/kernel/process.c
++++ b/arch/x86/kernel/process.c
+@@ -110,6 +110,7 @@ void exit_thread(struct task_struct *tsk)
+ 
+ 	free_vm86(t);
+ 
++	cet_disable_free_shstk(tsk);
+ 	fpu__drop(fpu);
+ }
+ 
+@@ -180,6 +181,12 @@ int copy_thread_tls(unsigned long clone_flags, unsigned long sp,
+ 	if (clone_flags & CLONE_SETTLS)
+ 		ret = set_new_tls(p, tls);
+ 
++#ifdef CONFIG_X86_64
++	/* Allocate a new shadow stack for pthread */
++	if (!ret && (clone_flags & (CLONE_VFORK | CLONE_VM)) == CLONE_VM)
++		ret = cet_setup_thread_shstk(p);
++#endif
++
+ 	if (!ret && unlikely(test_tsk_thread_flag(current, TIF_IO_BITMAP)))
+ 		io_bitmap_share(p);
+ 
+-- 
+2.26.0
+
diff --git a/0037-mm-mmap-Add-Shadow-Stack-pages-to-memory-accounting.patch b/0037-mm-mmap-Add-Shadow-Stack-pages-to-memory-accounting.patch
new file mode 100644
index 000000000..6eb442ce9
--- /dev/null
+++ b/0037-mm-mmap-Add-Shadow-Stack-pages-to-memory-accounting.patch
@@ -0,0 +1,41 @@
+From e0a2e193ee30bb951fa445abf20224e07057282f Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Thu, 13 Sep 2018 12:36:48 -0700
+Subject: [PATCH 37/52] mm/mmap: Add Shadow Stack pages to memory accounting
+
+Add Shadow Stack pages to memory accounting.
+
+v8:
+- Change Shadow Stake pages from data_vm to stack_vm.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ mm/mmap.c | 5 +++++
+ 1 file changed, 5 insertions(+)
+
+diff --git a/mm/mmap.c b/mm/mmap.c
+index d681a20eb4ea..99d1251c8a3b 100644
+--- a/mm/mmap.c
++++ b/mm/mmap.c
+@@ -1679,6 +1679,9 @@ static inline int accountable_mapping(struct file *file, vm_flags_t vm_flags)
+ 	if (file && is_file_hugepages(file))
+ 		return 0;
+ 
++	if (arch_copy_pte_mapping(vm_flags))
++		return 1;
++
+ 	return (vm_flags & (VM_NORESERVE | VM_SHARED | VM_WRITE)) == VM_WRITE;
+ }
+ 
+@@ -3294,6 +3297,8 @@ void vm_stat_account(struct mm_struct *mm, vm_flags_t flags, long npages)
+ 		mm->stack_vm += npages;
+ 	else if (is_data_mapping(flags))
+ 		mm->data_vm += npages;
++	else if (arch_copy_pte_mapping(flags))
++		mm->stack_vm += npages;
+ }
+ 
+ static vm_fault_t special_mapping_fault(struct vm_fault *vmf);
+-- 
+2.26.0
+
diff --git a/0038-x86-cet-shstk-Add-arch_prctl-functions-for-Shadow-St.patch b/0038-x86-cet-shstk-Add-arch_prctl-functions-for-Shadow-St.patch
new file mode 100644
index 000000000..60720e6a0
--- /dev/null
+++ b/0038-x86-cet-shstk-Add-arch_prctl-functions-for-Shadow-St.patch
@@ -0,0 +1,240 @@
+From 9bdae575945d43c5f96ef731ec844e05a8d22a94 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Thu, 3 May 2018 13:04:29 -0700
+Subject: [PATCH 38/52] x86/cet/shstk: Add arch_prctl functions for Shadow
+ Stack
+
+arch_prctl(ARCH_X86_CET_STATUS, u64 *addr)
+    Return CET feature status.
+
+    The parameter 'addr' is a pointer to a user buffer.  On returning to
+    the caller, the kernel fills the following information:
+
+    *addr = SHSTK/IBT status
+    *(addr + 1) = SHSTK base address
+    *(addr + 2) = SHSTK size
+
+arch_prctl(ARCH_X86_CET_DISABLE, u64 features)
+    Disable CET features specified in 'features'.  Return -EPERM if CET is
+    locked.
+
+arch_prctl(ARCH_X86_CET_LOCK)
+    Lock in CET feature.
+
+arch_prctl(ARCH_X86_CET_ALLOC_SHSTK, u64 *addr)
+    Allocate a new SHSTK.
+
+    The parameter 'addr' is a pointer to a user buffer and indicates the
+    desired SHSTK size to allocate.  On returning to the caller the buffer
+    contains the address of the new SHSTK.
+
+v10:
+- Change input parameters from unsigned long to u64, to make it clear they
+  are 64-bit.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/include/asm/cet.h        |  4 ++
+ arch/x86/include/uapi/asm/prctl.h |  5 ++
+ arch/x86/kernel/Makefile          |  2 +-
+ arch/x86/kernel/cet.c             | 29 +++++++++++
+ arch/x86/kernel/cet_prctl.c       | 80 +++++++++++++++++++++++++++++++
+ arch/x86/kernel/process.c         |  4 +-
+ 6 files changed, 121 insertions(+), 3 deletions(-)
+ create mode 100644 arch/x86/kernel/cet_prctl.c
+
+diff --git a/arch/x86/include/asm/cet.h b/arch/x86/include/asm/cet.h
+index 9a3e2da9c1c4..845ffe677e9b 100644
+--- a/arch/x86/include/asm/cet.h
++++ b/arch/x86/include/asm/cet.h
+@@ -14,16 +14,20 @@ struct sc_ext;
+ struct cet_status {
+ 	unsigned long	shstk_base;
+ 	unsigned long	shstk_size;
++	unsigned int	locked:1;
+ 	unsigned int	shstk_enabled:1;
+ };
+ 
+ #ifdef CONFIG_X86_INTEL_CET
++int prctl_cet(int option, u64 arg2);
+ int cet_setup_shstk(void);
+ int cet_setup_thread_shstk(struct task_struct *p);
++int cet_alloc_shstk(unsigned long *arg);
+ void cet_disable_free_shstk(struct task_struct *p);
+ int cet_restore_signal(bool ia32, struct sc_ext *sc);
+ int cet_setup_signal(bool ia32, unsigned long rstor, struct sc_ext *sc);
+ #else
++static inline int prctl_cet(int option, u64 arg2) { return -EINVAL; }
+ static inline int cet_setup_thread_shstk(struct task_struct *p) { return 0; }
+ static inline void cet_disable_free_shstk(struct task_struct *p) {}
+ static inline int cet_restore_signal(bool ia32, struct sc_ext *sc) { return -EINVAL; }
+diff --git a/arch/x86/include/uapi/asm/prctl.h b/arch/x86/include/uapi/asm/prctl.h
+index 5a6aac9fa41f..d962f0ec9ccf 100644
+--- a/arch/x86/include/uapi/asm/prctl.h
++++ b/arch/x86/include/uapi/asm/prctl.h
+@@ -14,4 +14,9 @@
+ #define ARCH_MAP_VDSO_32	0x2002
+ #define ARCH_MAP_VDSO_64	0x2003
+ 
++#define ARCH_X86_CET_STATUS		0x3001
++#define ARCH_X86_CET_DISABLE		0x3002
++#define ARCH_X86_CET_LOCK		0x3003
++#define ARCH_X86_CET_ALLOC_SHSTK	0x3004
++
+ #endif /* _ASM_X86_PRCTL_H */
+diff --git a/arch/x86/kernel/Makefile b/arch/x86/kernel/Makefile
+index b83601e6b9ed..ca701b8e07c4 100644
+--- a/arch/x86/kernel/Makefile
++++ b/arch/x86/kernel/Makefile
+@@ -143,7 +143,7 @@ obj-$(CONFIG_UNWINDER_ORC)		+= unwind_orc.o
+ obj-$(CONFIG_UNWINDER_FRAME_POINTER)	+= unwind_frame.o
+ obj-$(CONFIG_UNWINDER_GUESS)		+= unwind_guess.o
+ 
+-obj-$(CONFIG_X86_INTEL_CET)		+= cet.o
++obj-$(CONFIG_X86_INTEL_CET)		+= cet.o cet_prctl.o
+ 
+ ###
+ # 64 bit specific files
+diff --git a/arch/x86/kernel/cet.c b/arch/x86/kernel/cet.c
+index 24cefffa4b70..b3f614b119eb 100644
+--- a/arch/x86/kernel/cet.c
++++ b/arch/x86/kernel/cet.c
+@@ -145,6 +145,35 @@ static int create_rstor_token(bool ia32, unsigned long ssp,
+ 	return 0;
+ }
+ 
++int cet_alloc_shstk(unsigned long *arg)
++{
++	unsigned long len = *arg;
++	unsigned long addr;
++	unsigned long token;
++	unsigned long ssp;
++
++	addr = alloc_shstk(len);
++
++	if (IS_ERR((void *)addr))
++		return PTR_ERR((void *)addr);
++
++	/* Restore token is 8 bytes and aligned to 8 bytes */
++	ssp = addr + len;
++	token = ssp;
++
++	if (!in_ia32_syscall())
++		token |= TOKEN_MODE_64;
++	ssp -= 8;
++
++	if (write_user_shstk_64(ssp, token)) {
++		vm_munmap(addr, len);
++		return -EINVAL;
++	}
++
++	*arg = addr;
++	return 0;
++}
++
+ int cet_setup_shstk(void)
+ {
+ 	unsigned long addr, size;
+diff --git a/arch/x86/kernel/cet_prctl.c b/arch/x86/kernel/cet_prctl.c
+new file mode 100644
+index 000000000000..d0717c15d535
+--- /dev/null
++++ b/arch/x86/kernel/cet_prctl.c
+@@ -0,0 +1,80 @@
++/* SPDX-License-Identifier: GPL-2.0 */
++
++#include <linux/errno.h>
++#include <linux/uaccess.h>
++#include <linux/prctl.h>
++#include <linux/compat.h>
++#include <linux/mman.h>
++#include <linux/elfcore.h>
++#include <asm/processor.h>
++#include <asm/prctl.h>
++#include <asm/cet.h>
++
++/* See Documentation/x86/intel_cet.rst. */
++
++static int handle_get_status(u64 arg2)
++{
++	struct cet_status *cet = &current->thread.cet;
++	unsigned int features = 0;
++	u64 buf[3];
++
++	if (cet->shstk_enabled)
++		features |= GNU_PROPERTY_X86_FEATURE_1_SHSTK;
++
++	buf[0] = (u64)features;
++	buf[1] = (u64)cet->shstk_base;
++	buf[2] = (u64)cet->shstk_size;
++	return copy_to_user((u64 __user *)arg2, buf, sizeof(buf));
++}
++
++static int handle_alloc_shstk(u64 arg2)
++{
++	int err = 0;
++	unsigned long arg;
++	unsigned long addr = 0;
++	unsigned long size = 0;
++
++	if (get_user(arg, (unsigned long __user *)arg2))
++		return -EFAULT;
++
++	size = arg;
++	err = cet_alloc_shstk(&arg);
++	if (err)
++		return err;
++
++	addr = arg;
++	if (put_user((u64)addr, (u64 __user *)arg2)) {
++		vm_munmap(addr, size);
++		return -EFAULT;
++	}
++
++	return 0;
++}
++
++int prctl_cet(int option, u64 arg2)
++{
++	struct cet_status *cet = &current->thread.cet;
++
++	switch (option) {
++	case ARCH_X86_CET_STATUS:
++		return handle_get_status(arg2);
++
++	case ARCH_X86_CET_DISABLE:
++		if (cet->locked)
++			return -EPERM;
++		if (arg2 & GNU_PROPERTY_X86_FEATURE_1_SHSTK)
++			cet_disable_free_shstk(current);
++
++		return 0;
++
++	case ARCH_X86_CET_LOCK:
++		cet->locked = 1;
++		return 0;
++
++	case ARCH_X86_CET_ALLOC_SHSTK:
++		return handle_alloc_shstk(arg2);
++
++	default:
++		return -EINVAL;
++	}
++}
+diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
+index ca7e6ef8e77e..e0de3c0de2f4 100644
+--- a/arch/x86/kernel/process.c
++++ b/arch/x86/kernel/process.c
+@@ -994,7 +994,7 @@ long do_arch_prctl_common(struct task_struct *task, int option,
+ 		return get_cpuid_mode();
+ 	case ARCH_SET_CPUID:
+ 		return set_cpuid_mode(task, cpuid_enabled);
++	default:
++		return prctl_cet(option, cpuid_enabled);
+ 	}
+-
+-	return -EINVAL;
+ }
+-- 
+2.26.0
+
diff --git a/0039-x86-cet-ibt-Add-Kconfig-option-for-user-mode-Indirec.patch b/0039-x86-cet-ibt-Add-Kconfig-option-for-user-mode-Indirec.patch
new file mode 100644
index 000000000..ff3e67393
--- /dev/null
+++ b/0039-x86-cet-ibt-Add-Kconfig-option-for-user-mode-Indirec.patch
@@ -0,0 +1,52 @@
+From 96c5dadd5ff45e91498fb1b46629bd6feec67ce7 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Wed, 4 Oct 2017 12:35:32 -0700
+Subject: [PATCH 39/52] x86/cet/ibt: Add Kconfig option for user-mode Indirect
+ Branch Tracking
+
+Introduce Kconfig option X86_INTEL_BRANCH_TRACKING_USER.
+
+Indirect Branch Tracking (IBT) provides protection against CALL-/JMP-
+oriented programming attacks.  It is active when the kernel has this
+feature enabled, and the processor and the application support it.
+When this feature is enabled, legacy non-IBT applications continue to
+work, but without IBT protection.
+
+v10:
+- Change build-time CET check to config depends on.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/Kconfig | 16 ++++++++++++++++
+ 1 file changed, 16 insertions(+)
+
+diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
+index 4042350acb1c..a52e44fc9bb1 100644
+--- a/arch/x86/Kconfig
++++ b/arch/x86/Kconfig
+@@ -1982,6 +1982,22 @@ config X86_INTEL_SHADOW_STACK_USER
+ 
+ 	  If unsure, say y.
+ 
++config X86_INTEL_BRANCH_TRACKING_USER
++	prompt "Intel Indirect Branch Tracking for user-mode"
++	def_bool n
++	depends on CPU_SUP_INTEL && X86_64
++	depends on $(cc-option,-fcf-protection)
++	select X86_INTEL_CET
++	---help---
++	  Indirect Branch Tracking (IBT) provides protection against
++	  CALL-/JMP-oriented programming attacks.  It is active when
++	  the kernel has this feature enabled, and the processor and
++	  the application support it.  When this feature is enabled,
++	  legacy non-IBT applications continue to work, but without
++	  IBT protection.
++
++	  If unsure, say y
++
+ config EFI
+ 	bool "EFI runtime service support"
+ 	depends on ACPI
+-- 
+2.26.0
+
diff --git a/0040-x86-cet-ibt-User-mode-Indirect-Branch-Tracking-suppo.patch b/0040-x86-cet-ibt-User-mode-Indirect-Branch-Tracking-suppo.patch
new file mode 100644
index 000000000..d92a2c3d5
--- /dev/null
+++ b/0040-x86-cet-ibt-User-mode-Indirect-Branch-Tracking-suppo.patch
@@ -0,0 +1,180 @@
+From c35663b9ce3e8b45cc25bf37aae10b12e3d15982 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Thu, 3 May 2018 13:30:56 -0700
+Subject: [PATCH 40/52] x86/cet/ibt: User-mode Indirect Branch Tracking support
+
+Introduce user-mode Indirect Branch Tracking (IBT) support.  Update setup
+routines to include IBT.
+
+v10:
+- Change no_cet_ibt to no_user_ibt.
+
+v9:
+- Change cpu_feature_enabled() to static_cpu_has().
+
+v2:
+- Change noibt to no_cet_ibt.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/include/asm/cet.h                    |  5 +++
+ arch/x86/include/asm/disabled-features.h      |  8 ++++-
+ arch/x86/kernel/cet.c                         | 33 +++++++++++++++++++
+ arch/x86/kernel/cpu/common.c                  | 17 ++++++++++
+ .../arch/x86/include/asm/disabled-features.h  |  8 ++++-
+ 5 files changed, 69 insertions(+), 2 deletions(-)
+
+diff --git a/arch/x86/include/asm/cet.h b/arch/x86/include/asm/cet.h
+index 845ffe677e9b..6b0fe0ea2ceb 100644
+--- a/arch/x86/include/asm/cet.h
++++ b/arch/x86/include/asm/cet.h
+@@ -16,6 +16,9 @@ struct cet_status {
+ 	unsigned long	shstk_size;
+ 	unsigned int	locked:1;
+ 	unsigned int	shstk_enabled:1;
++	unsigned int	ibt_enabled:1;
++	unsigned int	ibt_bitmap_used:1;
++	unsigned long	ibt_bitmap_base;
+ };
+ 
+ #ifdef CONFIG_X86_INTEL_CET
+@@ -26,6 +29,8 @@ int cet_alloc_shstk(unsigned long *arg);
+ void cet_disable_free_shstk(struct task_struct *p);
+ int cet_restore_signal(bool ia32, struct sc_ext *sc);
+ int cet_setup_signal(bool ia32, unsigned long rstor, struct sc_ext *sc);
++int cet_setup_ibt(void);
++void cet_disable_ibt(void);
+ #else
+ static inline int prctl_cet(int option, u64 arg2) { return -EINVAL; }
+ static inline int cet_setup_thread_shstk(struct task_struct *p) { return 0; }
+diff --git a/arch/x86/include/asm/disabled-features.h b/arch/x86/include/asm/disabled-features.h
+index a0e1b24cfa02..52c9c07cfacc 100644
+--- a/arch/x86/include/asm/disabled-features.h
++++ b/arch/x86/include/asm/disabled-features.h
+@@ -62,6 +62,12 @@
+ #define DISABLE_SHSTK	(1<<(X86_FEATURE_SHSTK & 31))
+ #endif
+ 
++#ifdef CONFIG_X86_INTEL_BRANCH_TRACKING_USER
++#define DISABLE_IBT	0
++#else
++#define DISABLE_IBT	(1<<(X86_FEATURE_IBT & 31))
++#endif
++
+ /*
+  * Make sure to add features to the correct mask
+  */
+@@ -83,7 +89,7 @@
+ #define DISABLED_MASK15	0
+ #define DISABLED_MASK16	(DISABLE_PKU|DISABLE_OSPKE|DISABLE_LA57|DISABLE_UMIP|DISABLE_SHSTK)
+ #define DISABLED_MASK17	0
+-#define DISABLED_MASK18	0
++#define DISABLED_MASK18	(DISABLE_IBT)
+ #define DISABLED_MASK_CHECK BUILD_BUG_ON_ZERO(NCAPINTS != 19)
+ 
+ #endif /* _ASM_X86_DISABLED_FEATURES_H */
+diff --git a/arch/x86/kernel/cet.c b/arch/x86/kernel/cet.c
+index b3f614b119eb..b04443fea5b5 100644
+--- a/arch/x86/kernel/cet.c
++++ b/arch/x86/kernel/cet.c
+@@ -13,6 +13,8 @@
+ #include <linux/uaccess.h>
+ #include <linux/sched/signal.h>
+ #include <linux/compat.h>
++#include <linux/vmalloc.h>
++#include <linux/bitops.h>
+ #include <asm/msr.h>
+ #include <asm/user.h>
+ #include <asm/fpu/internal.h>
+@@ -339,3 +341,34 @@ int cet_setup_signal(bool ia32, unsigned long rstor_addr, struct sc_ext *sc_ext)
+ 
+ 	return 0;
+ }
++
++int cet_setup_ibt(void)
++{
++	u64 msr_val;
++
++	if (!static_cpu_has(X86_FEATURE_IBT))
++		return -EOPNOTSUPP;
++
++	start_update_msrs();
++	rdmsrl(MSR_IA32_U_CET, msr_val);
++	msr_val |= (MSR_IA32_CET_ENDBR_EN | MSR_IA32_CET_NO_TRACK_EN);
++	wrmsrl(MSR_IA32_U_CET, msr_val);
++	end_update_msrs();
++	current->thread.cet.ibt_enabled = 1;
++	return 0;
++}
++
++void cet_disable_ibt(void)
++{
++	u64 msr_val;
++
++	if (!static_cpu_has(X86_FEATURE_IBT))
++		return;
++
++	start_update_msrs();
++	rdmsrl(MSR_IA32_U_CET, msr_val);
++	msr_val &= MSR_IA32_CET_SHSTK_EN;
++	wrmsrl(MSR_IA32_U_CET, msr_val);
++	end_update_msrs();
++	current->thread.cet.ibt_enabled = 0;
++}
+diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
+index 355eb6d1a6c0..b80cfa73f394 100644
+--- a/arch/x86/kernel/cpu/common.c
++++ b/arch/x86/kernel/cpu/common.c
+@@ -493,6 +493,23 @@ static __init int setup_disable_shstk(char *s)
+ __setup("no_user_shstk", setup_disable_shstk);
+ #endif
+ 
++#ifdef CONFIG_X86_INTEL_BRANCH_TRACKING_USER
++static __init int setup_disable_ibt(char *s)
++{
++	/* require an exact match without trailing characters */
++	if (s[0] != '\0')
++		return 0;
++
++	if (!boot_cpu_has(X86_FEATURE_IBT))
++		return 1;
++
++	setup_clear_cpu_cap(X86_FEATURE_IBT);
++	pr_info("x86: 'no_user_ibt' specified, disabling user Branch Tracking\n");
++	return 1;
++}
++__setup("no_user_ibt", setup_disable_ibt);
++#endif
++
+ /*
+  * Some CPU features depend on higher CPUID levels, which may not always
+  * be available due to CPUID level capping or broken virtualization
+diff --git a/tools/arch/x86/include/asm/disabled-features.h b/tools/arch/x86/include/asm/disabled-features.h
+index a0e1b24cfa02..52c9c07cfacc 100644
+--- a/tools/arch/x86/include/asm/disabled-features.h
++++ b/tools/arch/x86/include/asm/disabled-features.h
+@@ -62,6 +62,12 @@
+ #define DISABLE_SHSTK	(1<<(X86_FEATURE_SHSTK & 31))
+ #endif
+ 
++#ifdef CONFIG_X86_INTEL_BRANCH_TRACKING_USER
++#define DISABLE_IBT	0
++#else
++#define DISABLE_IBT	(1<<(X86_FEATURE_IBT & 31))
++#endif
++
+ /*
+  * Make sure to add features to the correct mask
+  */
+@@ -83,7 +89,7 @@
+ #define DISABLED_MASK15	0
+ #define DISABLED_MASK16	(DISABLE_PKU|DISABLE_OSPKE|DISABLE_LA57|DISABLE_UMIP|DISABLE_SHSTK)
+ #define DISABLED_MASK17	0
+-#define DISABLED_MASK18	0
++#define DISABLED_MASK18	(DISABLE_IBT)
+ #define DISABLED_MASK_CHECK BUILD_BUG_ON_ZERO(NCAPINTS != 19)
+ 
+ #endif /* _ASM_X86_DISABLED_FEATURES_H */
+-- 
+2.26.0
+
diff --git a/0041-x86-cet-ibt-Handle-signals-for-Indirect-Branch-Track.patch b/0041-x86-cet-ibt-Handle-signals-for-Indirect-Branch-Track.patch
new file mode 100644
index 000000000..a157df992
--- /dev/null
+++ b/0041-x86-cet-ibt-Handle-signals-for-Indirect-Branch-Track.patch
@@ -0,0 +1,108 @@
+From 499ff2f608125f06b212f46e9c95211cea6be049 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 28 May 2019 12:29:14 -0700
+Subject: [PATCH 41/52] x86/cet/ibt: Handle signals for Indirect Branch
+ Tracking
+
+Indirect Branch Tracking setting does not change in signal delivering or
+sigreturn; except the WAIT_ENDBR status.  In general, a task is in
+WAIT_ENDBR after an indirect CALL/JMP and before the next instruction
+starts.
+
+WAIT_ENDBR status can be read from MSR_IA32_U_CET.  It is reset for signal
+delivering, but preserved on a task's stack and restored for sigreturn.
+
+v9:
+- Fix missing WAIT_ENDBR in signal handling.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/kernel/cet.c        | 30 ++++++++++++++++++++++++++++--
+ arch/x86/kernel/fpu/signal.c |  8 +++++---
+ 2 files changed, 33 insertions(+), 5 deletions(-)
+
+diff --git a/arch/x86/kernel/cet.c b/arch/x86/kernel/cet.c
+index b04443fea5b5..9e5fb994e664 100644
+--- a/arch/x86/kernel/cet.c
++++ b/arch/x86/kernel/cet.c
+@@ -295,6 +295,16 @@ int cet_restore_signal(bool ia32, struct sc_ext *sc_ext)
+ 		msr_val |= MSR_IA32_CET_SHSTK_EN;
+ 	}
+ 
++	if (cet->ibt_enabled) {
++		msr_val |= (MSR_IA32_CET_ENDBR_EN | MSR_IA32_CET_NO_TRACK_EN);
++
++		if (cet->ibt_bitmap_used)
++			msr_val |= (cet->ibt_bitmap_base | MSR_IA32_CET_LEG_IW_EN);
++
++		if (sc_ext->wait_endbr)
++			msr_val |= MSR_IA32_CET_WAIT_ENDBR;
++	}
++
+ 	cet_user_state->user_cet = msr_val;
+ 	return 0;
+ }
+@@ -333,9 +343,25 @@ int cet_setup_signal(bool ia32, unsigned long rstor_addr, struct sc_ext *sc_ext)
+ 		sc_ext->ssp = new_ssp;
+ 	}
+ 
+-	if (ssp) {
++	if (ssp || cet->ibt_enabled) {
++
+ 		start_update_msrs();
+-		wrmsrl(MSR_IA32_PL3_SSP, ssp);
++
++		if (ssp)
++			wrmsrl(MSR_IA32_PL3_SSP, ssp);
++
++		if (cet->ibt_enabled) {
++			u64 r;
++
++			rdmsrl(MSR_IA32_U_CET, r);
++
++			if (r & MSR_IA32_CET_WAIT_ENDBR) {
++				sc_ext->wait_endbr = 1;
++				r &= ~MSR_IA32_CET_WAIT_ENDBR;
++				wrmsrl(MSR_IA32_U_CET, r);
++			}
++		}
++
+ 		end_update_msrs();
+ 	}
+ 
+diff --git a/arch/x86/kernel/fpu/signal.c b/arch/x86/kernel/fpu/signal.c
+index 816cd7f69b2d..90c821232bb6 100644
+--- a/arch/x86/kernel/fpu/signal.c
++++ b/arch/x86/kernel/fpu/signal.c
+@@ -57,7 +57,8 @@ int save_cet_to_sigframe(void __user *fp, unsigned long restorer, int is_ia32)
+ {
+ 	int err = 0;
+ 
+-	if (!current->thread.cet.shstk_enabled)
++	if (!current->thread.cet.shstk_enabled &&
++	    !current->thread.cet.ibt_enabled)
+ 		return 0;
+ 
+ 	if (fp) {
+@@ -87,7 +88,8 @@ static int restore_cet_from_sigframe(int is_ia32, void __user *fp)
+ {
+ 	int err = 0;
+ 
+-	if (!current->thread.cet.shstk_enabled)
++	if (!current->thread.cet.shstk_enabled &&
++	    !current->thread.cet.ibt_enabled)
+ 		return 0;
+ 
+ 	if (fp) {
+@@ -551,7 +553,7 @@ static unsigned long fpu__alloc_sigcontext_ext(unsigned long sp)
+ 	 * sigcontext_ext is at: fpu + fpu_user_xstate_size +
+ 	 * FP_XSTATE_MAGIC2_SIZE, then aligned to 8.
+ 	 */
+-	if (cet->shstk_enabled)
++	if (cet->shstk_enabled || cet->ibt_enabled)
+ 		sp -= (sizeof(struct sc_ext) + 8);
+ 
+ 	return sp;
+-- 
+2.26.0
+
diff --git a/0042-x86-cet-ibt-ELF-header-parsing-for-Indirect-Branch-T.patch b/0042-x86-cet-ibt-ELF-header-parsing-for-Indirect-Branch-T.patch
new file mode 100644
index 000000000..f87babd6b
--- /dev/null
+++ b/0042-x86-cet-ibt-ELF-header-parsing-for-Indirect-Branch-T.patch
@@ -0,0 +1,52 @@
+From 60e97b851c7cf6d45a7feb7ee77b2ed11662f8ef Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Tue, 30 Apr 2019 15:16:22 -0700
+Subject: [PATCH 42/52] x86/cet/ibt: ELF header parsing for Indirect Branch
+ Tracking
+
+Update arch_setup_elf_property() for Indirect Branch Tracking.
+
+v9:
+- Change cpu_feature_enabled() to static_cpu_has().
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/Kconfig             | 2 ++
+ arch/x86/kernel/process_64.c | 8 ++++++++
+ 2 files changed, 10 insertions(+)
+
+diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
+index a52e44fc9bb1..4318acefaff3 100644
+--- a/arch/x86/Kconfig
++++ b/arch/x86/Kconfig
+@@ -1988,6 +1988,8 @@ config X86_INTEL_BRANCH_TRACKING_USER
+ 	depends on CPU_SUP_INTEL && X86_64
+ 	depends on $(cc-option,-fcf-protection)
+ 	select X86_INTEL_CET
++	select ARCH_USE_GNU_PROPERTY
++	select ARCH_BINFMT_ELF_STATE
+ 	---help---
+ 	  Indirect Branch Tracking (IBT) provides protection against
+ 	  CALL-/JMP-oriented programming attacks.  It is active when
+diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
+index 133a917c0d35..f28d425b1df4 100644
+--- a/arch/x86/kernel/process_64.c
++++ b/arch/x86/kernel/process_64.c
+@@ -757,6 +757,14 @@ int arch_setup_elf_property(struct arch_elf_state *state)
+ 			r = cet_setup_shstk();
+ 	}
+ 
++	if (r < 0)
++		return r;
++
++	if (static_cpu_has(X86_FEATURE_IBT)) {
++		if (state->gnu_property & GNU_PROPERTY_X86_FEATURE_1_IBT)
++			r = cet_setup_ibt();
++	}
++
+ 	return r;
+ }
+ #endif
+-- 
+2.26.0
+
diff --git a/0043-x86-cet-ibt-Add-arch_prctl-functions-for-Indirect-Br.patch b/0043-x86-cet-ibt-Add-arch_prctl-functions-for-Indirect-Br.patch
new file mode 100644
index 000000000..f823c153d
--- /dev/null
+++ b/0043-x86-cet-ibt-Add-arch_prctl-functions-for-Indirect-Br.patch
@@ -0,0 +1,40 @@
+From 3b8b6af4937aea1f65b8ae2e45ad5ce27a3badd7 Mon Sep 17 00:00:00 2001
+From: "H.J. Lu" <hjl.tools@gmail.com>
+Date: Tue, 21 Aug 2018 14:13:05 -0700
+Subject: [PATCH 43/52] x86/cet/ibt: Add arch_prctl functions for Indirect
+ Branch Tracking
+
+Update ARCH_X86_CET_STATUS and ARCH_X86_CET_DISABLE for Indirect Branch
+Tracking.
+
+Signed-off-by: H.J. Lu <hjl.tools@gmail.com>
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/kernel/cet_prctl.c | 4 ++++
+ 1 file changed, 4 insertions(+)
+
+diff --git a/arch/x86/kernel/cet_prctl.c b/arch/x86/kernel/cet_prctl.c
+index d0717c15d535..3a89d20b05f4 100644
+--- a/arch/x86/kernel/cet_prctl.c
++++ b/arch/x86/kernel/cet_prctl.c
+@@ -20,6 +20,8 @@ static int handle_get_status(u64 arg2)
+ 
+ 	if (cet->shstk_enabled)
+ 		features |= GNU_PROPERTY_X86_FEATURE_1_SHSTK;
++	if (cet->ibt_enabled)
++		features |= GNU_PROPERTY_X86_FEATURE_1_IBT;
+ 
+ 	buf[0] = (u64)features;
+ 	buf[1] = (u64)cet->shstk_base;
+@@ -64,6 +66,8 @@ int prctl_cet(int option, u64 arg2)
+ 			return -EPERM;
+ 		if (arg2 & GNU_PROPERTY_X86_FEATURE_1_SHSTK)
+ 			cet_disable_free_shstk(current);
++		if (arg2 & GNU_PROPERTY_X86_FEATURE_1_IBT)
++			cet_disable_ibt();
+ 
+ 		return 0;
+ 
+-- 
+2.26.0
+
diff --git a/0044-x86-cet-Add-PTRACE-interface-for-CET.patch b/0044-x86-cet-Add-PTRACE-interface-for-CET.patch
new file mode 100644
index 000000000..25038916a
--- /dev/null
+++ b/0044-x86-cet-Add-PTRACE-interface-for-CET.patch
@@ -0,0 +1,147 @@
+From 75c9a773c26c06445bc79b898e70250d9f97249b Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Mon, 23 Apr 2018 12:55:13 -0700
+Subject: [PATCH 44/52] x86/cet: Add PTRACE interface for CET
+
+Add REGSET_CET64/REGSET_CET32 to get/set CET MSRs:
+
+    IA32_U_CET (user-mode CET settings) and
+    IA32_PL3_SSP (user-mode Shadow Stack)
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/include/asm/fpu/regset.h |  7 +++---
+ arch/x86/kernel/fpu/regset.c      | 41 +++++++++++++++++++++++++++++++
+ arch/x86/kernel/ptrace.c          | 16 ++++++++++++
+ include/uapi/linux/elf.h          |  1 +
+ 4 files changed, 62 insertions(+), 3 deletions(-)
+
+diff --git a/arch/x86/include/asm/fpu/regset.h b/arch/x86/include/asm/fpu/regset.h
+index d5bdffb9d27f..edad0d889084 100644
+--- a/arch/x86/include/asm/fpu/regset.h
++++ b/arch/x86/include/asm/fpu/regset.h
+@@ -7,11 +7,12 @@
+ 
+ #include <linux/regset.h>
+ 
+-extern user_regset_active_fn regset_fpregs_active, regset_xregset_fpregs_active;
++extern user_regset_active_fn regset_fpregs_active, regset_xregset_fpregs_active,
++				cetregs_active;
+ extern user_regset_get_fn fpregs_get, xfpregs_get, fpregs_soft_get,
+-				xstateregs_get;
++				xstateregs_get, cetregs_get;
+ extern user_regset_set_fn fpregs_set, xfpregs_set, fpregs_soft_set,
+-				 xstateregs_set;
++				 xstateregs_set, cetregs_set;
+ 
+ /*
+  * xstateregs_active == regset_fpregs_active. Please refer to the comment
+diff --git a/arch/x86/kernel/fpu/regset.c b/arch/x86/kernel/fpu/regset.c
+index bd1d0649f8ce..988a92771c8e 100644
+--- a/arch/x86/kernel/fpu/regset.c
++++ b/arch/x86/kernel/fpu/regset.c
+@@ -156,6 +156,47 @@ int xstateregs_set(struct task_struct *target, const struct user_regset *regset,
+ 	return ret;
+ }
+ 
++int cetregs_active(struct task_struct *target, const struct user_regset *regset)
++{
++#ifdef CONFIG_X86_INTEL_CET
++	if (target->thread.cet.shstk_enabled || target->thread.cet.ibt_enabled)
++		return regset->n;
++#endif
++	return 0;
++}
++
++int cetregs_get(struct task_struct *target, const struct user_regset *regset,
++		unsigned int pos, unsigned int count,
++		void *kbuf, void __user *ubuf)
++{
++	struct fpu *fpu = &target->thread.fpu;
++	struct cet_user_state *cetregs;
++
++	if (!boot_cpu_has(X86_FEATURE_SHSTK))
++		return -ENODEV;
++
++	cetregs = get_xsave_addr(&fpu->state.xsave, XFEATURE_CET_USER);
++
++	fpu__prepare_read(fpu);
++	return user_regset_copyout(&pos, &count, &kbuf, &ubuf, cetregs, 0, -1);
++}
++
++int cetregs_set(struct task_struct *target, const struct user_regset *regset,
++		  unsigned int pos, unsigned int count,
++		  const void *kbuf, const void __user *ubuf)
++{
++	struct fpu *fpu = &target->thread.fpu;
++	struct cet_user_state *cetregs;
++
++	if (!boot_cpu_has(X86_FEATURE_SHSTK))
++		return -ENODEV;
++
++	cetregs = get_xsave_addr(&fpu->state.xsave, XFEATURE_CET_USER);
++
++	fpu__prepare_write(fpu);
++	return user_regset_copyin(&pos, &count, &kbuf, &ubuf, cetregs, 0, -1);
++}
++
+ #if defined CONFIG_X86_32 || defined CONFIG_IA32_EMULATION
+ 
+ /*
+diff --git a/arch/x86/kernel/ptrace.c b/arch/x86/kernel/ptrace.c
+index f0e1ddbc2fd7..c362abdf6ef1 100644
+--- a/arch/x86/kernel/ptrace.c
++++ b/arch/x86/kernel/ptrace.c
+@@ -53,7 +53,9 @@ enum x86_regset {
+ 	REGSET_IOPERM64 = REGSET_XFP,
+ 	REGSET_XSTATE,
+ 	REGSET_TLS,
++	REGSET_CET64 = REGSET_TLS,
+ 	REGSET_IOPERM32,
++	REGSET_CET32,
+ };
+ 
+ struct pt_regs_offset {
+@@ -1255,6 +1257,13 @@ static struct user_regset x86_64_regsets[] __ro_after_init = {
+ 		.size = sizeof(long), .align = sizeof(long),
+ 		.active = ioperm_active, .get = ioperm_get
+ 	},
++	[REGSET_CET64] = {
++		.core_note_type = NT_X86_CET,
++		.n = sizeof(struct cet_user_state) / sizeof(u64),
++		.size = sizeof(u64), .align = sizeof(u64),
++		.active = cetregs_active, .get = cetregs_get,
++		.set = cetregs_set
++	},
+ };
+ 
+ static const struct user_regset_view user_x86_64_view = {
+@@ -1310,6 +1319,13 @@ static struct user_regset x86_32_regsets[] __ro_after_init = {
+ 		.size = sizeof(u32), .align = sizeof(u32),
+ 		.active = ioperm_active, .get = ioperm_get
+ 	},
++	[REGSET_CET32] = {
++		.core_note_type = NT_X86_CET,
++		.n = sizeof(struct cet_user_state) / sizeof(u64),
++		.size = sizeof(u64), .align = sizeof(u64),
++		.active = cetregs_active, .get = cetregs_get,
++		.set = cetregs_set
++	},
+ };
+ 
+ static const struct user_regset_view user_x86_32_view = {
+diff --git a/include/uapi/linux/elf.h b/include/uapi/linux/elf.h
+index 518651708d8f..793f171c37c2 100644
+--- a/include/uapi/linux/elf.h
++++ b/include/uapi/linux/elf.h
+@@ -402,6 +402,7 @@ typedef struct elf64_shdr {
+ #define NT_386_TLS	0x200		/* i386 TLS slots (struct user_desc) */
+ #define NT_386_IOPERM	0x201		/* x86 io permission bitmap (1=deny) */
+ #define NT_X86_XSTATE	0x202		/* x86 extended state using xsave */
++#define NT_X86_CET	0x203		/* x86 cet state */
+ #define NT_S390_HIGH_GPRS	0x300	/* s390 upper register halves */
+ #define NT_S390_TIMER	0x301		/* s390 timer register */
+ #define NT_S390_TODCMP	0x302		/* s390 TOD clock comparator register */
+-- 
+2.26.0
+
diff --git a/0045-x86-insn-Add-Control-flow-Enforcement-CET-instructio.patch b/0045-x86-insn-Add-Control-flow-Enforcement-CET-instructio.patch
new file mode 100644
index 000000000..480601ed0
--- /dev/null
+++ b/0045-x86-insn-Add-Control-flow-Enforcement-CET-instructio.patch
@@ -0,0 +1,165 @@
+From 2b86a9f3ab7fe75d668420565a7fc5477780ce15 Mon Sep 17 00:00:00 2001
+From: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Date: Wed, 15 Aug 2018 15:57:27 -0700
+Subject: [PATCH 45/52] x86/insn: Add Control-flow Enforcement (CET)
+ instructions to the opcode map
+
+Add the following CET instructions to the opcode map.
+
+INCSSP:
+    Increment Shadow Stack pointer (SSP).
+
+RDSSP:
+    Read SSP into a GPR.
+
+SAVEPREVSSP:
+    Use "previous ssp" token at top of current Shadow Stack (SHSTK) to
+    create a "restore token" on the previous (outgoing) SHSTK.
+
+RSTORSSP:
+    Restore from a "restore token" to SSP.
+
+WRSS:
+    Write to kernel-mode SHSTK (kernel-mode instruction).
+
+WRUSS:
+    Write to user-mode SHSTK (kernel-mode instruction).
+
+SETSSBSY:
+    Verify the "supervisor token" pointed by MSR_IA32_PL0_SSP, set the
+    token busy, and set then Shadow Stack pointer(SSP) to the value of
+    MSR_IA32_PL0_SSP.
+
+CLRSSBSY:
+    Verify the "supervisor token" and clear its busy bit.
+
+ENDBR64/ENDBR32:
+    Mark a valid 64/32 bit control transfer endpoint.
+
+Detailed information of CET instructions can be found in Intel Software
+Developer's Manual.
+
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Adrian Hunter <adrian.hunter@intel.com>
+Reviewed-by: Tony Luck <tony.luck@intel.com>
+Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
+---
+ arch/x86/lib/x86-opcode-map.txt       | 17 +++++++++++------
+ tools/arch/x86/lib/x86-opcode-map.txt | 17 +++++++++++------
+ 2 files changed, 22 insertions(+), 12 deletions(-)
+
+diff --git a/arch/x86/lib/x86-opcode-map.txt b/arch/x86/lib/x86-opcode-map.txt
+index 53adc1762ec0..ec31f5b60323 100644
+--- a/arch/x86/lib/x86-opcode-map.txt
++++ b/arch/x86/lib/x86-opcode-map.txt
+@@ -366,7 +366,7 @@ AVXcode: 1
+ 1b: BNDCN Gv,Ev (F2) | BNDMOV Ev,Gv (66) | BNDMK Gv,Ev (F3) | BNDSTX Ev,Gv
+ 1c: Grp20 (1A),(1C)
+ 1d:
+-1e:
++1e: Grp21 (1A)
+ 1f: NOP Ev
+ # 0x0f 0x20-0x2f
+ 20: MOV Rd,Cd
+@@ -803,8 +803,8 @@ f0: MOVBE Gy,My | MOVBE Gw,Mw (66) | CRC32 Gd,Eb (F2) | CRC32 Gd,Eb (66&F2)
+ f1: MOVBE My,Gy | MOVBE Mw,Gw (66) | CRC32 Gd,Ey (F2) | CRC32 Gd,Ew (66&F2)
+ f2: ANDN Gy,By,Ey (v)
+ f3: Grp17 (1A)
+-f5: BZHI Gy,Ey,By (v) | PEXT Gy,By,Ey (F3),(v) | PDEP Gy,By,Ey (F2),(v)
+-f6: ADCX Gy,Ey (66) | ADOX Gy,Ey (F3) | MULX By,Gy,rDX,Ey (F2),(v)
++f5: BZHI Gy,Ey,By (v) | PEXT Gy,By,Ey (F3),(v) | PDEP Gy,By,Ey (F2),(v) | WRUSSD/Q My,Gy (66)
++f6: ADCX Gy,Ey (66) | ADOX Gy,Ey (F3) | MULX By,Gy,rDX,Ey (F2),(v) | WRSSD/Q My,Gy
+ f7: BEXTR Gy,Ey,By (v) | SHLX Gy,Ey,By (66),(v) | SARX Gy,Ey,By (F3),(v) | SHRX Gy,Ey,By (F2),(v)
+ f8: MOVDIR64B Gv,Mdqq (66) | ENQCMD Gv,Mdqq (F2) | ENQCMDS Gv,Mdqq (F3)
+ f9: MOVDIRI My,Gy
+@@ -970,7 +970,7 @@ GrpTable: Grp7
+ 2: LGDT Ms | XGETBV (000),(11B) | XSETBV (001),(11B) | VMFUNC (100),(11B) | XEND (101)(11B) | XTEST (110)(11B) | ENCLU (111),(11B)
+ 3: LIDT Ms
+ 4: SMSW Mw/Rv
+-5: rdpkru (110),(11B) | wrpkru (111),(11B)
++5: rdpkru (110),(11B) | wrpkru (111),(11B) | SAVEPREVSSP (F3),(010),(11B) | RSTORSSP Mq (F3) | SETSSBSY (F3),(000),(11B)
+ 6: LMSW Ew
+ 7: INVLPG Mb | SWAPGS (o64),(000),(11B) | RDTSCP (001),(11B)
+ EndTable
+@@ -1041,8 +1041,8 @@ GrpTable: Grp15
+ 2: vldmxcsr Md (v1) | WRFSBASE Ry (F3),(11B)
+ 3: vstmxcsr Md (v1) | WRGSBASE Ry (F3),(11B)
+ 4: XSAVE | ptwrite Ey (F3),(11B)
+-5: XRSTOR | lfence (11B)
+-6: XSAVEOPT | clwb (66) | mfence (11B) | TPAUSE Rd (66),(11B) | UMONITOR Rv (F3),(11B) | UMWAIT Rd (F2),(11B)
++5: XRSTOR | lfence (11B) | INCSSPD/Q Ry (F3),(11B)
++6: XSAVEOPT | clwb (66) | mfence (11B) | TPAUSE Rd (66),(11B) | UMONITOR Rv (F3),(11B) | UMWAIT Rd (F2),(11B) | CLRSSBSY Mq (F3)
+ 7: clflush | clflushopt (66) | sfence (11B)
+ EndTable
+ 
+@@ -1077,6 +1077,11 @@ GrpTable: Grp20
+ 0: cldemote Mb
+ EndTable
+ 
++GrpTable: Grp21
++1: RDSSPD/Q Ry (F3),(11B)
++7: ENDBR64 (F3),(010),(11B) | ENDBR32 (F3),(011),(11B)
++EndTable
++
+ # AMD's Prefetch Group
+ GrpTable: GrpP
+ 0: PREFETCH
+diff --git a/tools/arch/x86/lib/x86-opcode-map.txt b/tools/arch/x86/lib/x86-opcode-map.txt
+index 53adc1762ec0..ec31f5b60323 100644
+--- a/tools/arch/x86/lib/x86-opcode-map.txt
++++ b/tools/arch/x86/lib/x86-opcode-map.txt
+@@ -366,7 +366,7 @@ AVXcode: 1
+ 1b: BNDCN Gv,Ev (F2) | BNDMOV Ev,Gv (66) | BNDMK Gv,Ev (F3) | BNDSTX Ev,Gv
+ 1c: Grp20 (1A),(1C)
+ 1d:
+-1e:
++1e: Grp21 (1A)
+ 1f: NOP Ev
+ # 0x0f 0x20-0x2f
+ 20: MOV Rd,Cd
+@@ -803,8 +803,8 @@ f0: MOVBE Gy,My | MOVBE Gw,Mw (66) | CRC32 Gd,Eb (F2) | CRC32 Gd,Eb (66&F2)
+ f1: MOVBE My,Gy | MOVBE Mw,Gw (66) | CRC32 Gd,Ey (F2) | CRC32 Gd,Ew (66&F2)
+ f2: ANDN Gy,By,Ey (v)
+ f3: Grp17 (1A)
+-f5: BZHI Gy,Ey,By (v) | PEXT Gy,By,Ey (F3),(v) | PDEP Gy,By,Ey (F2),(v)
+-f6: ADCX Gy,Ey (66) | ADOX Gy,Ey (F3) | MULX By,Gy,rDX,Ey (F2),(v)
++f5: BZHI Gy,Ey,By (v) | PEXT Gy,By,Ey (F3),(v) | PDEP Gy,By,Ey (F2),(v) | WRUSSD/Q My,Gy (66)
++f6: ADCX Gy,Ey (66) | ADOX Gy,Ey (F3) | MULX By,Gy,rDX,Ey (F2),(v) | WRSSD/Q My,Gy
+ f7: BEXTR Gy,Ey,By (v) | SHLX Gy,Ey,By (66),(v) | SARX Gy,Ey,By (F3),(v) | SHRX Gy,Ey,By (F2),(v)
+ f8: MOVDIR64B Gv,Mdqq (66) | ENQCMD Gv,Mdqq (F2) | ENQCMDS Gv,Mdqq (F3)
+ f9: MOVDIRI My,Gy
+@@ -970,7 +970,7 @@ GrpTable: Grp7
+ 2: LGDT Ms | XGETBV (000),(11B) | XSETBV (001),(11B) | VMFUNC (100),(11B) | XEND (101)(11B) | XTEST (110)(11B) | ENCLU (111),(11B)
+ 3: LIDT Ms
+ 4: SMSW Mw/Rv
+-5: rdpkru (110),(11B) | wrpkru (111),(11B)
++5: rdpkru (110),(11B) | wrpkru (111),(11B) | SAVEPREVSSP (F3),(010),(11B) | RSTORSSP Mq (F3) | SETSSBSY (F3),(000),(11B)
+ 6: LMSW Ew
+ 7: INVLPG Mb | SWAPGS (o64),(000),(11B) | RDTSCP (001),(11B)
+ EndTable
+@@ -1041,8 +1041,8 @@ GrpTable: Grp15
+ 2: vldmxcsr Md (v1) | WRFSBASE Ry (F3),(11B)
+ 3: vstmxcsr Md (v1) | WRGSBASE Ry (F3),(11B)
+ 4: XSAVE | ptwrite Ey (F3),(11B)
+-5: XRSTOR | lfence (11B)
+-6: XSAVEOPT | clwb (66) | mfence (11B) | TPAUSE Rd (66),(11B) | UMONITOR Rv (F3),(11B) | UMWAIT Rd (F2),(11B)
++5: XRSTOR | lfence (11B) | INCSSPD/Q Ry (F3),(11B)
++6: XSAVEOPT | clwb (66) | mfence (11B) | TPAUSE Rd (66),(11B) | UMONITOR Rv (F3),(11B) | UMWAIT Rd (F2),(11B) | CLRSSBSY Mq (F3)
+ 7: clflush | clflushopt (66) | sfence (11B)
+ EndTable
+ 
+@@ -1077,6 +1077,11 @@ GrpTable: Grp20
+ 0: cldemote Mb
+ EndTable
+ 
++GrpTable: Grp21
++1: RDSSPD/Q Ry (F3),(11B)
++7: ENDBR64 (F3),(010),(11B) | ENDBR32 (F3),(011),(11B)
++EndTable
++
+ # AMD's Prefetch Group
+ GrpTable: GrpP
+ 0: PREFETCH
+-- 
+2.26.0
+
diff --git a/0046-x86-insn-perf-tools-Add-CET-instructions-to-the-new-.patch b/0046-x86-insn-perf-tools-Add-CET-instructions-to-the-new-.patch
new file mode 100644
index 000000000..c9be185a6
--- /dev/null
+++ b/0046-x86-insn-perf-tools-Add-CET-instructions-to-the-new-.patch
@@ -0,0 +1,625 @@
+From 6a0412f9b83c74418f8fbd56640f34d18b57a09e Mon Sep 17 00:00:00 2001
+From: Adrian Hunter <adrian.hunter@intel.com>
+Date: Fri, 17 Jan 2020 15:23:47 +0200
+Subject: [PATCH 46/52] x86/insn: perf tools: Add CET instructions to the new
+ instructions test
+
+Add to the "x86 instruction decoder - new instructions" test the following
+instructions:
+
+	incsspd
+	incsspq
+	rdsspd
+	rdsspq
+	saveprevssp
+	rstorssp
+	wrssd
+	wrssq
+	wrussd
+	wrussq
+	setssbsy
+	clrssbsy
+	endbr32
+	endbr64
+
+And the notrack prefix for indirect calls and jumps.
+
+For information about the instructions, refer Intel Control-flow
+Enforcement Technology Specification May 2019 (334525-003).
+
+Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
+---
+ tools/perf/arch/x86/tests/insn-x86-dat-32.c  | 112 +++++++++
+ tools/perf/arch/x86/tests/insn-x86-dat-64.c  | 196 +++++++++++++++
+ tools/perf/arch/x86/tests/insn-x86-dat-src.c | 236 +++++++++++++++++++
+ 3 files changed, 544 insertions(+)
+
+diff --git a/tools/perf/arch/x86/tests/insn-x86-dat-32.c b/tools/perf/arch/x86/tests/insn-x86-dat-32.c
+index e6461abc9e7b..9708ae892061 100644
+--- a/tools/perf/arch/x86/tests/insn-x86-dat-32.c
++++ b/tools/perf/arch/x86/tests/insn-x86-dat-32.c
+@@ -2085,6 +2085,118 @@
+ "67 f3 0f 38 f8 1c    \tenqcmds (%si),%bx",},
+ {{0x67, 0xf3, 0x0f, 0x38, 0xf8, 0x8c, 0x34, 0x12, }, 8, 0, "", "",
+ "67 f3 0f 38 f8 8c 34 12 \tenqcmds 0x1234(%si),%cx",},
++{{0xf3, 0x0f, 0xae, 0xe8, }, 4, 0, "", "",
++"f3 0f ae e8          \tincsspd %eax",},
++{{0x0f, 0xae, 0x28, }, 3, 0, "", "",
++"0f ae 28             \txrstor (%eax)",},
++{{0x0f, 0xae, 0x2d, 0x78, 0x56, 0x34, 0x12, }, 7, 0, "", "",
++"0f ae 2d 78 56 34 12 \txrstor 0x12345678",},
++{{0x0f, 0xae, 0xac, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 8, 0, "", "",
++"0f ae ac c8 78 56 34 12 \txrstor 0x12345678(%eax,%ecx,8)",},
++{{0x0f, 0xae, 0xe8, }, 3, 0, "", "",
++"0f ae e8             \tlfence ",},
++{{0xf3, 0x0f, 0x1e, 0xc8, }, 4, 0, "", "",
++"f3 0f 1e c8          \trdsspd %eax",},
++{{0xf3, 0x0f, 0x01, 0xea, }, 4, 0, "", "",
++"f3 0f 01 ea          \tsaveprevssp ",},
++{{0xf3, 0x0f, 0x01, 0x28, }, 4, 0, "", "",
++"f3 0f 01 28          \trstorssp (%eax)",},
++{{0xf3, 0x0f, 0x01, 0x2d, 0x78, 0x56, 0x34, 0x12, }, 8, 0, "", "",
++"f3 0f 01 2d 78 56 34 12 \trstorssp 0x12345678",},
++{{0xf3, 0x0f, 0x01, 0xac, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 9, 0, "", "",
++"f3 0f 01 ac c8 78 56 34 12 \trstorssp 0x12345678(%eax,%ecx,8)",},
++{{0x0f, 0x38, 0xf6, 0x08, }, 4, 0, "", "",
++"0f 38 f6 08          \twrssd  %ecx,(%eax)",},
++{{0x0f, 0x38, 0xf6, 0x15, 0x78, 0x56, 0x34, 0x12, }, 8, 0, "", "",
++"0f 38 f6 15 78 56 34 12 \twrssd  %edx,0x12345678",},
++{{0x0f, 0x38, 0xf6, 0x94, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 9, 0, "", "",
++"0f 38 f6 94 c8 78 56 34 12 \twrssd  %edx,0x12345678(%eax,%ecx,8)",},
++{{0x66, 0x0f, 0x38, 0xf5, 0x08, }, 5, 0, "", "",
++"66 0f 38 f5 08       \twrussd %ecx,(%eax)",},
++{{0x66, 0x0f, 0x38, 0xf5, 0x15, 0x78, 0x56, 0x34, 0x12, }, 9, 0, "", "",
++"66 0f 38 f5 15 78 56 34 12 \twrussd %edx,0x12345678",},
++{{0x66, 0x0f, 0x38, 0xf5, 0x94, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 10, 0, "", "",
++"66 0f 38 f5 94 c8 78 56 34 12 \twrussd %edx,0x12345678(%eax,%ecx,8)",},
++{{0xf3, 0x0f, 0x01, 0xe8, }, 4, 0, "", "",
++"f3 0f 01 e8          \tsetssbsy ",},
++{{0x0f, 0x01, 0xee, }, 3, 0, "", "",
++"0f 01 ee             \trdpkru ",},
++{{0x0f, 0x01, 0xef, }, 3, 0, "", "",
++"0f 01 ef             \twrpkru ",},
++{{0xf3, 0x0f, 0xae, 0x30, }, 4, 0, "", "",
++"f3 0f ae 30          \tclrssbsy (%eax)",},
++{{0xf3, 0x0f, 0xae, 0x35, 0x78, 0x56, 0x34, 0x12, }, 8, 0, "", "",
++"f3 0f ae 35 78 56 34 12 \tclrssbsy 0x12345678",},
++{{0xf3, 0x0f, 0xae, 0xb4, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 9, 0, "", "",
++"f3 0f ae b4 c8 78 56 34 12 \tclrssbsy 0x12345678(%eax,%ecx,8)",},
++{{0xf3, 0x0f, 0x1e, 0xfb, }, 4, 0, "", "",
++"f3 0f 1e fb          \tendbr32 ",},
++{{0xf3, 0x0f, 0x1e, 0xfa, }, 4, 0, "", "",
++"f3 0f 1e fa          \tendbr64 ",},
++{{0xff, 0xd0, }, 2, 0, "call", "indirect",
++"ff d0                \tcall   *%eax",},
++{{0xff, 0x10, }, 2, 0, "call", "indirect",
++"ff 10                \tcall   *(%eax)",},
++{{0xff, 0x15, 0x78, 0x56, 0x34, 0x12, }, 6, 0, "call", "indirect",
++"ff 15 78 56 34 12    \tcall   *0x12345678",},
++{{0xff, 0x94, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 7, 0, "call", "indirect",
++"ff 94 c8 78 56 34 12 \tcall   *0x12345678(%eax,%ecx,8)",},
++{{0xf2, 0xff, 0xd0, }, 3, 0, "call", "indirect",
++"f2 ff d0             \tbnd call *%eax",},
++{{0xf2, 0xff, 0x10, }, 3, 0, "call", "indirect",
++"f2 ff 10             \tbnd call *(%eax)",},
++{{0xf2, 0xff, 0x15, 0x78, 0x56, 0x34, 0x12, }, 7, 0, "call", "indirect",
++"f2 ff 15 78 56 34 12 \tbnd call *0x12345678",},
++{{0xf2, 0xff, 0x94, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 8, 0, "call", "indirect",
++"f2 ff 94 c8 78 56 34 12 \tbnd call *0x12345678(%eax,%ecx,8)",},
++{{0x3e, 0xff, 0xd0, }, 3, 0, "call", "indirect",
++"3e ff d0             \tnotrack call *%eax",},
++{{0x3e, 0xff, 0x10, }, 3, 0, "call", "indirect",
++"3e ff 10             \tnotrack call *(%eax)",},
++{{0x3e, 0xff, 0x15, 0x78, 0x56, 0x34, 0x12, }, 7, 0, "call", "indirect",
++"3e ff 15 78 56 34 12 \tnotrack call *0x12345678",},
++{{0x3e, 0xff, 0x94, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 8, 0, "call", "indirect",
++"3e ff 94 c8 78 56 34 12 \tnotrack call *0x12345678(%eax,%ecx,8)",},
++{{0x3e, 0xf2, 0xff, 0xd0, }, 4, 0, "call", "indirect",
++"3e f2 ff d0          \tnotrack bnd call *%eax",},
++{{0x3e, 0xf2, 0xff, 0x10, }, 4, 0, "call", "indirect",
++"3e f2 ff 10          \tnotrack bnd call *(%eax)",},
++{{0x3e, 0xf2, 0xff, 0x15, 0x78, 0x56, 0x34, 0x12, }, 8, 0, "call", "indirect",
++"3e f2 ff 15 78 56 34 12 \tnotrack bnd call *0x12345678",},
++{{0x3e, 0xf2, 0xff, 0x94, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 9, 0, "call", "indirect",
++"3e f2 ff 94 c8 78 56 34 12 \tnotrack bnd call *0x12345678(%eax,%ecx,8)",},
++{{0xff, 0xe0, }, 2, 0, "jmp", "indirect",
++"ff e0                \tjmp    *%eax",},
++{{0xff, 0x20, }, 2, 0, "jmp", "indirect",
++"ff 20                \tjmp    *(%eax)",},
++{{0xff, 0x25, 0x78, 0x56, 0x34, 0x12, }, 6, 0, "jmp", "indirect",
++"ff 25 78 56 34 12    \tjmp    *0x12345678",},
++{{0xff, 0xa4, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 7, 0, "jmp", "indirect",
++"ff a4 c8 78 56 34 12 \tjmp    *0x12345678(%eax,%ecx,8)",},
++{{0xf2, 0xff, 0xe0, }, 3, 0, "jmp", "indirect",
++"f2 ff e0             \tbnd jmp *%eax",},
++{{0xf2, 0xff, 0x20, }, 3, 0, "jmp", "indirect",
++"f2 ff 20             \tbnd jmp *(%eax)",},
++{{0xf2, 0xff, 0x25, 0x78, 0x56, 0x34, 0x12, }, 7, 0, "jmp", "indirect",
++"f2 ff 25 78 56 34 12 \tbnd jmp *0x12345678",},
++{{0xf2, 0xff, 0xa4, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 8, 0, "jmp", "indirect",
++"f2 ff a4 c8 78 56 34 12 \tbnd jmp *0x12345678(%eax,%ecx,8)",},
++{{0x3e, 0xff, 0xe0, }, 3, 0, "jmp", "indirect",
++"3e ff e0             \tnotrack jmp *%eax",},
++{{0x3e, 0xff, 0x20, }, 3, 0, "jmp", "indirect",
++"3e ff 20             \tnotrack jmp *(%eax)",},
++{{0x3e, 0xff, 0x25, 0x78, 0x56, 0x34, 0x12, }, 7, 0, "jmp", "indirect",
++"3e ff 25 78 56 34 12 \tnotrack jmp *0x12345678",},
++{{0x3e, 0xff, 0xa4, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 8, 0, "jmp", "indirect",
++"3e ff a4 c8 78 56 34 12 \tnotrack jmp *0x12345678(%eax,%ecx,8)",},
++{{0x3e, 0xf2, 0xff, 0xe0, }, 4, 0, "jmp", "indirect",
++"3e f2 ff e0          \tnotrack bnd jmp *%eax",},
++{{0x3e, 0xf2, 0xff, 0x20, }, 4, 0, "jmp", "indirect",
++"3e f2 ff 20          \tnotrack bnd jmp *(%eax)",},
++{{0x3e, 0xf2, 0xff, 0x25, 0x78, 0x56, 0x34, 0x12, }, 8, 0, "jmp", "indirect",
++"3e f2 ff 25 78 56 34 12 \tnotrack bnd jmp *0x12345678",},
++{{0x3e, 0xf2, 0xff, 0xa4, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 9, 0, "jmp", "indirect",
++"3e f2 ff a4 c8 78 56 34 12 \tnotrack bnd jmp *0x12345678(%eax,%ecx,8)",},
+ {{0x0f, 0x01, 0xcf, }, 3, 0, "", "",
+ "0f 01 cf             \tencls  ",},
+ {{0x0f, 0x01, 0xd7, }, 3, 0, "", "",
+diff --git a/tools/perf/arch/x86/tests/insn-x86-dat-64.c b/tools/perf/arch/x86/tests/insn-x86-dat-64.c
+index 567ecccfad7c..5da17d41d302 100644
+--- a/tools/perf/arch/x86/tests/insn-x86-dat-64.c
++++ b/tools/perf/arch/x86/tests/insn-x86-dat-64.c
+@@ -2263,6 +2263,202 @@
+ "67 f3 0f 38 f8 18    \tenqcmds (%eax),%ebx",},
+ {{0x67, 0xf3, 0x0f, 0x38, 0xf8, 0x88, 0x78, 0x56, 0x34, 0x12, }, 10, 0, "", "",
+ "67 f3 0f 38 f8 88 78 56 34 12 \tenqcmds 0x12345678(%eax),%ecx",},
++{{0xf3, 0x0f, 0xae, 0xe8, }, 4, 0, "", "",
++"f3 0f ae e8          \tincsspd %eax",},
++{{0xf3, 0x41, 0x0f, 0xae, 0xe8, }, 5, 0, "", "",
++"f3 41 0f ae e8       \tincsspd %r8d",},
++{{0xf3, 0x48, 0x0f, 0xae, 0xe8, }, 5, 0, "", "",
++"f3 48 0f ae e8       \tincsspq %rax",},
++{{0xf3, 0x49, 0x0f, 0xae, 0xe8, }, 5, 0, "", "",
++"f3 49 0f ae e8       \tincsspq %r8",},
++{{0x0f, 0xae, 0x28, }, 3, 0, "", "",
++"0f ae 28             \txrstor (%rax)",},
++{{0x41, 0x0f, 0xae, 0x28, }, 4, 0, "", "",
++"41 0f ae 28          \txrstor (%r8)",},
++{{0x0f, 0xae, 0x2c, 0x25, 0x78, 0x56, 0x34, 0x12, }, 8, 0, "", "",
++"0f ae 2c 25 78 56 34 12 \txrstor 0x12345678",},
++{{0x0f, 0xae, 0xac, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 8, 0, "", "",
++"0f ae ac c8 78 56 34 12 \txrstor 0x12345678(%rax,%rcx,8)",},
++{{0x41, 0x0f, 0xae, 0xac, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 9, 0, "", "",
++"41 0f ae ac c8 78 56 34 12 \txrstor 0x12345678(%r8,%rcx,8)",},
++{{0x0f, 0xae, 0xe8, }, 3, 0, "", "",
++"0f ae e8             \tlfence ",},
++{{0xf3, 0x0f, 0x1e, 0xc8, }, 4, 0, "", "",
++"f3 0f 1e c8          \trdsspd %eax",},
++{{0xf3, 0x41, 0x0f, 0x1e, 0xc8, }, 5, 0, "", "",
++"f3 41 0f 1e c8       \trdsspd %r8d",},
++{{0xf3, 0x48, 0x0f, 0x1e, 0xc8, }, 5, 0, "", "",
++"f3 48 0f 1e c8       \trdsspq %rax",},
++{{0xf3, 0x49, 0x0f, 0x1e, 0xc8, }, 5, 0, "", "",
++"f3 49 0f 1e c8       \trdsspq %r8",},
++{{0xf3, 0x0f, 0x01, 0xea, }, 4, 0, "", "",
++"f3 0f 01 ea          \tsaveprevssp ",},
++{{0xf3, 0x0f, 0x01, 0x28, }, 4, 0, "", "",
++"f3 0f 01 28          \trstorssp (%rax)",},
++{{0xf3, 0x41, 0x0f, 0x01, 0x28, }, 5, 0, "", "",
++"f3 41 0f 01 28       \trstorssp (%r8)",},
++{{0xf3, 0x0f, 0x01, 0x2c, 0x25, 0x78, 0x56, 0x34, 0x12, }, 9, 0, "", "",
++"f3 0f 01 2c 25 78 56 34 12 \trstorssp 0x12345678",},
++{{0xf3, 0x0f, 0x01, 0xac, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 9, 0, "", "",
++"f3 0f 01 ac c8 78 56 34 12 \trstorssp 0x12345678(%rax,%rcx,8)",},
++{{0xf3, 0x41, 0x0f, 0x01, 0xac, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 10, 0, "", "",
++"f3 41 0f 01 ac c8 78 56 34 12 \trstorssp 0x12345678(%r8,%rcx,8)",},
++{{0x0f, 0x38, 0xf6, 0x08, }, 4, 0, "", "",
++"0f 38 f6 08          \twrssd  %ecx,(%rax)",},
++{{0x41, 0x0f, 0x38, 0xf6, 0x10, }, 5, 0, "", "",
++"41 0f 38 f6 10       \twrssd  %edx,(%r8)",},
++{{0x0f, 0x38, 0xf6, 0x14, 0x25, 0x78, 0x56, 0x34, 0x12, }, 9, 0, "", "",
++"0f 38 f6 14 25 78 56 34 12 \twrssd  %edx,0x12345678",},
++{{0x0f, 0x38, 0xf6, 0x94, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 9, 0, "", "",
++"0f 38 f6 94 c8 78 56 34 12 \twrssd  %edx,0x12345678(%rax,%rcx,8)",},
++{{0x41, 0x0f, 0x38, 0xf6, 0x94, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 10, 0, "", "",
++"41 0f 38 f6 94 c8 78 56 34 12 \twrssd  %edx,0x12345678(%r8,%rcx,8)",},
++{{0x48, 0x0f, 0x38, 0xf6, 0x08, }, 5, 0, "", "",
++"48 0f 38 f6 08       \twrssq  %rcx,(%rax)",},
++{{0x49, 0x0f, 0x38, 0xf6, 0x10, }, 5, 0, "", "",
++"49 0f 38 f6 10       \twrssq  %rdx,(%r8)",},
++{{0x48, 0x0f, 0x38, 0xf6, 0x14, 0x25, 0x78, 0x56, 0x34, 0x12, }, 10, 0, "", "",
++"48 0f 38 f6 14 25 78 56 34 12 \twrssq  %rdx,0x12345678",},
++{{0x48, 0x0f, 0x38, 0xf6, 0x94, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 10, 0, "", "",
++"48 0f 38 f6 94 c8 78 56 34 12 \twrssq  %rdx,0x12345678(%rax,%rcx,8)",},
++{{0x49, 0x0f, 0x38, 0xf6, 0x94, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 10, 0, "", "",
++"49 0f 38 f6 94 c8 78 56 34 12 \twrssq  %rdx,0x12345678(%r8,%rcx,8)",},
++{{0x66, 0x0f, 0x38, 0xf5, 0x08, }, 5, 0, "", "",
++"66 0f 38 f5 08       \twrussd %ecx,(%rax)",},
++{{0x66, 0x41, 0x0f, 0x38, 0xf5, 0x10, }, 6, 0, "", "",
++"66 41 0f 38 f5 10    \twrussd %edx,(%r8)",},
++{{0x66, 0x0f, 0x38, 0xf5, 0x14, 0x25, 0x78, 0x56, 0x34, 0x12, }, 10, 0, "", "",
++"66 0f 38 f5 14 25 78 56 34 12 \twrussd %edx,0x12345678",},
++{{0x66, 0x0f, 0x38, 0xf5, 0x94, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 10, 0, "", "",
++"66 0f 38 f5 94 c8 78 56 34 12 \twrussd %edx,0x12345678(%rax,%rcx,8)",},
++{{0x66, 0x41, 0x0f, 0x38, 0xf5, 0x94, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 11, 0, "", "",
++"66 41 0f 38 f5 94 c8 78 56 34 12 \twrussd %edx,0x12345678(%r8,%rcx,8)",},
++{{0x66, 0x48, 0x0f, 0x38, 0xf5, 0x08, }, 6, 0, "", "",
++"66 48 0f 38 f5 08    \twrussq %rcx,(%rax)",},
++{{0x66, 0x49, 0x0f, 0x38, 0xf5, 0x10, }, 6, 0, "", "",
++"66 49 0f 38 f5 10    \twrussq %rdx,(%r8)",},
++{{0x66, 0x48, 0x0f, 0x38, 0xf5, 0x14, 0x25, 0x78, 0x56, 0x34, 0x12, }, 11, 0, "", "",
++"66 48 0f 38 f5 14 25 78 56 34 12 \twrussq %rdx,0x12345678",},
++{{0x66, 0x48, 0x0f, 0x38, 0xf5, 0x94, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 11, 0, "", "",
++"66 48 0f 38 f5 94 c8 78 56 34 12 \twrussq %rdx,0x12345678(%rax,%rcx,8)",},
++{{0x66, 0x49, 0x0f, 0x38, 0xf5, 0x94, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 11, 0, "", "",
++"66 49 0f 38 f5 94 c8 78 56 34 12 \twrussq %rdx,0x12345678(%r8,%rcx,8)",},
++{{0xf3, 0x0f, 0x01, 0xe8, }, 4, 0, "", "",
++"f3 0f 01 e8          \tsetssbsy ",},
++{{0x0f, 0x01, 0xee, }, 3, 0, "", "",
++"0f 01 ee             \trdpkru ",},
++{{0x0f, 0x01, 0xef, }, 3, 0, "", "",
++"0f 01 ef             \twrpkru ",},
++{{0xf3, 0x0f, 0xae, 0x30, }, 4, 0, "", "",
++"f3 0f ae 30          \tclrssbsy (%rax)",},
++{{0xf3, 0x41, 0x0f, 0xae, 0x30, }, 5, 0, "", "",
++"f3 41 0f ae 30       \tclrssbsy (%r8)",},
++{{0xf3, 0x0f, 0xae, 0x34, 0x25, 0x78, 0x56, 0x34, 0x12, }, 9, 0, "", "",
++"f3 0f ae 34 25 78 56 34 12 \tclrssbsy 0x12345678",},
++{{0xf3, 0x0f, 0xae, 0xb4, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 9, 0, "", "",
++"f3 0f ae b4 c8 78 56 34 12 \tclrssbsy 0x12345678(%rax,%rcx,8)",},
++{{0xf3, 0x41, 0x0f, 0xae, 0xb4, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 10, 0, "", "",
++"f3 41 0f ae b4 c8 78 56 34 12 \tclrssbsy 0x12345678(%r8,%rcx,8)",},
++{{0xf3, 0x0f, 0x1e, 0xfb, }, 4, 0, "", "",
++"f3 0f 1e fb          \tendbr32 ",},
++{{0xf3, 0x0f, 0x1e, 0xfa, }, 4, 0, "", "",
++"f3 0f 1e fa          \tendbr64 ",},
++{{0xff, 0xd0, }, 2, 0, "call", "indirect",
++"ff d0                \tcallq  *%rax",},
++{{0xff, 0x10, }, 2, 0, "call", "indirect",
++"ff 10                \tcallq  *(%rax)",},
++{{0x41, 0xff, 0x10, }, 3, 0, "call", "indirect",
++"41 ff 10             \tcallq  *(%r8)",},
++{{0xff, 0x14, 0x25, 0x78, 0x56, 0x34, 0x12, }, 7, 0, "call", "indirect",
++"ff 14 25 78 56 34 12 \tcallq  *0x12345678",},
++{{0xff, 0x94, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 7, 0, "call", "indirect",
++"ff 94 c8 78 56 34 12 \tcallq  *0x12345678(%rax,%rcx,8)",},
++{{0x41, 0xff, 0x94, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 8, 0, "call", "indirect",
++"41 ff 94 c8 78 56 34 12 \tcallq  *0x12345678(%r8,%rcx,8)",},
++{{0xf2, 0xff, 0xd0, }, 3, 0, "call", "indirect",
++"f2 ff d0             \tbnd callq *%rax",},
++{{0xf2, 0xff, 0x10, }, 3, 0, "call", "indirect",
++"f2 ff 10             \tbnd callq *(%rax)",},
++{{0xf2, 0x41, 0xff, 0x10, }, 4, 0, "call", "indirect",
++"f2 41 ff 10          \tbnd callq *(%r8)",},
++{{0xf2, 0xff, 0x14, 0x25, 0x78, 0x56, 0x34, 0x12, }, 8, 0, "call", "indirect",
++"f2 ff 14 25 78 56 34 12 \tbnd callq *0x12345678",},
++{{0xf2, 0xff, 0x94, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 8, 0, "call", "indirect",
++"f2 ff 94 c8 78 56 34 12 \tbnd callq *0x12345678(%rax,%rcx,8)",},
++{{0xf2, 0x41, 0xff, 0x94, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 9, 0, "call", "indirect",
++"f2 41 ff 94 c8 78 56 34 12 \tbnd callq *0x12345678(%r8,%rcx,8)",},
++{{0x3e, 0xff, 0xd0, }, 3, 0, "call", "indirect",
++"3e ff d0             \tnotrack callq *%rax",},
++{{0x3e, 0xff, 0x10, }, 3, 0, "call", "indirect",
++"3e ff 10             \tnotrack callq *(%rax)",},
++{{0x3e, 0x41, 0xff, 0x10, }, 4, 0, "call", "indirect",
++"3e 41 ff 10          \tnotrack callq *(%r8)",},
++{{0x3e, 0xff, 0x14, 0x25, 0x78, 0x56, 0x34, 0x12, }, 8, 0, "call", "indirect",
++"3e ff 14 25 78 56 34 12 \tnotrack callq *0x12345678",},
++{{0x3e, 0xff, 0x94, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 8, 0, "call", "indirect",
++"3e ff 94 c8 78 56 34 12 \tnotrack callq *0x12345678(%rax,%rcx,8)",},
++{{0x3e, 0x41, 0xff, 0x94, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 9, 0, "call", "indirect",
++"3e 41 ff 94 c8 78 56 34 12 \tnotrack callq *0x12345678(%r8,%rcx,8)",},
++{{0x3e, 0xf2, 0xff, 0xd0, }, 4, 0, "call", "indirect",
++"3e f2 ff d0          \tnotrack bnd callq *%rax",},
++{{0x3e, 0xf2, 0xff, 0x10, }, 4, 0, "call", "indirect",
++"3e f2 ff 10          \tnotrack bnd callq *(%rax)",},
++{{0x3e, 0xf2, 0x41, 0xff, 0x10, }, 5, 0, "call", "indirect",
++"3e f2 41 ff 10       \tnotrack bnd callq *(%r8)",},
++{{0x3e, 0xf2, 0xff, 0x14, 0x25, 0x78, 0x56, 0x34, 0x12, }, 9, 0, "call", "indirect",
++"3e f2 ff 14 25 78 56 34 12 \tnotrack bnd callq *0x12345678",},
++{{0x3e, 0xf2, 0xff, 0x94, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 9, 0, "call", "indirect",
++"3e f2 ff 94 c8 78 56 34 12 \tnotrack bnd callq *0x12345678(%rax,%rcx,8)",},
++{{0x3e, 0xf2, 0x41, 0xff, 0x94, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 10, 0, "call", "indirect",
++"3e f2 41 ff 94 c8 78 56 34 12 \tnotrack bnd callq *0x12345678(%r8,%rcx,8)",},
++{{0xff, 0xe0, }, 2, 0, "jmp", "indirect",
++"ff e0                \tjmpq   *%rax",},
++{{0xff, 0x20, }, 2, 0, "jmp", "indirect",
++"ff 20                \tjmpq   *(%rax)",},
++{{0x41, 0xff, 0x20, }, 3, 0, "jmp", "indirect",
++"41 ff 20             \tjmpq   *(%r8)",},
++{{0xff, 0x24, 0x25, 0x78, 0x56, 0x34, 0x12, }, 7, 0, "jmp", "indirect",
++"ff 24 25 78 56 34 12 \tjmpq   *0x12345678",},
++{{0xff, 0xa4, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 7, 0, "jmp", "indirect",
++"ff a4 c8 78 56 34 12 \tjmpq   *0x12345678(%rax,%rcx,8)",},
++{{0x41, 0xff, 0xa4, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 8, 0, "jmp", "indirect",
++"41 ff a4 c8 78 56 34 12 \tjmpq   *0x12345678(%r8,%rcx,8)",},
++{{0xf2, 0xff, 0xe0, }, 3, 0, "jmp", "indirect",
++"f2 ff e0             \tbnd jmpq *%rax",},
++{{0xf2, 0xff, 0x20, }, 3, 0, "jmp", "indirect",
++"f2 ff 20             \tbnd jmpq *(%rax)",},
++{{0xf2, 0x41, 0xff, 0x20, }, 4, 0, "jmp", "indirect",
++"f2 41 ff 20          \tbnd jmpq *(%r8)",},
++{{0xf2, 0xff, 0x24, 0x25, 0x78, 0x56, 0x34, 0x12, }, 8, 0, "jmp", "indirect",
++"f2 ff 24 25 78 56 34 12 \tbnd jmpq *0x12345678",},
++{{0xf2, 0xff, 0xa4, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 8, 0, "jmp", "indirect",
++"f2 ff a4 c8 78 56 34 12 \tbnd jmpq *0x12345678(%rax,%rcx,8)",},
++{{0xf2, 0x41, 0xff, 0xa4, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 9, 0, "jmp", "indirect",
++"f2 41 ff a4 c8 78 56 34 12 \tbnd jmpq *0x12345678(%r8,%rcx,8)",},
++{{0x3e, 0xff, 0xe0, }, 3, 0, "jmp", "indirect",
++"3e ff e0             \tnotrack jmpq *%rax",},
++{{0x3e, 0xff, 0x20, }, 3, 0, "jmp", "indirect",
++"3e ff 20             \tnotrack jmpq *(%rax)",},
++{{0x3e, 0x41, 0xff, 0x20, }, 4, 0, "jmp", "indirect",
++"3e 41 ff 20          \tnotrack jmpq *(%r8)",},
++{{0x3e, 0xff, 0x24, 0x25, 0x78, 0x56, 0x34, 0x12, }, 8, 0, "jmp", "indirect",
++"3e ff 24 25 78 56 34 12 \tnotrack jmpq *0x12345678",},
++{{0x3e, 0xff, 0xa4, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 8, 0, "jmp", "indirect",
++"3e ff a4 c8 78 56 34 12 \tnotrack jmpq *0x12345678(%rax,%rcx,8)",},
++{{0x3e, 0x41, 0xff, 0xa4, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 9, 0, "jmp", "indirect",
++"3e 41 ff a4 c8 78 56 34 12 \tnotrack jmpq *0x12345678(%r8,%rcx,8)",},
++{{0x3e, 0xf2, 0xff, 0xe0, }, 4, 0, "jmp", "indirect",
++"3e f2 ff e0          \tnotrack bnd jmpq *%rax",},
++{{0x3e, 0xf2, 0xff, 0x20, }, 4, 0, "jmp", "indirect",
++"3e f2 ff 20          \tnotrack bnd jmpq *(%rax)",},
++{{0x3e, 0xf2, 0x41, 0xff, 0x20, }, 5, 0, "jmp", "indirect",
++"3e f2 41 ff 20       \tnotrack bnd jmpq *(%r8)",},
++{{0x3e, 0xf2, 0xff, 0x24, 0x25, 0x78, 0x56, 0x34, 0x12, }, 9, 0, "jmp", "indirect",
++"3e f2 ff 24 25 78 56 34 12 \tnotrack bnd jmpq *0x12345678",},
++{{0x3e, 0xf2, 0xff, 0xa4, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 9, 0, "jmp", "indirect",
++"3e f2 ff a4 c8 78 56 34 12 \tnotrack bnd jmpq *0x12345678(%rax,%rcx,8)",},
++{{0x3e, 0xf2, 0x41, 0xff, 0xa4, 0xc8, 0x78, 0x56, 0x34, 0x12, }, 10, 0, "jmp", "indirect",
++"3e f2 41 ff a4 c8 78 56 34 12 \tnotrack bnd jmpq *0x12345678(%r8,%rcx,8)",},
+ {{0x0f, 0x01, 0xcf, }, 3, 0, "", "",
+ "0f 01 cf             \tencls  ",},
+ {{0x0f, 0x01, 0xd7, }, 3, 0, "", "",
+diff --git a/tools/perf/arch/x86/tests/insn-x86-dat-src.c b/tools/perf/arch/x86/tests/insn-x86-dat-src.c
+index ddbf07c50bb8..c3808e94c46e 100644
+--- a/tools/perf/arch/x86/tests/insn-x86-dat-src.c
++++ b/tools/perf/arch/x86/tests/insn-x86-dat-src.c
+@@ -1771,6 +1771,145 @@ int main(void)
+ 	asm volatile("enqcmds (%eax),%ebx");
+ 	asm volatile("enqcmds 0x12345678(%eax),%ecx");
+ 
++	/* incsspd/q */
++
++	asm volatile("incsspd %eax");
++	asm volatile("incsspd %r8d");
++	asm volatile("incsspq %rax");
++	asm volatile("incsspq %r8");
++	/* Also check instructions in the same group encoding as incsspd/q */
++	asm volatile("xrstor (%rax)");
++	asm volatile("xrstor (%r8)");
++	asm volatile("xrstor (0x12345678)");
++	asm volatile("xrstor 0x12345678(%rax,%rcx,8)");
++	asm volatile("xrstor 0x12345678(%r8,%rcx,8)");
++	asm volatile("lfence");
++
++	/* rdsspd/q */
++
++	asm volatile("rdsspd %eax");
++	asm volatile("rdsspd %r8d");
++	asm volatile("rdsspq %rax");
++	asm volatile("rdsspq %r8");
++
++	/* saveprevssp */
++
++	asm volatile("saveprevssp");
++
++	/* rstorssp */
++
++	asm volatile("rstorssp (%rax)");
++	asm volatile("rstorssp (%r8)");
++	asm volatile("rstorssp (0x12345678)");
++	asm volatile("rstorssp 0x12345678(%rax,%rcx,8)");
++	asm volatile("rstorssp 0x12345678(%r8,%rcx,8)");
++
++	/* wrssd/q */
++
++	asm volatile("wrssd %ecx,(%rax)");
++	asm volatile("wrssd %edx,(%r8)");
++	asm volatile("wrssd %edx,(0x12345678)");
++	asm volatile("wrssd %edx,0x12345678(%rax,%rcx,8)");
++	asm volatile("wrssd %edx,0x12345678(%r8,%rcx,8)");
++	asm volatile("wrssq %rcx,(%rax)");
++	asm volatile("wrssq %rdx,(%r8)");
++	asm volatile("wrssq %rdx,(0x12345678)");
++	asm volatile("wrssq %rdx,0x12345678(%rax,%rcx,8)");
++	asm volatile("wrssq %rdx,0x12345678(%r8,%rcx,8)");
++
++	/* wrussd/q */
++
++	asm volatile("wrussd %ecx,(%rax)");
++	asm volatile("wrussd %edx,(%r8)");
++	asm volatile("wrussd %edx,(0x12345678)");
++	asm volatile("wrussd %edx,0x12345678(%rax,%rcx,8)");
++	asm volatile("wrussd %edx,0x12345678(%r8,%rcx,8)");
++	asm volatile("wrussq %rcx,(%rax)");
++	asm volatile("wrussq %rdx,(%r8)");
++	asm volatile("wrussq %rdx,(0x12345678)");
++	asm volatile("wrussq %rdx,0x12345678(%rax,%rcx,8)");
++	asm volatile("wrussq %rdx,0x12345678(%r8,%rcx,8)");
++
++	/* setssbsy */
++
++	asm volatile("setssbsy");
++	/* Also check instructions in the same group encoding as setssbsy */
++	asm volatile("rdpkru");
++	asm volatile("wrpkru");
++
++	/* clrssbsy */
++
++	asm volatile("clrssbsy (%rax)");
++	asm volatile("clrssbsy (%r8)");
++	asm volatile("clrssbsy (0x12345678)");
++	asm volatile("clrssbsy 0x12345678(%rax,%rcx,8)");
++	asm volatile("clrssbsy 0x12345678(%r8,%rcx,8)");
++
++	/* endbr32/64 */
++
++	asm volatile("endbr32");
++	asm volatile("endbr64");
++
++	/* call with/without notrack prefix */
++
++	asm volatile("callq *%rax");				/* Expecting: call indirect 0 */
++	asm volatile("callq *(%rax)");				/* Expecting: call indirect 0 */
++	asm volatile("callq *(%r8)");				/* Expecting: call indirect 0 */
++	asm volatile("callq *(0x12345678)");			/* Expecting: call indirect 0 */
++	asm volatile("callq *0x12345678(%rax,%rcx,8)");		/* Expecting: call indirect 0 */
++	asm volatile("callq *0x12345678(%r8,%rcx,8)");		/* Expecting: call indirect 0 */
++
++	asm volatile("bnd callq *%rax");			/* Expecting: call indirect 0 */
++	asm volatile("bnd callq *(%rax)");			/* Expecting: call indirect 0 */
++	asm volatile("bnd callq *(%r8)");			/* Expecting: call indirect 0 */
++	asm volatile("bnd callq *(0x12345678)");		/* Expecting: call indirect 0 */
++	asm volatile("bnd callq *0x12345678(%rax,%rcx,8)");	/* Expecting: call indirect 0 */
++	asm volatile("bnd callq *0x12345678(%r8,%rcx,8)");	/* Expecting: call indirect 0 */
++
++	asm volatile("notrack callq *%rax");			/* Expecting: call indirect 0 */
++	asm volatile("notrack callq *(%rax)");			/* Expecting: call indirect 0 */
++	asm volatile("notrack callq *(%r8)");			/* Expecting: call indirect 0 */
++	asm volatile("notrack callq *(0x12345678)");		/* Expecting: call indirect 0 */
++	asm volatile("notrack callq *0x12345678(%rax,%rcx,8)");	/* Expecting: call indirect 0 */
++	asm volatile("notrack callq *0x12345678(%r8,%rcx,8)");	/* Expecting: call indirect 0 */
++
++	asm volatile("notrack bnd callq *%rax");		/* Expecting: call indirect 0 */
++	asm volatile("notrack bnd callq *(%rax)");		/* Expecting: call indirect 0 */
++	asm volatile("notrack bnd callq *(%r8)");		/* Expecting: call indirect 0 */
++	asm volatile("notrack bnd callq *(0x12345678)");	/* Expecting: call indirect 0 */
++	asm volatile("notrack bnd callq *0x12345678(%rax,%rcx,8)");	/* Expecting: call indirect 0 */
++	asm volatile("notrack bnd callq *0x12345678(%r8,%rcx,8)");	/* Expecting: call indirect 0 */
++
++	/* jmp with/without notrack prefix */
++
++	asm volatile("jmpq *%rax");				/* Expecting: jmp indirect 0 */
++	asm volatile("jmpq *(%rax)");				/* Expecting: jmp indirect 0 */
++	asm volatile("jmpq *(%r8)");				/* Expecting: jmp indirect 0 */
++	asm volatile("jmpq *(0x12345678)");			/* Expecting: jmp indirect 0 */
++	asm volatile("jmpq *0x12345678(%rax,%rcx,8)");		/* Expecting: jmp indirect 0 */
++	asm volatile("jmpq *0x12345678(%r8,%rcx,8)");		/* Expecting: jmp indirect 0 */
++
++	asm volatile("bnd jmpq *%rax");				/* Expecting: jmp indirect 0 */
++	asm volatile("bnd jmpq *(%rax)");			/* Expecting: jmp indirect 0 */
++	asm volatile("bnd jmpq *(%r8)");			/* Expecting: jmp indirect 0 */
++	asm volatile("bnd jmpq *(0x12345678)");			/* Expecting: jmp indirect 0 */
++	asm volatile("bnd jmpq *0x12345678(%rax,%rcx,8)");	/* Expecting: jmp indirect 0 */
++	asm volatile("bnd jmpq *0x12345678(%r8,%rcx,8)");	/* Expecting: jmp indirect 0 */
++
++	asm volatile("notrack jmpq *%rax");			/* Expecting: jmp indirect 0 */
++	asm volatile("notrack jmpq *(%rax)");			/* Expecting: jmp indirect 0 */
++	asm volatile("notrack jmpq *(%r8)");			/* Expecting: jmp indirect 0 */
++	asm volatile("notrack jmpq *(0x12345678)");		/* Expecting: jmp indirect 0 */
++	asm volatile("notrack jmpq *0x12345678(%rax,%rcx,8)");	/* Expecting: jmp indirect 0 */
++	asm volatile("notrack jmpq *0x12345678(%r8,%rcx,8)");	/* Expecting: jmp indirect 0 */
++
++	asm volatile("notrack bnd jmpq *%rax");			/* Expecting: jmp indirect 0 */
++	asm volatile("notrack bnd jmpq *(%rax)");		/* Expecting: jmp indirect 0 */
++	asm volatile("notrack bnd jmpq *(%r8)");		/* Expecting: jmp indirect 0 */
++	asm volatile("notrack bnd jmpq *(0x12345678)");		/* Expecting: jmp indirect 0 */
++	asm volatile("notrack bnd jmpq *0x12345678(%rax,%rcx,8)");	/* Expecting: jmp indirect 0 */
++	asm volatile("notrack bnd jmpq *0x12345678(%r8,%rcx,8)");	/* Expecting: jmp indirect 0 */
++
+ #else  /* #ifdef __x86_64__ */
+ 
+ 	/* bound r32, mem (same op code as EVEX prefix) */
+@@ -3434,6 +3573,103 @@ int main(void)
+ 	asm volatile("enqcmds (%si),%bx");
+ 	asm volatile("enqcmds 0x1234(%si),%cx");
+ 
++	/* incsspd */
++
++	asm volatile("incsspd %eax");
++	/* Also check instructions in the same group encoding as incsspd */
++	asm volatile("xrstor (%eax)");
++	asm volatile("xrstor (0x12345678)");
++	asm volatile("xrstor 0x12345678(%eax,%ecx,8)");
++	asm volatile("lfence");
++
++	/* rdsspd */
++
++	asm volatile("rdsspd %eax");
++
++	/* saveprevssp */
++
++	asm volatile("saveprevssp");
++
++	/* rstorssp */
++
++	asm volatile("rstorssp (%eax)");
++	asm volatile("rstorssp (0x12345678)");
++	asm volatile("rstorssp 0x12345678(%eax,%ecx,8)");
++
++	/* wrssd */
++
++	asm volatile("wrssd %ecx,(%eax)");
++	asm volatile("wrssd %edx,(0x12345678)");
++	asm volatile("wrssd %edx,0x12345678(%eax,%ecx,8)");
++
++	/* wrussd */
++
++	asm volatile("wrussd %ecx,(%eax)");
++	asm volatile("wrussd %edx,(0x12345678)");
++	asm volatile("wrussd %edx,0x12345678(%eax,%ecx,8)");
++
++	/* setssbsy */
++
++	asm volatile("setssbsy");
++	/* Also check instructions in the same group encoding as setssbsy */
++	asm volatile("rdpkru");
++	asm volatile("wrpkru");
++
++	/* clrssbsy */
++
++	asm volatile("clrssbsy (%eax)");
++	asm volatile("clrssbsy (0x12345678)");
++	asm volatile("clrssbsy 0x12345678(%eax,%ecx,8)");
++
++	/* endbr32/64 */
++
++	asm volatile("endbr32");
++	asm volatile("endbr64");
++
++	/* call with/without notrack prefix */
++
++	asm volatile("call *%eax");				/* Expecting: call indirect 0 */
++	asm volatile("call *(%eax)");				/* Expecting: call indirect 0 */
++	asm volatile("call *(0x12345678)");			/* Expecting: call indirect 0 */
++	asm volatile("call *0x12345678(%eax,%ecx,8)");		/* Expecting: call indirect 0 */
++
++	asm volatile("bnd call *%eax");				/* Expecting: call indirect 0 */
++	asm volatile("bnd call *(%eax)");			/* Expecting: call indirect 0 */
++	asm volatile("bnd call *(0x12345678)");			/* Expecting: call indirect 0 */
++	asm volatile("bnd call *0x12345678(%eax,%ecx,8)");	/* Expecting: call indirect 0 */
++
++	asm volatile("notrack call *%eax");			/* Expecting: call indirect 0 */
++	asm volatile("notrack call *(%eax)");			/* Expecting: call indirect 0 */
++	asm volatile("notrack call *(0x12345678)");		/* Expecting: call indirect 0 */
++	asm volatile("notrack call *0x12345678(%eax,%ecx,8)");	/* Expecting: call indirect 0 */
++
++	asm volatile("notrack bnd call *%eax");			/* Expecting: call indirect 0 */
++	asm volatile("notrack bnd call *(%eax)");		/* Expecting: call indirect 0 */
++	asm volatile("notrack bnd call *(0x12345678)");		/* Expecting: call indirect 0 */
++	asm volatile("notrack bnd call *0x12345678(%eax,%ecx,8)"); /* Expecting: call indirect 0 */
++
++	/* jmp with/without notrack prefix */
++
++	asm volatile("jmp *%eax");				/* Expecting: jmp indirect 0 */
++	asm volatile("jmp *(%eax)");				/* Expecting: jmp indirect 0 */
++	asm volatile("jmp *(0x12345678)");			/* Expecting: jmp indirect 0 */
++	asm volatile("jmp *0x12345678(%eax,%ecx,8)");		/* Expecting: jmp indirect 0 */
++
++	asm volatile("bnd jmp *%eax");				/* Expecting: jmp indirect 0 */
++	asm volatile("bnd jmp *(%eax)");			/* Expecting: jmp indirect 0 */
++	asm volatile("bnd jmp *(0x12345678)");			/* Expecting: jmp indirect 0 */
++	asm volatile("bnd jmp *0x12345678(%eax,%ecx,8)");	/* Expecting: jmp indirect 0 */
++
++	asm volatile("notrack jmp *%eax");			/* Expecting: jmp indirect 0 */
++	asm volatile("notrack jmp *(%eax)");			/* Expecting: jmp indirect 0 */
++	asm volatile("notrack jmp *(0x12345678)");		/* Expecting: jmp indirect 0 */
++	asm volatile("notrack jmp *0x12345678(%eax,%ecx,8)");	/* Expecting: jmp indirect 0 */
++
++	asm volatile("notrack bnd jmp *%eax");			/* Expecting: jmp indirect 0 */
++	asm volatile("notrack bnd jmp *(%eax)");		/* Expecting: jmp indirect 0 */
++	asm volatile("notrack bnd jmp *(0x12345678)");		/* Expecting: jmp indirect 0 */
++	asm volatile("notrack bnd jmp *0x12345678(%eax,%ecx,8)"); /* Expecting: jmp indirect 0 */
++
+ #endif /* #ifndef __x86_64__ */
+ 
+ 	/* SGX */
+-- 
+2.26.0
+
diff --git a/0047-x86-Discard-.note.gnu.property-sections-in-vDSO.patch b/0047-x86-Discard-.note.gnu.property-sections-in-vDSO.patch
new file mode 100644
index 000000000..99e2ced1b
--- /dev/null
+++ b/0047-x86-Discard-.note.gnu.property-sections-in-vDSO.patch
@@ -0,0 +1,69 @@
+From 47314f4b4c4307702e5c98ab30e7e74be3295b4e Mon Sep 17 00:00:00 2001
+From: "H.J. Lu" <hjl.tools@gmail.com>
+Date: Fri, 24 Jan 2020 09:37:03 -0800
+Subject: [PATCH 47/52] x86: Discard .note.gnu.property sections in vDSO
+
+With the command-line option, -mx86-used-note=yes, the x86 assembler
+in binutils 2.32 and above generates a program property note in a note
+section, .note.gnu.property, to encode used x86 ISAs and features.  But
+x86 kernel vDSO linker script only contains a single NOTE segment:
+
+PHDRS
+{
+ text PT_LOAD FLAGS(5) FILEHDR PHDRS; /* PF_R|PF_X */
+ dynamic PT_DYNAMIC FLAGS(4); /* PF_R */
+ note PT_NOTE FLAGS(4); /* PF_R */
+ eh_frame_hdr 0x6474e550;
+}
+
+The NOTE segment generated by vDSO linker script is aligned to 4 bytes.
+But .note.gnu.property section must be aligned to 8 bytes on x86-64 and
+we get
+
+[hjl@gnu-skx-1 vdso]$ readelf -n vdso64.so
+
+Displaying notes found in: .note
+  Owner                Data size 	Description
+  Linux                0x00000004	Unknown note type: (0x00000000)
+   description data: 06 00 00 00
+readelf: Warning: note with invalid namesz and/or descsz found at offset 0x20
+readelf: Warning:  type: 0x78, namesize: 0x00000100, descsize: 0x756e694c, alignment: 8
+[hjl@gnu-skx-1 vdso]$
+
+Since note.gnu.property section in vDSO is not checked by dynamic linker,
+this patch discards .note.gnu.property sections in vDSO by adding
+
+/DISCARD/ : {
+ *(.note.gnu.property)
+}
+
+before .notes sections in vDSO linker script.
+
+Signed-off-by: H.J. Lu <hjl.tools@gmail.com>
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+---
+ arch/x86/entry/vdso/vdso-layout.lds.S | 7 +++++++
+ 1 file changed, 7 insertions(+)
+
+diff --git a/arch/x86/entry/vdso/vdso-layout.lds.S b/arch/x86/entry/vdso/vdso-layout.lds.S
+index ea7e0155c604..4d152933547d 100644
+--- a/arch/x86/entry/vdso/vdso-layout.lds.S
++++ b/arch/x86/entry/vdso/vdso-layout.lds.S
+@@ -57,6 +57,13 @@ SECTIONS
+ 		*(.gnu.linkonce.b.*)
+ 	}						:text
+ 
++	/*
++	 * Discard .note.gnu.property sections which are unused and have
++	 * different alignment requirement from vDSO note sections.
++	 */
++	/DISCARD/ : {
++		*(.note.gnu.property)
++	}
+ 	.note		: { *(.note.*) }		:text	:note
+ 
+ 	.eh_frame_hdr	: { *(.eh_frame_hdr) }		:text	:eh_frame_hdr
+-- 
+2.26.0
+
diff --git a/0048-Add-RUNTIME_DISCARD_EXIT-to-generic-DISCARDS.patch b/0048-Add-RUNTIME_DISCARD_EXIT-to-generic-DISCARDS.patch
new file mode 100644
index 000000000..b810469db
--- /dev/null
+++ b/0048-Add-RUNTIME_DISCARD_EXIT-to-generic-DISCARDS.patch
@@ -0,0 +1,54 @@
+From b297af1156cb54529dfaf96dc0072a682674a1f1 Mon Sep 17 00:00:00 2001
+From: "H.J. Lu" <hjl.tools@gmail.com>
+Date: Thu, 30 Jan 2020 12:31:22 -0800
+Subject: [PATCH 48/52] Add RUNTIME_DISCARD_EXIT to generic DISCARDS
+
+In x86 kernel, .exit.text and .exit.data sections are discarded at
+runtime, not by linker.  Add RUNTIME_DISCARD_EXIT to generic DISCARDS
+and define it in x86 kernel linker script to keep them.
+
+Signed-off-by: H.J. Lu <hjl.tools@gmail.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+---
+ arch/x86/kernel/vmlinux.lds.S     |  1 +
+ include/asm-generic/vmlinux.lds.h | 10 ++++++++--
+ 2 files changed, 9 insertions(+), 2 deletions(-)
+
+diff --git a/arch/x86/kernel/vmlinux.lds.S b/arch/x86/kernel/vmlinux.lds.S
+index e3296aa028fe..7206e1ac23dd 100644
+--- a/arch/x86/kernel/vmlinux.lds.S
++++ b/arch/x86/kernel/vmlinux.lds.S
+@@ -21,6 +21,7 @@
+ #define LOAD_OFFSET __START_KERNEL_map
+ #endif
+ 
++#define RUNTIME_DISCARD_EXIT
+ #define EMITS_PT_NOTE
+ #define RO_EXCEPTION_TABLE_ALIGN	16
+ 
+diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
+index e00f41aa8ec4..6b943fb8c5fd 100644
+--- a/include/asm-generic/vmlinux.lds.h
++++ b/include/asm-generic/vmlinux.lds.h
+@@ -894,10 +894,16 @@
+  * section definitions so that such archs put those in earlier section
+  * definitions.
+  */
++#ifdef RUNTIME_DISCARD_EXIT
++#define EXIT_DISCARDS
++#else
++#define EXIT_DISCARDS							\
++	EXIT_TEXT							\
++	EXIT_DATA
++#endif
+ #define DISCARDS							\
+ 	/DISCARD/ : {							\
+-	EXIT_TEXT							\
+-	EXIT_DATA							\
++	EXIT_DISCARDS							\
+ 	EXIT_CALL							\
+ 	*(.discard)							\
+ 	*(.discard.*)							\
+-- 
+2.26.0
+
diff --git a/0049-Discard-.note.gnu.property-sections-in-generic-NOTES.patch b/0049-Discard-.note.gnu.property-sections-in-generic-NOTES.patch
new file mode 100644
index 000000000..6e8e15ee2
--- /dev/null
+++ b/0049-Discard-.note.gnu.property-sections-in-generic-NOTES.patch
@@ -0,0 +1,81 @@
+From f665b275dab0cee4ac3777a95c0e3d23841c791d Mon Sep 17 00:00:00 2001
+From: "H.J. Lu" <hjl.tools@gmail.com>
+Date: Thu, 30 Jan 2020 12:39:09 -0800
+Subject: [PATCH 49/52] Discard .note.gnu.property sections in generic NOTES
+
+With the command-line option, -mx86-used-note=yes, the x86 assembler
+in binutils 2.32 and above generates a program property note in a note
+section, .note.gnu.property, to encode used x86 ISAs and features.  But
+kernel linker script only contains a single NOTE segment:
+
+PHDRS {
+ text PT_LOAD FLAGS(5);
+ data PT_LOAD FLAGS(6);
+ percpu PT_LOAD FLAGS(6);
+ init PT_LOAD FLAGS(7);
+ note PT_NOTE FLAGS(0);
+}
+SECTIONS
+{
+...
+ .notes : AT(ADDR(.notes) - 0xffffffff80000000) { __start_notes = .; KEEP(*(.not
+e.*)) __stop_notes = .; } :text :note
+...
+}
+
+The NOTE segment generated by kernel linker script is aligned to 4 bytes.
+But .note.gnu.property section must be aligned to 8 bytes on x86-64 and
+we get
+
+[hjl@gnu-skx-1 linux]$ readelf -n vmlinux
+
+Displaying notes found in: .notes
+  Owner                Data size Description
+  Xen                  0x00000006 Unknown note type: (0x00000006)
+   description data: 6c 69 6e 75 78 00
+  Xen                  0x00000004 Unknown note type: (0x00000007)
+   description data: 32 2e 36 00
+  xen-3.0              0x00000005 Unknown note type: (0x006e6558)
+   description data: 08 00 00 00 03
+readelf: Warning: note with invalid namesz and/or descsz found at offset 0x50
+readelf: Warning:  type: 0xffffffff, namesize: 0x006e6558, descsize:
+0x80000000, alignment: 8
+[hjl@gnu-skx-1 linux]$
+
+Since note.gnu.property section in kernel image is never used, this patch
+discards .note.gnu.property sections in kernel linker script by adding
+
+/DISCARD/ : {
+  *(.note.gnu.property)
+}
+
+before kernel NOTE segment in generic NOTES.
+
+Signed-off-by: H.J. Lu <hjl.tools@gmail.com>
+Reviewed-by: Kees Cook <keescook@chromium.org>
+---
+ include/asm-generic/vmlinux.lds.h | 7 +++++++
+ 1 file changed, 7 insertions(+)
+
+diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
+index 6b943fb8c5fd..6659a7c07c84 100644
+--- a/include/asm-generic/vmlinux.lds.h
++++ b/include/asm-generic/vmlinux.lds.h
+@@ -818,7 +818,14 @@
+ #define TRACEDATA
+ #endif
+ 
++/*
++ * Discard .note.gnu.property sections which are unused and have
++ * different alignment requirement from kernel note sections.
++ */
+ #define NOTES								\
++	/DISCARD/ : {							\
++		*(.note.gnu.property)					\
++	}								\
+ 	.notes : AT(ADDR(.notes) - LOAD_OFFSET) {			\
+ 		__start_notes = .;					\
+ 		KEEP(*(.note.*))					\
+-- 
+2.26.0
+
diff --git a/0050-x86-vdso-32-Add-ENDBR32-to-__kernel_vsyscall-entry-p.patch b/0050-x86-vdso-32-Add-ENDBR32-to-__kernel_vsyscall-entry-p.patch
new file mode 100644
index 000000000..a92706946
--- /dev/null
+++ b/0050-x86-vdso-32-Add-ENDBR32-to-__kernel_vsyscall-entry-p.patch
@@ -0,0 +1,32 @@
+From 823be9132f4d5e011cd591b6636a387346236885 Mon Sep 17 00:00:00 2001
+From: "H.J. Lu" <hjl.tools@gmail.com>
+Date: Fri, 28 Sep 2018 06:21:50 -0700
+Subject: [PATCH 50/52] x86/vdso/32: Add ENDBR32 to __kernel_vsyscall entry
+ point
+
+Add ENDBR32 to __kernel_vsyscall entry point.
+
+Signed-off-by: H.J. Lu <hjl.tools@gmail.com>
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Acked-by: Andy Lutomirski <luto@kernel.org>
+---
+ arch/x86/entry/vdso/vdso32/system_call.S | 3 +++
+ 1 file changed, 3 insertions(+)
+
+diff --git a/arch/x86/entry/vdso/vdso32/system_call.S b/arch/x86/entry/vdso/vdso32/system_call.S
+index de1fff7188aa..5cf74ebd4746 100644
+--- a/arch/x86/entry/vdso/vdso32/system_call.S
++++ b/arch/x86/entry/vdso/vdso32/system_call.S
+@@ -14,6 +14,9 @@
+ 	ALIGN
+ __kernel_vsyscall:
+ 	CFI_STARTPROC
++#ifdef CONFIG_X86_INTEL_BRANCH_TRACKING_USER
++	endbr32
++#endif
+ 	/*
+ 	 * Reshuffle regs so that all of any of the entry instructions
+ 	 * will preserve enough state.
+-- 
+2.26.0
+
diff --git a/0051-x86-vdso-Insert-endbr32-endbr64-to-vDSO.patch b/0051-x86-vdso-Insert-endbr32-endbr64-to-vDSO.patch
new file mode 100644
index 000000000..1f7fcd1db
--- /dev/null
+++ b/0051-x86-vdso-Insert-endbr32-endbr64-to-vDSO.patch
@@ -0,0 +1,35 @@
+From 3a79ee6a030a38e0bb70787f848521bae19f84c2 Mon Sep 17 00:00:00 2001
+From: "H.J. Lu" <hjl.tools@gmail.com>
+Date: Fri, 16 Mar 2018 04:18:48 -0700
+Subject: [PATCH 51/52] x86/vdso: Insert endbr32/endbr64 to vDSO
+
+When Indirect Branch Tracking (IBT) is enabled, vDSO functions may be
+called indirectly, and must have ENDBR32 or ENDBR64 as the first
+instruction.  The compiler must support -fcf-protection=branch so that it
+can be used to compile vDSO.
+
+Signed-off-by: H.J. Lu <hjl.tools@gmail.com>
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+Acked-by: Andy Lutomirski <luto@kernel.org>
+---
+ arch/x86/entry/vdso/Makefile | 4 ++++
+ 1 file changed, 4 insertions(+)
+
+diff --git a/arch/x86/entry/vdso/Makefile b/arch/x86/entry/vdso/Makefile
+index 433a1259f61d..6de3493acfe4 100644
+--- a/arch/x86/entry/vdso/Makefile
++++ b/arch/x86/entry/vdso/Makefile
+@@ -120,6 +120,10 @@ $(obj)/%-x32.o: $(obj)/%.o FORCE
+ 
+ targets += vdsox32.lds $(vobjx32s-y)
+ 
++ifdef CONFIG_X86_INTEL_BRANCH_TRACKING_USER
++    $(obj)/vclock_gettime.o $(obj)/vgetcpu.o $(obj)/vdso32/vclock_gettime.o: KBUILD_CFLAGS += -fcf-protection=branch
++endif
++
+ $(obj)/%.so: OBJCOPYFLAGS := -S
+ $(obj)/%.so: $(obj)/%.so.dbg FORCE
+ 	$(call if_changed,objcopy)
+-- 
+2.26.0
+
diff --git a/0052-x86-Disallow-vsyscall-emulation-when-CET-is-enabled.patch b/0052-x86-Disallow-vsyscall-emulation-when-CET-is-enabled.patch
new file mode 100644
index 000000000..b77b72e82
--- /dev/null
+++ b/0052-x86-Disallow-vsyscall-emulation-when-CET-is-enabled.patch
@@ -0,0 +1,59 @@
+From 76faf117f8295ed4479b4ed5d24ee3d73d29e24a Mon Sep 17 00:00:00 2001
+From: "H.J. Lu" <hjl.tools@gmail.com>
+Date: Wed, 29 Jan 2020 08:44:11 -0800
+Subject: [PATCH 52/52] x86: Disallow vsyscall emulation when CET is enabled
+
+Emulation of the legacy vsyscall page is required by some programs built
+before 2013.  Newer programs after 2013 don't use it.  Disallow vsyscall
+emulation when Control-flow Enforcement (CET) is enabled to enhance
+security.
+
+Signed-off-by: H.J. Lu <hjl.tools@gmail.com>
+Signed-off-by: Yu-cheng Yu <yu-cheng.yu@intel.com>
+---
+ arch/x86/Kconfig | 8 ++++++--
+ 1 file changed, 6 insertions(+), 2 deletions(-)
+
+diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
+index 4318acefaff3..bf24728d9219 100644
+--- a/arch/x86/Kconfig
++++ b/arch/x86/Kconfig
+@@ -1213,7 +1213,7 @@ config X86_ESPFIX64
+ config X86_VSYSCALL_EMULATION
+ 	bool "Enable vsyscall emulation" if EXPERT
+ 	default y
+-	depends on X86_64
++	depends on X86_64 && !X86_INTEL_CET
+ 	---help---
+ 	 This enables emulation of the legacy vsyscall page.  Disabling
+ 	 it is roughly equivalent to booting with vsyscall=none, except
+@@ -1228,6 +1228,8 @@ config X86_VSYSCALL_EMULATION
+ 	 Disabling this option saves about 7K of kernel size and
+ 	 possibly 4K of additional runtime pagetable memory.
+ 
++	 This option is disabled when Intel CET is enabled.
++
+ config X86_IOPL_IOPERM
+ 	bool "IOPERM and IOPL Emulation"
+ 	default y
+@@ -2380,7 +2382,7 @@ config COMPAT_VDSO
+ 
+ choice
+ 	prompt "vsyscall table for legacy applications"
+-	depends on X86_64
++	depends on X86_64 && !X86_INTEL_CET
+ 	default LEGACY_VSYSCALL_XONLY
+ 	help
+ 	  Legacy user code that does not know how to find the vDSO expects
+@@ -2397,6 +2399,8 @@ choice
+ 
+ 	  If unsure, select "Emulate execution only".
+ 
++	  This option is not enabled when Intel CET is enabled.
++
+ 	config LEGACY_VSYSCALL_EMULATE
+ 		bool "Full emulation"
+ 		help
+-- 
+2.26.0
+
diff --git a/kernel.spec b/kernel.spec
index c214381db..f1d461858 100644
--- a/kernel.spec
+++ b/kernel.spec
@@ -1,5 +1,58 @@
 Patch100000: 0001-Don-t-enable-GPE-if-GPE-dispatch-failed.patch
 
+Patch200001: 0001-x86-fpu-xstate-Fix-last_good_offset-in-setup_xstate_.patch
+Patch200002: 0002-x86-fpu-xstate-Fix-XSAVES-offsets-in-setup_xstate_co.patch
+Patch200003: 0003-x86-fpu-xstate-Warn-when-checking-alignment-of-disab.patch
+Patch200004: 0004-x86-fpu-xstate-Rename-validate_xstate_header-to-vali.patch
+Patch200005: 0005-x86-fpu-xstate-Define-new-macros-for-supervisor-and-.patch
+Patch200006: 0006-x86-fpu-xstate-Separate-user-and-supervisor-xfeature.patch
+Patch200007: 0007-x86-fpu-xstate-Introduce-XSAVES-supervisor-states.patch
+Patch200008: 0008-x86-fpu-xstate-Define-new-functions-for-clearing-fpr.patch
+Patch200009: 0009-x86-fpu-xstate-Update-sanitize_restored_xstate-for-s.patch
+Patch200010: 0010-x86-fpu-xstate-Update-copy_kernel_to_xregs_err-for-X.patch
+Patch200011: 0011-x86-fpu-Introduce-copy_supervisor_to_kernel.patch
+Patch200012: 0012-x86-fpu-xstate-Restore-supervisor-xstates-for-__fpu_.patch
+Patch200013: 0013-Documentation-x86-Add-CET-description.patch
+Patch200014: 0014-x86-cpufeatures-Add-CET-CPU-feature-flags-for-Contro.patch
+Patch200015: 0015-x86-fpu-xstate-Introduce-CET-MSR-XSAVES-supervisor-s.patch
+Patch200016: 0016-x86-cet-Add-control-protection-fault-handler.patch
+Patch200017: 0017-x86-cet-shstk-Add-Kconfig-option-for-user-mode-Shado.patch
+Patch200018: 0018-mm-Introduce-VM_SHSTK-for-Shadow-Stack-memory.patch
+Patch200019: 0019-Add-guard-pages-around-a-Shadow-Stack.patch
+Patch200020: 0020-x86-mm-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_HW.patch
+Patch200021: 0021-x86-mm-Introduce-_PAGE_DIRTY_SW.patch
+Patch200022: 0022-x86-mm-Update-pte_modify-pmd_modify-and-_PAGE_CHG_MA.patch
+Patch200023: 0023-drm-i915-gvt-Change-_PAGE_DIRTY-to-_PAGE_DIRTY_BITS.patch
+Patch200024: 0024-x86-mm-Modify-ptep_set_wrprotect-and-pmdp_set_wrprot.patch
+Patch200025: 0025-x86-mm-Shadow-Stack-page-fault-error-checking.patch
+Patch200026: 0026-mm-Handle-Shadow-Stack-page-fault.patch
+Patch200027: 0027-mm-Handle-THP-HugeTLB-Shadow-Stack-page-fault.patch
+Patch200028: 0028-mm-Update-can_follow_write_pte-for-Shadow-Stack.patch
+Patch200029: 0029-x86-cet-shstk-User-mode-Shadow-Stack-support.patch
+Patch200030: 0030-x86-cet-shstk-Introduce-WRUSS-instruction.patch
+Patch200031: 0031-x86-cet-shstk-Handle-signals-for-Shadow-Stack.patch
+Patch200032: 0032-ELF-UAPI-and-Kconfig-additions-for-ELF-program-prope.patch
+Patch200033: 0033-ELF-Add-ELF-program-property-parsing-support.patch
+Patch200034: 0034-ELF-Introduce-arch_setup_elf_property.patch
+Patch200035: 0035-x86-cet-shstk-ELF-header-parsing-for-Shadow-Stack.patch
+Patch200036: 0036-x86-cet-shstk-Handle-thread-Shadow-Stack.patch
+Patch200037: 0037-mm-mmap-Add-Shadow-Stack-pages-to-memory-accounting.patch
+Patch200038: 0038-x86-cet-shstk-Add-arch_prctl-functions-for-Shadow-St.patch
+Patch200039: 0039-x86-cet-ibt-Add-Kconfig-option-for-user-mode-Indirec.patch
+Patch200040: 0040-x86-cet-ibt-User-mode-Indirect-Branch-Tracking-suppo.patch
+Patch200041: 0041-x86-cet-ibt-Handle-signals-for-Indirect-Branch-Track.patch
+Patch200042: 0042-x86-cet-ibt-ELF-header-parsing-for-Indirect-Branch-T.patch
+Patch200043: 0043-x86-cet-ibt-Add-arch_prctl-functions-for-Indirect-Br.patch
+Patch200044: 0044-x86-cet-Add-PTRACE-interface-for-CET.patch
+Patch200045: 0045-x86-insn-Add-Control-flow-Enforcement-CET-instructio.patch
+Patch200046: 0046-x86-insn-perf-tools-Add-CET-instructions-to-the-new-.patch
+Patch200047: 0047-x86-Discard-.note.gnu.property-sections-in-vDSO.patch
+Patch200048: 0048-Add-RUNTIME_DISCARD_EXIT-to-generic-DISCARDS.patch
+Patch200049: 0049-Discard-.note.gnu.property-sections-in-generic-NOTES.patch
+Patch200050: 0050-x86-vdso-32-Add-ENDBR32-to-__kernel_vsyscall-entry-p.patch
+Patch200051: 0051-x86-vdso-Insert-endbr32-endbr64-to-vDSO.patch
+Patch200052: 0052-x86-Disallow-vsyscall-emulation-when-CET-is-enabled.patch
+
 # We have to override the new %%install behavior because, well... the kernel is special.
 %global __spec_install_pre %{___build_pre}
 
-- 
2.26.0

