From 6b22554109f9861b4f6712f0f4e46885f258f9a4 Mon Sep 17 00:00:00 2001
From: "H.J. Lu" <hjl.tools@gmail.com>
Date: Mon, 6 Jul 2020 13:23:52 -0700
Subject: [PATCH] Update 2 x86: Don't hardcode thresholds for "rep movsb/stosb"

cherry picked from commit 3f4b61a0b8de67ef9f20737919c713ddfc4bd620
---
 ...olds-for-rep-movsb-stosb-to-tunables.patch | 86 +++++++++++--------
 1 file changed, 49 insertions(+), 37 deletions(-)

diff --git a/0001-x86-Add-thresholds-for-rep-movsb-stosb-to-tunables.patch b/0001-x86-Add-thresholds-for-rep-movsb-stosb-to-tunables.patch
index 5f5b76e..e6e6ef5 100644
--- a/0001-x86-Add-thresholds-for-rep-movsb-stosb-to-tunables.patch
+++ b/0001-x86-Add-thresholds-for-rep-movsb-stosb-to-tunables.patch
@@ -1,6 +1,6 @@
-From 243400ef095a78c15d6ec4d0665f94f16b94a006 Mon Sep 17 00:00:00 2001
+From 4d035933792f45bf09ffd31cd3ee415447b5bc5a Mon Sep 17 00:00:00 2001
 From: "H.J. Lu" <hjl.tools@gmail.com>
-Date: Sat, 9 May 2020 11:13:57 -0700
+Date: Mon, 6 Jul 2020 11:48:09 -0700
 Subject: [PATCH] x86: Add thresholds for "rep movsb/stosb" to tunables
 
 Add x86_rep_movsb_threshold and x86_rep_stosb_threshold to tunables
@@ -8,18 +8,22 @@ to update thresholds for "rep movsb" and "rep stosb" at run-time.
 
 Note that the user specified threshold for "rep movsb" smaller than
 the minimum threshold will be ignored.
+
+Reviewed-by: Carlos O'Donell <carlos@redhat.com>
+
+(cherry picked from commit 3f4b61a0b8de67ef9f20737919c713ddfc4bd620)
 ---
- manual/tunables.texi                          | 16 +++++++
- sysdeps/x86/cacheinfo.c                       | 46 +++++++++++++++++++
- sysdeps/x86/cpu-features.c                    |  4 ++
- sysdeps/x86/cpu-features.h                    |  4 ++
- sysdeps/x86/dl-tunables.list                  |  6 +++
- .../multiarch/memmove-vec-unaligned-erms.S    | 16 +------
- .../multiarch/memset-vec-unaligned-erms.S     | 12 +----
- 7 files changed, 78 insertions(+), 26 deletions(-)
+ manual/tunables.texi                          | 16 +++++++++
+ sysdeps/x86/cacheinfo.c                       | 36 +++++++++++++++++++
+ sysdeps/x86/cpu-features.c                    |  4 +++
+ sysdeps/x86/cpu-features.h                    |  4 +++
+ sysdeps/x86/dl-tunables.list                  | 24 +++++++++++++
+ .../multiarch/memmove-vec-unaligned-erms.S    | 16 +--------
+ .../multiarch/memset-vec-unaligned-erms.S     | 12 +------
+ 7 files changed, 86 insertions(+), 26 deletions(-)
 
 diff --git a/manual/tunables.texi b/manual/tunables.texi
-index ec18b10834..8054f79be0 100644
+index ec18b10834..4e68c7ff91 100644
 --- a/manual/tunables.texi
 +++ b/manual/tunables.texi
 @@ -396,6 +396,22 @@ to set threshold in bytes for non temporal store.
@@ -27,17 +31,17 @@ index ec18b10834..8054f79be0 100644
  @end deftp
  
 +@deftp Tunable glibc.cpu.x86_rep_movsb_threshold
-+The @code{glibc.cpu.x86_rep_movsb_threshold} tunable allows the user
-+to set threshold in bytes to start using "rep movsb".  Note that the
-+user specified threshold smaller than the minimum threshold will be
-+ignored.
++The @code{glibc.cpu.x86_rep_movsb_threshold} tunable allows the user to
++set threshold in bytes to start using "rep movsb".  The value must be
++greater than zero, and currently defaults to 2048 bytes.
 +
 +This tunable is specific to i386 and x86-64.
 +@end deftp
 +
 +@deftp Tunable glibc.cpu.x86_rep_stosb_threshold
-+The @code{glibc.cpu.x86_rep_stosb_threshold} tunable allows the user
-+to set threshold in bytes to start using "rep stosb".
++The @code{glibc.cpu.x86_rep_stosb_threshold} tunable allows the user to
++set threshold in bytes to start using "rep stosb".  The value must be
++greater than zero, and currently defaults to 2048 bytes.
 +
 +This tunable is specific to i386 and x86-64.
 +@end deftp
@@ -46,34 +50,23 @@ index ec18b10834..8054f79be0 100644
  The @code{glibc.cpu.x86_ibt} tunable allows the user to control how
  indirect branch tracking (IBT) should be enabled.  Accepted values are
 diff --git a/sysdeps/x86/cacheinfo.c b/sysdeps/x86/cacheinfo.c
-index e3e8ef27bb..47fc5c3844 100644
+index e3e8ef27bb..2d1571f78a 100644
 --- a/sysdeps/x86/cacheinfo.c
 +++ b/sysdeps/x86/cacheinfo.c
-@@ -476,6 +476,23 @@ long int __x86_raw_shared_cache_size attribute_hidden = 1024 * 1024;
+@@ -476,6 +476,12 @@ long int __x86_raw_shared_cache_size attribute_hidden = 1024 * 1024;
  /* Threshold to use non temporal store.  */
  long int __x86_shared_non_temporal_threshold attribute_hidden;
  
-+/* Threshold to use Enhanced REP MOVSB.  Since there is overhead to set
-+   up REP MOVSB operation, REP MOVSB isn't faster on short data.  The
-+   memcpy micro benchmark in glibc shows that 2KB is the approximate
-+   value above which REP MOVSB becomes faster than SSE2 optimization
-+   on processors with Enhanced REP MOVSB.  Since larger register size
-+   can move more data with a single load and store, the threshold is
-+   higher with larger register size.  */
++/* Threshold to use Enhanced REP MOVSB.  */
 +long int __x86_rep_movsb_threshold attribute_hidden = 2048;
 +
-+/* Threshold to use Enhanced REP STOSB.  Since there is overhead to set
-+   up REP STOSB operation, REP STOSB isn't faster on short data.  The
-+   memset micro benchmark in glibc shows that 2KB is the approximate
-+   value above which REP STOSB becomes faster on processors with
-+   Enhanced REP STOSB.  Since the stored value is fixed, larger register
-+   size has minimal impact on threshold.  */
++/* Threshold to use Enhanced REP STOSB.  */
 +long int __x86_rep_stosb_threshold attribute_hidden = 2048;
 +
  #ifndef DISABLE_PREFETCHW
  /* PREFETCHW support flag for use in memory and string routines.  */
  int __x86_prefetchw attribute_hidden;
-@@ -786,6 +803,35 @@ intel_bug_no_cache_info:
+@@ -786,6 +792,36 @@ intel_bug_no_cache_info:
      = (cpu_features->non_temporal_threshold != 0
         ? cpu_features->non_temporal_threshold
         : __x86_shared_cache_size * threads * 3 / 4);
@@ -104,8 +97,9 @@ index e3e8ef27bb..47fc5c3844 100644
 +  else
 +    __x86_rep_movsb_threshold = rep_movsb_threshold;
 +
-+  if (cpu_features->rep_stosb_threshold)
-+    __x86_rep_stosb_threshold = cpu_features->rep_stosb_threshold;
++# if HAVE_TUNABLES
++  __x86_rep_stosb_threshold = cpu_features->rep_stosb_threshold;
++# endif
  }
  
  #endif
@@ -140,18 +134,36 @@ index aea83e6e31..082c21baa7 100644
  
  /* Used from outside of glibc to get access to the CPU features
 diff --git a/sysdeps/x86/dl-tunables.list b/sysdeps/x86/dl-tunables.list
-index 251b926ce4..43bf6c2389 100644
+index 251b926ce4..1a4a93a070 100644
 --- a/sysdeps/x86/dl-tunables.list
 +++ b/sysdeps/x86/dl-tunables.list
-@@ -30,6 +30,12 @@ glibc {
+@@ -30,6 +30,30 @@ glibc {
      x86_non_temporal_threshold {
        type: SIZE_T
      }
 +    x86_rep_movsb_threshold {
 +      type: SIZE_T
++      # Since there is overhead to set up REP MOVSB operation, REP MOVSB
++      # isn't faster on short data.  The memcpy micro benchmark in glibc
++      # shows that 2KB is the approximate value above which REP MOVSB
++      # becomes faster than SSE2 optimization on processors with Enhanced
++      # REP MOVSB.  Since larger register size can move more data with a
++      # single load and store, the threshold is higher with larger register
++      # size.  Note: Since the REP MOVSB threshold must be greater than 8
++      # times of vector size, the minium value must be updated at run-time.
++      minval: 1
++      default: 2048
 +    }
 +    x86_rep_stosb_threshold {
 +      type: SIZE_T
++      # Since there is overhead to set up REP STOSB operation, REP STOSB
++      # isn't faster on short data.  The memset micro benchmark in glibc
++      # shows that 2KB is the approximate value above which REP STOSB
++      # becomes faster on processors with Enhanced REP STOSB.  Since the
++      # stored value is fixed, larger register size has minimal impact
++      # on threshold.
++      minval: 1
++      default: 2048
 +    }
      x86_data_cache_size {
        type: SIZE_T
-- 
2.26.2

